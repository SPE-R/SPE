% \documentclass[12pt]{article}

% \parskip 6pt
% \parindent 0pt

% \usepackage{a4wide}
% \usepackage{url}

% %Temporary fix to get right number
% \setcounter{secnumdepth}{2}

% \begin{document}
\SweaveOpts{keep.source=TRUE,eps=FALSE, results=hide,  prefix.string=./graph/occoh-caco}

\section{Nested case-control study and case-cohort study:\newline Risk factors of coronary heart disease}

In this exercise we shall apply both the nested case-control (NCC) 
design and the case-cohort (CC)  design in sampling
control subjects from a defined cohort or closed study population. 
The case group comprises those cohort members who die from coronary heart disease (CHD) during a $> 20$ years follow-up of the cohort.  
The risk factors of interest are cigarette smoking, systolic blood pressure,  and total cholesterol level.

Our study population is an occupational cohort comprising 1501 men
working in blue-collar jobs in one Nordic country. 
Eligible subjects had no history of coronary heart disease
when recruited to the study in the early 1990s. 
Smoking habits and many other items were inquired at baseline
by a questionnaire, and 
blood pressure was measured by a research nurse, the values being
 written down on the questionnaire. Serum samples were also taken from the cohort members at the same time and were stored in a freezer. For some reason,
the data in the questionnaires were not entered to any computer file, but the questionnaires were kept in a safe storehouse for further purposes. 
Also, no biochemical analyses were initially performed for the sera 
collected from the participants.  However, dates of birth and dates of entry to the study were recorded
in an electronic file.

In 2010 the study was suddenly reactivated by those investigators of the original team who were still alive then.
As the first step mortality follow-up of the cohort members was
executed by record linkage to the national population register, from which
the dates of death and emigration were obtained. Another linkage was performed with the national register of causes of death in order to get the
deaths from coronary heard disease identified. As a result a data file
{\tt occoh.txt} was completed containing the following variables:

\begin{center}
\begin{tabular}{r l}
\texttt{id} & = identification number, \\
\texttt{birth} & = date of birth,\\
\texttt{entry} & = date of recruitment and baseline measurements, \\
\texttt{exit} & = date of exit from mortality follow-up,\\
\texttt{death} & = indicator for vital status at the end of follow-up, \\
               & = 1, if dead from any cause, and = 0, if alive, \\
\texttt{chdeath} & = indicator for death from coronary heart disease, \\  
              & = 1, if ``yes'', and 0, if ``no''. \\
\end{tabular}
\end{center}

This exercise is divided into five main parts:
\begin{itemize}
\item[(1)] 
  Description of the study base or the follow-up experience of the
  whole cohort, identification of the cases and illustrating the risk sets. 
\item[(2)]
  Nested case-control study within the cohort: 
  (i) selection of controls by risk set or time-matched sampling
  using function {\tt ccwc()} in package {\tt Epi}, 
  (ii) collection of exposure data for cases and controls 
   from the pertinent data base of the whole cohort to the
  case-control data set using function {\tt merge()}, and (iii)
  analysis of case-control data using function {\tt clogit()}
  in package {\tt survival()},
\item[(3)]
 Case-cohort study within the cohort:
 (i) selection of a subcohort by simple random sampling from
 the cohort,
 (ii) fitting the Cox model to the data by weighted partial
 likelihood using function {\tt coxph()} in package {\tt survival()}
 with appropriate weighting and correction of estimated
 covariance matrix for the model coefficients; also
 using function {\tt cch()} in package {\tt survival()}
 for the same task. % with specification  {\tt method "LinYing"},
%  (iii) fitting the same Cox model to the data by Prentice's
% pseudo-likelihood using function {\tt coxph()} 
%  with appropriate data specification and correction of estimated
% covariance; also
% using function {\tt cch()} in {\tt survival()}
% for the same task.
 \item[(4)]
 Comparison of results from all previous analyses,
 also with those from a full cohort design.
 \item[(5)] 
 Further tasks and homework.
\end{itemize}
 
   
  
\subsection{Reading the cohort data, illustrating the study base and risk sets}

\begin{enumerate}[resume]

\item
Load the packages {\tt Epi} and {\tt survival}. 
Read in the cohort data file and name
the resulting data frame as \texttt{oc}.
See its structure and print the univariate summaries.
<<Read in occoh data, echo=T>>=
library(Epi)
library(survival)
url <- "http://bendixcarstensen.com/SPE/data"
oc <- read.table( paste(url, "occoh.txt", sep = "/"), header=T)
str(oc)
summary(oc)
@ 

\item
It is convenient to change all the dates into fractional calendar years
<<cal.yr, echo=T>>=
oc$ybirth <- cal.yr(oc$birth) 
oc$yentry <- cal.yr(oc$entry) 
oc$yexit <- cal.yr(oc$exit)
@

We shall also compute the age at entry and at exit, respectively,
as age will be the main time scale in our analyses.
<<age.yr, echo=T>>=
oc$agentry <- oc$yentry - oc$ybirth
oc$agexit <- oc$yexit - oc$ybirth 
@
\item
As the next step we shall create a {\tt lexis} object
from the data frame along the calendar period and age axes,
and as the outcome event we specify the coronary death.
<<oclexis, echo=T>>=
oc.lex <- Lexis( entry = list( per = yentry, 
                               age = yentry - ybirth ), 
                  exit = list( per = yexit),
           exit.status = chdeath,
                    id = id, data = oc)
str(oc.lex)
summary(oc.lex)
@

\item
At this stage it is informative to examine a graphical
presentation of the follow-up lines and outcome cases in a conventional 
Lexis diagram. To rationalize your work we have created a separate source file
{\tt plots-caco-ex.R} to do the graphics for this tas as well as for some forthcoming ones. 
The source source file is found in the same folder % {\tt http://bendixcarstensen.com/SPE/data/} 
where the data sets are.located -- Load the source file and have a look at the content of the first function in it
<<plotfile, echo=T, fig=F>>=
source( paste(url,"plots-caco-ex.R", sep = "/") )
plot1
@ 
Function {\tt plot1()} makes the graph required here. No arguments are needed when calling the function
<<plotlexis, echo=T, fig = F>>=
plot1()
@

\item
As age is here the main time axis, 
we shall illustrate the {\it study base} 
or the follow-up lines and outcome events 
along the age scale, being ordered by age at exit.
Function {\tt plot2()} in the same source file does the work.
Vertical lines at those ages when new coronary
deaths occur are drawn to identify the pertinent
{\it risk sets}. For that purpose it is useful first 
 to sort the data frame and the {\tt lexis} object 
 jointly by age at exit \& age at entry, 
 and to give a new ID number according to that order.
<<plotlexage, echo=T, fig=F>>=
oc.ord <- cbind(ID = 1:1501, oc[ order( oc$agexit, oc$agentry), ] )  
oc.lexord <- Lexis( entry = list( age = agentry ), 
                     exit = list( age = agexit),
              exit.status = chdeath,
                       id = ID, data = oc.ord)
plot2
plot2()
@

Using function {\tt plot3()} in the same source file we now
 zoom the graphical illustration of the risk sets into
event times occurring between 50 to 58 years.
<<plotlexage2, echo=T, fig=F>>=
plot3
plot3()
@

\end{enumerate} % [resume]

\subsection{Nested case-control study}

We shall now employ the strategy of {\it risk-set sampling}
or {\it time-matched} sampling of controls, {\it i.e.}
we are conducting a {\it nested case-control study}
within the cohort.

\begin{enumerate}[resume]
\item
The risk sets are defined according to the age at diagnosis of the case. Further matching is applied for age at entry by 1-year agebands.
For this purpose we first generate a categorical variable
{\tt agen2} for age at entry 
<<agentry2, echo=T>>=
oc.lex$agen2 <- cut(oc.lex$agentry, br = seq(40, 62, 1) )
@

Matched sampling from risk sets may be carried out using 
function {\tt ccwc()} found in the {\tt Epi} package.
Its main arguments are the times 
of {\tt entry} and {\tt exit} which specify the time at risk along the
main time scale (here age), and the outcome variable to be given 
in the {\tt fail} argument. The number of controls per case
is set to be two, and the additional matching factor is given. 
-- After setting the RNG seed (with your own number), 
make a call of this function and see
the structure of the resulting data frame {\tt cactrl} 
containing the cases and the chosen individual controls. 
<<risksetsample, echo=T>>=
set.seed(9863157)
cactrl <- 
   ccwc(entry=agentry, exit=agexit, fail=chdeath, 
        controls = 2, match= agen2, 
        include = list(id, agentry), 
        data=oc.lex, silent=F)
str(cactrl)
@
Check the meaning of the four first columns of the case-control 
data frame from the help page of function {\tt ccwc()}.

\item
Now we shall start  collecting data on the 
risk factors for the cases and their
matched controls, including determination of the total cholesterol levels from the frozen sera! The storehouse of the risk factor measurements for 
the whole cohort is file {\tt occoh-Xdata.txt}. It contains
values of the following variables.
\begin{center}
\begin{tabular}{r l}
\texttt{id} = & identification number, the same as in {\tt occoh.txt}, \\
\texttt{smok} = &  cigarette smoking with categories, \\
       & 1: ``never'', 2: ``former'', 3: ``1-14/d'', 4: ``15+/d'', \\
\texttt{sbp} = &  systolic blood pressure (mmHg), \\ 
\texttt{tchol} = &  total cholesterol level (mmol/l).
\end{tabular}
\end{center} 

<<ocX, echo=T>>=
ocX <- read.table( paste(url, "occoh-Xdata.txt", sep = "/"), header=T)
str(ocX)
@
 
\item 
In the next step we collect the values of the risk factors
for our cases and controls by merging the case-control data frame and 
the storehouse file. 
In this  operation we use
 the {\tt id} variable in both files as the key to link each
individual case and control with his own data on risk factors.
<<merge, echo=T>>=
oc.ncc <- merge(cactrl, ocX[, c("id", "smok", "tchol", "sbp")], 
   by = "id")
str(oc.ncc)
@

\item  
We shall treat smoking as categorical and
 total cholesterol and systolic blood pressure 
as quantitative risk factors, but the values of the
latter will be divided by 10 to get more interpretable effect estimates.

\begin{comment}
\medskip
\begin{tabular}{r l}
\texttt{cholgrp} = &  cholesterol class,  
        1: ``<5'', 2: ``5-<6.5'', 3: ``>=6.5'', \\
\texttt{sbpgrp} = &  blood pressure class,  
       1: ``<130'', 2: ``130-<150'', 3: ``150-<170'', 4: ``>=170''.
\end{tabular}

\end{comment}

\medskip
Convert the smoking variable into a factor.
<<factor smol, echo=T>>=
oc.ncc$smok <- factor(oc.ncc$smok, 
    labels = c("never", "ex", "1-14/d", ">14/d"))          
@

\item
It is useful to start the analysis of case-control data by 
simple tabulations by the categorized risk factors. 
Crude estimates of the rate ratios associated with them,
in which matching is ignored, can be obtained as instructed in Janne's lecture
on Poisson and logistic models on Saturday 23 May. We shall focus on smoking
<<cccrude smok, echo=T>>=
stat.table( index = list( smok, Fail ), 
          contents = list( count(), percent(smok) ),
           margins = T, data = oc.ncc )
smok.crncc <- glm( Fail ~ smok, family=binomial, data = oc.ncc)
round(ci.lin(smok.crncc, Exp=T)[, 5:7], 3) 
@



\item
A proper analysis takes into account matching that was employed 
in the selection of controls for each case from the 
pertinent risk set further restricted to 
subjects who were about the same age at entry as the case was.
Also, adjustment for the other risk factors is desirable.
In this analysis function {\tt clogit()} in {\tt survival} package is
utilized. It is in fact a wrapper of function {\tt coxph()}. 

<<clogit , echo=T>>=
m.clogit <- clogit( Fail ~ smok + I(sbp/10) + tchol + 
       strata(Set), data = oc.ncc )
summary(m.clogit)
round(ci.exp(m.clogit), 3)
@

Compare these with the crude estimates obtained above.

\end{enumerate} % [resume]



\subsection{Case-cohort study}

Now we start applying the second major outcome-selective 
sampling strategy
for collecting exposure data from a big study population

\begin{enumerate}[resume]

\item
The subcohort is selected as a  
  simple random sample ($n=260$) from the whole cohort.
The {\tt id}-numbers of the individuals that are 
selected will be stored in vector {\tt subcids}, and
{\tt subcind} is an indicator for inclusion to the subcohort. 
<<subc sample, echo=T>>= 
N <- 1501; n <- 260
set.seed(1579863)
subcids <- sample(N, n )
oc.lex$subcind <- 1*(oc.lex$id %in% subcids)
@

\item
We form the data frame {\tt oc.cc}
 to be used in the subsequent 
analysis selecting the union of the subcohort members 
and the case group from the data frame of the full cohort.
After that we collect the data of the risk factors from the
data storehouse for the  subjects in the case-cohort data 
<<casecoh data, echo=T>>= 
oc.cc <- subset( oc.lex, subcind==1 | chdeath ==1)
oc.cc <- merge( oc.cc, ocX[, c("id", "smok", "tchol", "sbp")], 
   by ="id")
str(oc.cc) 
@ 

\item Function {\tt plot4()} in the same source file
creates a graphical illustration of 
 the lifelines contained in the case-cohort data.
% in a subset covering failures between 54 and 61 years of age.
Lines for the subcohort non-cases are grey without bullet at exit,
those for subcohort cases are blue with blue bullet at exit, and
for cases outside the subcohort the lines are black and dotted with
black bullets at exit. 
<<casecoh-lines, echo=T, fig=F>>= 
plot4
plot4()
@

\item Define the categorical smoking variable again.
<<grouping , echo=T>>=
oc.cc$smok <- factor(oc.cc$smok, 
    labels = c("never", "ex", "1-14/d", ">14/d"))
@
\begin{comment}
<< >>		
oc.cc$cholgrp <- cut( oc.cc$tchol, br = c(2.4, 5, 6.5, 13), 
          include.lowest = T, right = F )
oc.cc$sbpgrp <- cut( oc.cc$sbp, br = c(95, 130, 150, 170, 240), 
          include.lowest = T, right = F )
@
\end{comment}

A crude estimate of the hazard ratio for the various smoking categories $k$
vs. non-smokers ($k=1$) can be obtained by tabulating cases $(D_k)$ and person-years ($y_k$)
 in the subcohort by smoking  and then computing the
relevant exposure odds ratio for each category:
$$ \text{HR}_k ^{\text{crude}} = \frac{D_k/D_1}{y_k/y_1} $$

<<cc-crude HR by smok>>=
sm.cc <- stat.table( index = smok, 
   contents = list( Cases = sum(lex.Xst), Pyrs = sum(lex.dur) ),
	 margins = T, data = oc.cc)
print(sm.cc, digits = c(sum=0, ratio=1))
HRcc <- (sm.cc[ 1, -5]/sm.cc[ 1, 1])/(sm.cc[ 2, -5]/sm.cc[2, 1])		
round(HRcc, 3)			
@

\item
To estimate jointly the rate ratios associated with the 
categorized risk factors we now fit the pertinent Cox model 
applying the method of {\it weighted partial likelihood} as 
presented by Ling \& Ying (1993) and Barlow (1994).
The weights for all cases and non-cases in the subcohort
are first computed and added to the data frame.
<<weights, echo=T>>=
N.nonc <- N-sum(oc.lex$chdeath)  # non-cases in whole cohort
n.nonc <- sum(oc.cc$subcind * (1-oc.cc$chdeath)) # non-cases in subcohort
wn <- N.nonc/n.nonc          # weight for non-cases in subcohort
c(N.nonc, n.nonc, wn)
oc.cc$w <- ifelse(oc.cc$subcind==1 & oc.cc$chdeath==0, wn, 1)
@

Next, the Cox model is fitted by the method of 
 weighted partial likelihood using {\tt coxph()},
such that the robust covariance matrix will be used
as the source of standard errors for the coefficients.

<<weighted cox, echo=T>>=
oc.cc$surob <- with(oc.cc, Surv(agentry, agexit, chdeath) )
cc.we <- coxph( surob ~  smok + I(sbp/10)  + tchol, robust = T,  
       weight = w, data = oc.cc)
summary(cc.we)
round( ci.exp(cc.we), 3)
@

The covariance matrix for the coefficients may also be computed by the dfbeta-method. After that a comparison is made between standard errors
from the naive, robust and dfbeta covariance matrix, respectively.
You will see that the naive SEs are essentially smaller than those
obtained by the robust and the dfbeta method, respectively.

<<weighted cox dfb, echo=T>>=
dfbw <- resid(cc.we, type='dfbeta')
covdfb.we <- cc.we$naive.var + 
   (n.nonc*(N.nonc-n.nonc)/N.nonc)*var(dfbw[ oc.cc$chdeath==0, ] )
cbind( sqrt(diag(cc.we$naive.var)), sqrt(diag(cc.we$var)),  
    sqrt(diag(covdfb.we))  )
@
 
\item
The same analysis can also be done using function {\tt cch()} 
in package {\tt survival} with {\tt method = "LinYing"}
as follows:

<<weighted cox LinYing, echo=T>>= 
cch.LY <- cch( surob ~  smok + I(sbp/10)  + tchol, stratum=NULL,
   subcoh = ~subcind, id = ~id,  cohort.size = N, data = oc.cc, 
    method ="LinYing" )
summary(cch.LY)
@

\item
The {\tt summary()} method for the {\tt cch()} object does not 
print the standard errors for the coefficients. The following comparison
demonstrates numerically that the method of Lin \& Ying
is the same as weighted partial likelihood coupled with
dfbeta covariance matrix.  
<<weighted cox SEs, echo=T>>= 
cbind( coef( cc.we), coef(cch.LY) )
round( cbind( sqrt(diag(cc.we$naive.var)), sqrt(diag(cc.we$var)),  
    sqrt(diag(covdfb.we)), sqrt(diag(cch.LY$var))  ), 3)
@    


\end{enumerate} % [resume]

\subsection{Full cohort analysis and comparisons}

Finally, suppose the investigators could afford to collect the
data of risk factors from the storehouse for the whole cohort.

\begin{enumerate}[resume]
\item
Let us form the data frame corresponding to the full cohort design
and convert again smoking to be categorical.
<<fullcoh, echo=T>>=
oc.full <- merge( oc.lex, ocX[, c("id", "smok", "tchol", "sbp")], 
   by.x = "id", by.y = "id") 
oc.full$smok <- factor(oc.full$smok, 
    labels = c("never", "ex", "1-14/d", ">14/d"))
@

Juts for comparison with the corresponding analysis in case-cohort data
perform a similar crude estimation of hazard ratios associated with smoking.
<<cox-crude HR by smok>>=
sm.coh <- stat.table( index = smok, 
   contents = list( Cases = sum(lex.Xst), Pyrs = sum(lex.dur) ),
	 margins = T, data = oc.full)
print(sm.coh, digits = c(sum=0, ratio=1))
HRcoh <- (sm.coh[ 1, -5]/sm.coh[ 1, 1])/(sm.coh[ 2, -5]/sm.coh[2, 1])		
round(HRcoh, 3)			
@


\item
Fit now the Cox model to the full cohort, and there is no need
to employ extra tricks upon the ordinary {\tt coxph()} fit.

<<cox full, echo=T>>=
cox.coh <- coxph( Surv(agentry, agexit, chdeath) ~ 
        smok + I(sbp/10)  + tchol, data = oc.full)
summary(cox.coh)        
@

\item
Lastly, a comparison of the point estimates and standard errors between 
the different designs, including variants of analysis for the case-cohort design, can be performed.

<<comparison, echo=T>>=
betas <- round(cbind( coef(cox.coh), 
     coef(m.clogit),
     coef(cc.we), coef(cch.LY) ), 3)
colnames(betas) <-  c("coh", "ncc", "cc.we", "cch.LY")
betas

SEs <- round(cbind( sqrt(diag(cox.coh$var)), 
    sqrt(diag(m.clogit$var)), sqrt(diag(cc.we$naive.var)),
    sqrt(diag(cc.we$var)),  sqrt(diag(covdfb.we)),
    sqrt(diag(cch.LY$var)) ), 3)
colnames(SEs) <- c("coh", "ncc", "ccwe-nai", 
       "ccwe-rob", "ccwe-dfb", "cch-LY")
SEs
@

You will notice that the point estimates of the coefficients
obtained from the full cohort, nested case-control, and case-cohort analyses, 
respectively, are somewhat variable. 

However,  
the standard errors across the NCC and different proper CC
 analyses are relatively similar. Those from a naive covariance matrix of a CC analysis,
though, are practically equal to the SEs from the full cohort analysis,
reflecting the fact that the naive analysis implicitly assumes
there being as much information available as there is with full cohort data.
\end{enumerate}

\subsection{Further exercises and homework}

\begin{enumerate}[resume]

\item
If you have time, you could run both the NCC study and CC study 
again but now with a larger control group or subcohort;
for example 4 controls per case in NCC and $n=520$ as the subcohort size in CC. Remember resetting the seed first. 
Pay attention in the results to how much closer 
will be the point estimates and the proper SEs to those
obtained from the full cohort design. 
\item
Instead of simple linear terms for {\tt sbp} and {\tt tchol} you could try to fit
spline models to describe their effects.
\item
A popular alternative to weighted partial likelihood
in the analysis of case-cohort data is the {\it pseudo-likelihood method}
(Prentice 1986), which is based on ``late entry'' to follow-up 
of the case subjects not belonging to 
the subcohort. A longer way of applying this
approach, which you could try at home after the course,
 would first require manipulation of the {\tt oc.cc} data frame,
as outlined on slide 34. Then  {\tt coxph()} would be 
called like in model object {\tt cc.we} above 
but now with weights = 1. Similar corrections
 on the covariance matrix are needed, too.
However, a shorter way is provided by function {\tt cch()} which
you can apply directly to the case-cohort data {\tt oc.cc} as before
but now with {\tt method = "Prentice"}. -- Try this and compare the results
with those obtained by weighted partial likelihood in models
{\tt cc.we} and  {\tt cch.LY}. 
\item
Yet another computational solution for
maximizing weighted partial likelihood is provided by 
a combination of functions {\tt twophase()} 
 and {\tt svycoxph()} of the {\tt survey} package.
\begin{comment}
In this approach, the starting point is a data frame, 
named e.g. {\tt oc.lex2},
that contains the whole cohort but includes also 
necessary columns for the interesting risk factors, such that
their values are known for the cases and the subcohort members
but are missing ({\tt NA}) for non-cases outside the subcohort.
The case-cohort design embedded in the cohort 
is viewed as a special case of a {\it two-phase sampling design}.
The pertinent design features are determined by
the key indicator variables in {\tt oc.lex2}: 
subcohort membership (cf. variable {\tt subcind} above)
and case status ({\tt chdeath}). 
Based on these items, 
% the design is first specified on all members of 
% the whole cohort using 
a call of function {\tt twophase()} with appropriate
arguments creates an object
 of class {\tt twophase}, named e.g. {\tt oc.2ph}, that
 contains the original data frame and all the survey design information
  (like weights) needed to analyse it.
After that, function {\tt svycoxph()}, which is  
a ``survey design version'' of {\tt coxph()}, 
is called with a similar model formula as before but in which,
instead of the {\tt data} argument, one would specify {\tt design = oc.2ph}.
% which takes care of the weights. 
\end{comment}
The approach is illustrated with an example
 in a vignette ``Two-phase designs in epidemiology'' by Thomas Lumley
(see {\tt http://cran.r-project.org/web/packages/survey/vignettes/epi.pdf}, p. 4--7).
-- You can try this at home and check that you would obtain similar results as
with models {\tt cc.we} and {\tt cch.LY}. 

\end{enumerate} % [resume]




