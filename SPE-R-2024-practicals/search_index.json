[["index.html", "Statistical Practice in Epidemiology with R SPE practicals About", " Statistical Practice in Epidemiology with R SPE practicals Krista Fischer1 Martyn Plummer2 Janne Pitkäniemi3 Bendix Carstensen4 Damien Georges5 Esa Läärä6 3-7 June 2024 About This booklet contains a set of exercises to apply concepts learned during the SPE-R course. Estonian Genome Center, University of Tartu, Estonia, Krista.Fischer@ut.ee↩︎ University of Warwick, UK, martyn.plummer@warwick.ac.uk↩︎ Finnish Cancer Registry, Helsinki, Finland, janne.pitkaniemi@cancer.fi↩︎ Steno Diabetes Center Copenhagen, Gentofte, Denmark, Dept. of Biostatistics, University of Copenhagen, Denmark, b@bxc.dk, http://BendixCarstensen.com↩︎ International Agency for Research on Cancer, Lyon, France, georgesd@iarc.who.int↩︎ Department of Mathematical Sciences, University of Oulu, Finland, Esa.Laara@oulu.fi, http://www.oulu.fi/university/researcher/esa-laara↩︎ "],["practice-with-basic-r.html", "Chapter 1 Practice with basic R 1.1 The working directory 1.2 The workspace 1.3 Using R as a calculator 1.4 Vectors 1.5 Sequences 1.6 Displaying and changing parts of a vector (indexing) 1.7 Lists 1.8 Data frames 1.9 Working with built-in data frames 1.10 Referencing parts of the data frame (indexing) 1.11 Summaries 1.12 Generating new variables 1.13 Turning a variable into a factor 1.14 Frequency tables 1.15 Grouping the values of a numeric variable", " Chapter 1 Practice with basic R The main purpose of this session is to give participants who have not had much (or any) experience with using R a chance to practice the basics and to ask questions. For others, it should serve as a reminder of some of the basic features of the language. R can be installed on all major platforms (i.e. Windows, macOS, Linux). We do not assume in this exercise that you are using any particular platform. Many people like to use the RStudio graphical user interface (GUI), which gives the same look and feel across all platforms. 1.1 The working directory A key concept in R is the working directory (or folder in the terminology of Windows). The working directory is where R looks for data files that you may want to read in and where R will write out any files you create. It is a good idea to keep separate working directories for different projects. In this course we recommend that you keep a separate working directory for each exercise. If you are working on the command line in a terminal, then you can change to the correct working directory and then launch R by typing R. If you are using a GUI then you will typically need to change to the correct working directory after starting R. In RStudio, you can change directory from the Session menu. However it is much more useful to create a new project to keep your source files and data. When you open a project in the RStudio GUI, your working directory is automatically changed to the directory associated with the project. You can display the current working directory with the getwd() (get working directory) function and set it with the setwd() (set working directory) function. The function dir() can be used to see what files you have in the working directory. 1.2 The workspace You can quit R by typing q() at the R command prompt. You will be asked if you want to save your workspace. We strongly recommend that you answer no to this question. If you answer yes then R will write a file named .RData into the working directory containing all the objects you created during your session. This file will be automatically loaded the next time you start R and this will restore all the objects in your workspace. It may seem convenient to keep your R objects from one session to another. But this has many disadvantages. You may not remember how an object was created. This becomes a problem if you need to redo your analysis after the data has been changed or updated, or if you accidentally delete the object. An object might be modified or overwritten. In this case your analysis will give you a different answer but you will not know why. You may not even notice that the answer has changed. It becomes impossible to clean your workspace if you cannot remember which objects are required by which analyses. As a result, your workspace will become cluttered with old objects. We strongly recommend that you follow some basic principles of reproducible research. Always start with a clean (empty) workspace. Read in the data you need from a pristine source. Put your R commands in a script file so that they can be run again in a future session. All modifications to the data that you need to make should be done in R using a script and not by editing the data source. This way, if the original data is modified or updated then you can run the script again on the updated data. 1.3 Using R as a calculator Try using R as an interactive calculator by typing different arithmetic expressions on the command line. Pressing the return key on the command line finishes the expression. R will then evaluate the expression and print out the result. Note that R allows you to recall previous commands using the vertical arrow key. You can edit a recalled command and then resubmit it by pressing the return key. Keeping that in mind, try the following: 12 + 16 (12 + 16) * 5 sqrt((12 + 16) * 5) # square root round(sqrt((12 + 16) * 5), 2) # round to two decimal places The hash symbol # denotes the start of a comment. Anything after the hash is ignored by R. Round braces are used a lot in R. In the above expressions, they are used in two different ways. Firstly, they can be used to establish the order of operations. In the example (12 + 16) * 5 they ensure that 12 is added to 16 before the result is multiplied by 5. If you omit the braces then you get a different answer 12 + 16 * 5 because multiplication has higher precedence than addition. The second use of round braces is in a function call (e.g. sqrt, round). To call a function in R, type the name followed by the arguments inside round braces. Some functions take multiple arguments, and in this case they are separated by commas. You can see that complicated expressions in R can have several levels of nested braces. To keep track of these, it helps to use a syntax-highlighting editor. For example, in RStudio, when you type an opening bracket (, RStudio will automatically add a closing bracket ), and when the cursor moves past a closing bracket, RStudio will automatically highlight the corresponding opening bracket. Features like this can make it much easier to write R code free from syntax errors. Instead of printing the result to the screen, you can store it in an object, say a &lt;- round(sqrt((12 + 16) * 5), 2) In this case R does not print anything to the screen. You can see the results of the calculation, stored in the object a, by typing a and also use a for further calculations, e.g: exp(a) log(a) # natural logarithm log10(a) # log to the base 10 The left arrow expression &lt;-, pronounced gets, is called the assignment operator, and is obtained by typing &lt; followed by - (with no space in between). It is also possible to use the equals sign = for assignment. Note that object names in R are case sensitive. So you can assign different values to objects named A and a. 1.4 Vectors All commands in R are functions which act on objects. One important kind of object is a vector, which is an ordered collection of numbers, or character strings (e.g. Charles Darwin), or logical values (TRUE or FALSE). The components of a vector must be of the same type (numeric, character, or logical). The combine function c(), together with the assignment operator, is used to create vectors. Thus v &lt;- c(4, 6, 1, 2.2) creates a vector v with components 4, 6, 1, 2.2 and assigns the result to the vector v. A key feature of the R language is that many operations are vectorized, meaning that you can carry out the same operation on each element of a vector in a single operation. Try v 3 + v 3 * v and you will see that R understands what to do in each case. R extends ordinary arithmetic with the concept of a missing value represented by the symbol NA (Not Available). Any operation on a missing value creates another missing value. You can see this by repeating the same operations on a vector containing a missing value: v &lt;- c(4, 6, NA) 3 + v 3 * v The fact that every operation on a missing value produces a missing value can be a nuisance when you want to create a summary statistic for a vector: mean(v) While it is true that the mean of v is unknown because the value of the third element is missing, we normally want the mean of the non-missing elements. Fortunately the mean function has an optional argument called na.rm which can be used for this. mean(v, na.rm = TRUE) Many functions in R have optional arguments that can be omitted, in which case they take their default value (For example, the mean function has default na.rm=FALSE). You can explicitly values to optional arguments in the function call to override the default behaviour. You can get a description of the structure of any object using the function str(). For example, str(v) shows that v is numeric with 4 components. If you just want to know the length of a vector then it is much easier to use the length function. length(v) 1.5 Sequences There are short-cut functions for creating vectors with a regular structure. For example, if you want a vector containing the sequence of integers from 1 to 10, you can use 1:10 The seq() function allows the creation of more general sequences. For example, the vector (15, 20, 25, … ,85) can be created with seq(from = 15, to = 85, by = 5) The objects created by the : operator and the seq() function are ordinary vectors, and can be combined with other vectors using the combine function: c(5, seq(from = 20, to = 85, by = 5)) You can learn more about functions by typing ? followed by the function name. For example ?seq gives information about the syntax and usage of the function seq(). Create a vector w with components 1, -1, 2, -2 Display this vector Obtain a description of w using str() Create the vector w+1, and display it. Create the vector v with components (5, 10, 15, … , 75) using seq(). Now add the components 0 and 1 to the beginning of v using c(). Find the length of this vector. 1.6 Displaying and changing parts of a vector (indexing) Square brackets in R are used to extract parts of vectors. So x[1] gives the first element of vector x. Since R is vectorized you can also supply a vector of integer index values inside the square brackets. Any expression that creates an integer vector will work. Try the following commands: x &lt;- c(2, 7, 0, 9, 10, 23, 11, 4, 7, 8, 6, 0) x[4] x[3:5] x[c(1, 5, 8)] Trying to extract an element that is beyond the end of the vector is, surprisingly, not an error. Instead, this returns a missing value N &lt;- length(x) x[N + 1] There is a reason for this behaviour, which we will discuss in the recap. R also allows logical subscripting. Try the following x &gt; 10 x[x &gt; 10] The first expression creates a logical vector of the same length as x, where each element has the value TRUE or FALSE depending on whether or not the corresponding element of x is greater than 10. If you supply a logical vector as an index, R selects only those elements for which the conditions is TRUE. You can combine two logical vectors with the operators &amp; (logical and) and | (logical or). For example, to select elements of x that are between 10 and 20 we combine two one-sided logical conditions for \\(x \\geq 10\\) and \\(x \\leq 20\\): x[x &gt;= 10 &amp; x &lt;= 20] The remaining elements of x that are either less than 10 or greater than 20 are selected with x[x &lt; 10 | x &gt; 20] Indexing can also be used to replace parts of a vector: x[1] &lt;- 1000 x This replaces the first element of x. Logical subscripting is useful for replacing parts of a vector that satisfy a certain condition. For example to replace all elements that take the value 0 with the value 1: x[x == 0] &lt;- 1 x If you want to replace parts of a vector then you need to make sure that the replacement value is either a single value, as in the example above, or a vector equal in length to the number of elements to be replaced. For example, to replace elements 2, 3, and 4 we need to supply a vector of replacement values of length 3. x[2:4] &lt;- c(0, 8, 1) x It is important to remember this when you are using logical subscripting because the number of elements to be replaced is not given explicitly in the R code, and it is easy to get confused about how many values need to be replaced. If we want to add 3 to every element that is less than 3 then we can break the operation down into 3 steps: y &lt;- x[x &lt; 3] y &lt;- y + 3 x[x &lt; 3] &lt;- y x First we extract the values to be modified, then we modify them, then we write back the modified values to the original positions. R experts will normally do this in a single expression. x[x &lt; 3] &lt;- x[x &lt; 3] + 3 Remember, if you are confused by a complicated expression you can usually break it down into simpler steps. If you want to create an entirely new vector based on some logical condition then use the ifelse() function. This function takes three arguments: the first is a logical vector; the second is the value taken by elements of the logical vector that are TRUE; and the third is the value taken by elements that are FALSE. In this example, we use the remainder operator %% to identify elements of x that have value 0 when divided by 2 (i.e. the even numbers) and then create a new character vector with the labels even and odd: x %% 2 ifelse(x %% 2 == 0, &quot;even&quot;, &quot;odd&quot;) Now try the following: Display elements that are less than 10, but greater than 4 Modify the vector x, replacing by 10 all values that are greater than 10 Modify the vector x, multiplying by 2 all elements that are smaller than 5 (Remember you can do this in steps). 1.7 Lists Collections of components of different types are called lists, and are created with the list() function. Thus m &lt;- list(4, TRUE, &quot;name of company&quot;) m creates a list with 3 components: the first is numeric, the second is logical and the third is character. A list element can be any object, including another list. This flexibility means that functions that need to return a lot of complex information, such as statistical modelling functions, often return a list. As with vectors, single square brackets are used to take a subset of a list, but the result will always be another list, even if you select only one element m[1:2] # A list containing first two elements of m m[3] # A list containing the third element of m If you just want to extract a single element of a list then you must use double square braces: m[[3]] # Extract third element Lists are more useful when their elements are named. You can name an element by using the syntax name=value in the call to the list function: mylist &lt;- list( name = c(&quot;Joe&quot;, &quot;Ann&quot;, &quot;Jack&quot;, &quot;Tom&quot;), age = c(34, 50, 27, 42) ) mylist This creates a new list with the elements name, a character vector of names, and age a numeric vector of ages. The components of the list can be extracted with a dollar sign $ mylist$name mylist$age 1.8 Data frames Data frames are a special structure used when we want to store several vectors of the same length, and corresponding elements of each vector refer to the same record. For example, here we create a simple data frame containing the names of some individuals along with their age in years, their sex (coded 1 or 2) and their height in cm. mydata &lt;- data.frame( name = c(&quot;Joe&quot;, &quot;Ann&quot;, &quot;Jack&quot;, &quot;Tom&quot;), age = c(34, 50, 27, 42), sex = c(1, 2, 1, 1), height = c(185, 170, 175, 182) ) The construction of a data frame is just like a named list (except that we use the constructor function data.frame instead of list). In fact data frames are also lists so, for example, you can extract vectors using the dollar sign: mydata$height On the other hand, data frames are also two dimensional objects: mydata When you print a data frame, each variable appears in a separate column. You can use square brackets with two comma-separated arguments to take subsets of rows or columns. mydata[1, ] mydata[, c(&quot;age&quot;, &quot;height&quot;)] mydata[2, 4] We will look into indexing of data frames in more detail below. Now let’s create another data frame with more individuals than the first one: yourdata &lt;- data.frame( name = c(&quot;Ann&quot;, &quot;Peter&quot;, &quot;Sue&quot;, &quot;Jack&quot;, &quot;Tom&quot;, &quot;Joe&quot;, &quot;Jane&quot;), weight = c(67, 81, 56, 90, 72, 79, 69) ) This new data frame contains the weights of the individuals. The two data sets can be joined together with the merge function. newdata &lt;- merge(mydata, yourdata) newdata The merge function uses the variables common to both data frames – in this case the variable name – to uniquely identify each row. By default, only rows that are in both data frames are preserved, the rest are discarded. In the above example, the records for Peter, Sue, and Jane, which are not in mydata are discarded. If you want to keep them, use the optional argument all=TRUE. newdata &lt;- merge(mydata, yourdata, all = TRUE) newdata This keeps a row for all individuals but since Peter, Sue and Jane have no recorded age, height, or sex these are missing values. 1.9 Working with built-in data frames We shall use the births data which concern 500 mothers who had singleton births (i.e. no twins) in a large London hospital. The outcome of interest is the birth weight of the baby, also dichotomised as normal or low birth weight. These data are available in the Epi package: library(Epi) data(births) objects() The function objects() shows what is in your workspace. To find out a bit more about births try help(births) The dataframe \"diet\" in the Epi package contains data from a follow-up study with coronary heart disease as the end-point. Load these data with data(diet) and print the contents of the data frame to the screen.. - Check that you now have two objects, births, and diet in your workspace. - Get help on the object diet. - Remove the object diet with the command remove(diet) Check that the object diet is no longer in your workspace. 1.10 Referencing parts of the data frame (indexing) Typing births will list the entire data frame – not usually very helpful. You can use the head function to see just the first few rows of a data frame head(births) Now try births[1, ] This will list all the values for the first row. Similarly, births[2, ] will list the value taken by the second row, and so on. To list the data for the first 10 subjects, try births[1:10, ] Often we want to extract rows of a data frame based on a condition. To select all subjects with height less than 180 cm from the data frame mydata we can use the subset() function. subset(mydata, height &lt; 180) 1.11 Summaries A good way to start an analysis is to ask for a summary of the data by typing summary(births) This prints some summary statistics (minimum, lower quartile, mean, median, upper quartile, maximum). For variables with missing values, the number of NAs is also printed. To see the names of the variables in the data frame try names(births) Variables in a data frame can be referred to by name, but to do so it is necessary also to specify the name of the data frame. Thus births$hyp refers to the variable hyp in the births data frame, and typing births$hyp will print the data on this variable. To summarize the variable hyp try summary(births$hyp) Alternatively you can use with(births, summary(hyp)) 1.12 Generating new variables New variables can be produced using assignment together with the usual mathematical operations and functions. For example logbw &lt;- log(births$bweight) produces the variable logbw in your workspace, while births$logbw &lt;- log(births$bweight) produces the variable logbw in the births data frame. You can also replace existing variables. For example bweight measures birth weight in grams. To convert the units to kilograms we replace the original variable with a new one: births$bweight &lt;- births$bweight / 1000 1.13 Turning a variable into a factor In R categorical variables are known as factors, and the different categories are called the levels of the factor. Variables such as hyp and sex are originally coded using integer codes, and by default R will interpret these codes as numeric values taken by the variables. Factors will become very important later in the course when we study modelling functions, where factors and numeric variables are treated very differently. For the moment, you can think of factors as value labels that are more informative than numeric codes. For R to recognize that the codes refer to categories it is necessary to convert the variables to be factors, and to label the levels. To convert the variable hyp to be a factor, try births$hyp &lt;- factor(births$hyp, labels = c(&quot;normal&quot;, &quot;hyper&quot;)) This takes the original numeric codes (0, 1) and replaces them with informative labels normal’’ and ``hyper for normal blood pressure and hypertension, respectively. Convert the variable sex into a factor with labels \"M\" and \"F\" for values 1 and 2, respectively 1.14 Frequency tables When starting to look at any new data frame the first step is to check that the values of the variables make sense and correspond to the codes defined in the coding schedule. For categorical variables (factors) this can be done by looking at one-way frequency tables and checking that only the specified codes (levels) occur. The most useful function for making simple frequency tables is table. The distribution of the factor hyp can be viewed using with(births, table(hyp)) or by specifying the data frame as in table(births$hyp) For simple expressions the choice is a matter of taste, but with is shorter for more complicated expressions. Find the frequency distribution of sex. If you give two or more arguments to the table function then it produces cross-tabulations. Find the two-way frequency distribution of sex and hyp. Create a logical variable called early according to whether gestwks is less than 30 or not. Make a frequency table of early. 1.15 Grouping the values of a numeric variable For a numeric variable like matage it is often useful to group the values and to create a new factor which codes the groups. For example we might cut the values taken by matage into the groups 20–29, 30–34, 35–39, 40–44, and then create a factor called agegrp with 4 levels corresponding to the four groups. The best way of doing this is with the function cut. Try births$agegrp &lt;- cut( births$matage, breaks = c(20, 30, 35, 40, 45), right = FALSE ) with(births, table(agegrp)) By default the factor levels are labelled [20-25), [25-30), etc., where [20-25) refers to the interval which includes the left hand end (20) but not the right hand end (25). This is the reason for right=FALSE. When right=TRUE (which is the default) the intervals include the right hand end but not the left hand. Observations which are not inside the range specified by the breaks argument result in missing values for the new factor. Hence it is important that the first element in breaks is smaller than the smallest value in your data, and the last element is larger than the largest value. Summarize the numeric variable gestwks, which records the length of gestation for the baby, and make a note of the range of values. Create a new factor gest4 which cuts gestwks at 20, 35, 37, 39, and 45 weeks, including the left hand end, but not the right hand. Make a table of the frequencies for the four levels of gest4. "],["reading-data-into-r.html", "Chapter 2 Reading data into R 2.1 Introduction 2.2 Data sources 2.3 Data in text files 2.4 Things that can go wrong 2.5 Forgetting the headers 2.6 Using the wrong separator 2.7 Mis-specifying the representation of missing values 2.8 Spreadsheet data 2.9 Reading data from the Internet 2.10 Reading from the clipboard 2.11 Binary data 2.12 Summary", " Chapter 2 Reading data into R 2.1 Introduction If you want to have rabbit stew, first catch the rabbit – Old saying, origin unknown R is a language and environment for data analysis. If you want to do something interesting with it, you need data. For teaching purposes, data sets are often embedded in R packages. The base R distribution contains a whole package dedicated to data which includes around 100 data sets. This is attached towards the end of the search path, and you can see its contents with objects(&quot;package:datasets&quot;) A description of all of these objects is available using the help() function. For example help(Titanic) gives an explanation of the Titanic data set, along with references giving the source of the data. The Epi package also contains some data sets. These are not available automatically when you load the Epi package, but you can make a copy in your workspace using the data() function. For example library(Epi) data(bdendo) will create a data frame called bdendo in your workspace containing data from a case-control study of endometrial cancer. Datasets in the Epi package also have help pages: type help(bdendo) for further information. To go back to the cooking analogy, these data sets are the equivalent of microwave ready meals, carefully packaged and requiring minimal work by the consumer. Your own data will never be able in this form and you must work harder to read it in to R. This exercise introduces you to the basics of reading external data into R. It consists of reading the same data from different formats. Although this may appear repetitive, it allows you to see the many options available to you, and should allow you to recognize when things go wrong. getting the data You will need to download the zip file data.zip from the course web site (https://github.com/SPE-R/SPE/raw/gh-spe-material/SPE-all-material.zip) and unpack this in your working directory. This will create a sub-directory data containing (among other things) the files fem.dat, fem-dot.dat, fem.csv, and fem.dta (Reminder: use setwd() to set your working directory). 2.2 Data sources Sources of data can be classified into three groups: Data in human readable form, which can be inspected with a text editor. Data in binary format, which can only be read by a program that understands that format (SAS, SPSS, Stata, Excel, …). Online data from a database management system (DBMS) This exercise will deal with the first two forms of data. Epidemiological data sets are rarely large enough to justify being kept in a DBMS. If you want further details on this topic, you can consult the R Data Import/Export manual that comes with R. 2.3 Data in text files Human-readable data files are generally kept in a rectangular format, with individual records in single rows and variables in columns. Such data can be read into a data frame in R. Before reading in the data, you should inspect the file in a text editor and ask three questions: How are columns in the table separated? How are missing values represented? Are variable names included in the file? The file fem.dat contains data on 118 female psychiatric patients. The data set contains nine variables. Name Description ID Patient identifier AGE Age in years IQ Intelligence Quotient (IQ) score ANXIETY Anxiety (1=none, 2=mild, 3=moderate,4=severe) DEPRESS Depression (1=none, 2=mild, 3=moderate or severe) SLEEP Sleeping normally (1=yes, 2=no) SEX Lost interest in sex (1=yes, 2=no) LIFE Considered suicide (1=yes, 2=no) WEIGHT Weight change (kg) in previous 6 months Inspect the file fem.dat with a text editor to answer the questions above. The most general function for reading in free-format data is read.table(). This function reads a text file and returns a data frame. It tries to guess the correct format of each variable in the data frame (integer, double precision, or text). Read in the table with: fem &lt;- read.table(&quot;./data/fem.dat&quot;, header = TRUE) Note that you must assign the result of read.table() to an object. If this is not done, the data frame will be printed to the screen and then lost. You can see the names of the variables with names(fem) The structure of the data frame can be seen with str(fem) You can also inspect the top few rows with head(fem) Note that the IQ of subject 9 is -99, which is an illegal value: nobody can have a negative IQ. In fact -99 has been used in this file to represent a missing value. In R the special value NA (Not Available) is used to represent missing values. All R functions recognize NA values and will handle them appropriately, although sometimes the appropriate response is to stop the calculation with an error message. You can recode the missing values with fem$IQ[fem$IQ == -99] &lt;- NA Of course it is much better to handle special missing value codes when reading in the data. This can be done with the na.strings argument of the read.table() function. See below. 2.4 Things that can go wrong Sooner or later when reading data into R, you will make a mistake. The frustrating part of reading data into R is that most mistakes are not fatal: they simply cause the function to return a data frame that is not what you wanted. There are three common mistakes, which you should learn to recognize. 2.5 Forgetting the headers The first row of the file fem.dat contains the variable names. The read.table() function does not assume this by default so you have to specify this with the argument header=TRUE. See what happens when you forget to include this option: fem2 &lt;- read.table(&quot;data/fem.dat&quot;) str(fem2) head(fem2) and compare the resulting data frame with fem. What are the names of the variables in the data frame? What is the class of the variables? Explanation: Remember that read.table() tries to guess the mode of the variables in the text file. Without the header=TRUE option it reads the first row, containing the variable names, as data, and guesses that all the variables are character, not numeric. By default, all character variables are coerced to factors by read.table. The result is a data frame consisting entirely of factors. (You can prevent the conversion of character variables to factors with the argument as.is=TRUE). If the variable names are not specified in the file, then they are given default names V1, V2, … . You will soon realise this mistake if you try to access a variable in the data frame by, for example fem2$IQ as the variable will not exist There is one case where omitting the header=TRUE option is harmless (apart from the situation where there is no header line, obviously). When the first row of the file contains one less value than subsequent lines, read.table() infers that the first row contains the variable names, and the first column of every subsequent row contains its row name. 2.6 Using the wrong separator By default, read.table assumes that data values are separated by any amount of white space. Other possibilities can be specified using the sep argument. See what happens when you assume the wrong separator, in this case a tab, which is specified using the escape sequence \"\\t\" fem3 &lt;- read.table(&quot;data/fem.dat&quot;, sep = &quot;\\t&quot;) str(fem3) How many variables are there in the data set? Explanation: If you mis-specify the separator, read.table() reads the whole line as a single character variable. Once again, character variables are coerced to factors, so you get a data frame with a single factor variable. 2.7 Mis-specifying the representation of missing values The file fem-dot.dat contains a version of the FEM dataset in which all missing values are represented with a dot. This is a common way of representing missing values, but is not recognized by default by the read.table() function, which assumes that missing values are represented by NA. Inspect the file with a text editor, and then see what happens when you read the file in incorrectly: fem4 &lt;- read.table(&quot;data/fem-dot.dat&quot;, header = TRUE) str(fem4) You should have enough clues by now to work out what went wrong. You can read the data correctly using the na.strings argument fem4 &lt;- read.table( &quot;data/fem-dot.dat&quot;, header = TRUE, na.strings = &quot;.&quot; ) 2.8 Spreadsheet data Spreadsheets have become a common way of exchanging data. All spreadsheet programs can save a single sheet in comma-separated variable (CSV) format, which can then be read into R. There are two functions in R for reading in CSV data: read.csv() and read.csv2(). Both of these are wrappers around the read.table() function, i.e. the read.table() function is still doing the work of reading in the data but the read.csv() function provides default argument values for reading in CSV file so all you need to do is specify the file name. You can see what these default arguments are with the args() function. args(read.csv) args(read.csv2) See if you can spot the difference between read.csv and read.csv2. Explanation: The CSV format is not a single standard. The file format depends on the locale of your computer – the settings that determine how numbers are represented. In some countries, the decimal separator is a point . and the variable separator in a CSV file is a comma ,. In other countries, the decimal separator is a comma , and the variable separator is a semi-colon ;. This is reflected in the different default values for the arguments sep and dec. The read.csv() function is used for the first format and the read.csv2() function is used for the second format. The file fem.csv contains the FEM dataset in CSV format. Inspect the file to work out which format is used, and read it into R. 2.9 Reading data from the Internet You can also read in data from a remote web site. The file argument of read.table() does not need to be a local file on your computer; it can be a Uniform Resource Locator (URL), i.e. a web address. A copy of the file fem.dat is held at (https://www.bendixcarstensen.com/SPE/data/fem.dat). You can read it in with fem6 &lt;- read.table( &quot;http://www.bendixcarstensen.com/SPE/data/fem.dat&quot;, header = TRUE ) str(fem6) 2.10 Reading from the clipboard On Microsoft Windows, you can copy values directly from an open Excel spreadsheet using the clipboard. Highlight the cells you want to copy in the spread sheet and select copy from the pull-down edit menu. Then type read.table(file=\"clipboard\") to read the data in. There are two reasons why this is a bad idea It is not reproducible. In order to read the data in again you need to complete exactly the same sequence of mouse moves and clicks, and there is no record of what you did before. Copying from the clipboard loses precision. If you have a value 1.23456789 in your spreadsheet, but have formatted the cell so it is displayed to two decimal places, then the value read into R will be the truncated value 1.23. 2.11 Binary data The foreign package allows you to read data in binary formats used by other statistical packages. Since R is an open source project, it can only read binary formats that are themselves open, in the sense that the standards for reading and writing data are well-documented. For example, there is a function in the foreign package for reading SAS XPORT files, a format that has been adopted as a standard by the US Food and Drug Administration ((http://www.sas.com/govedu/fda/faq.html)). However, there is no function in the foreign package for reading native SAS binaries (SAS7BDAT files). Other packages are available from CRAN ((http://cran.r-project.org)) that offer the possibility of reading SAS binary files: see the haven and sas7bdat packages. The file fem.dta contains the FEM dataset in the format used by Stata. Read it into R with library(foreign) fem5 &lt;- read.dta(&quot;data/fem.dta&quot;) head(fem5) The Stata data set contains value and variable labels. Stata variables with value labels are automatically converted to factors. There is no equivalent of variable labels in an R data frame, but the original variable labels are not lost. They are still attached to the data frame as an invisible attribute, which you can see with attr(fem5, &quot;var.labels&quot;) A lot of meta-data is attached to the data in the form of attributes. You can see the whole list of attributes with attributes(fem5) or just the attribute names with names(attributes(fem5)) The read.dta() function can only read data from Stata versions 5–12. The R Core Team has not been able to keep up with changes in the Stata format. You may wish to try the haven package and the readstata13 package, both available from CRAN. 2.12 Summary In this exercise we have seen how to create a data frame in R from an external text file. We have also reviewed some common mistakes that result in garbled data. The capabilities of the foreign package for reading binary data have also been demonstrated with a sample Stata data set. "],["data-manipulation-with-tidyverse.html", "Chapter 3 Data manipulation with tidyverse 3.1 Introduction 3.2 The births data 3.3 tibble vs data.frame 3.4 Piping functions 3.5 mutate columns 3.6 select columns, filter and arrange rows 3.7 group_by and summarise data 3.8 Multiple grouping 3.9 Bind and join tables 3.10 Data Visualization with ggplot2 3.11 pivoting data with tidyr 3.12 reading files with readr 3.13 String manipulation with stringr 3.14 purrr package to apply functions to list 3.15 Bonus: Rendering tables", " Chapter 3 Data manipulation with tidyverse 3.1 Introduction In this chapter we will produce more or less the same outputs than in chapter 1.2 and 1.4 using tidyverse packages framework. The main objective of this exercise is to get familiar you with some of the main tidyverse features. This is an optional practical for participants having already good basic R skills. &gt; All the rest of the course can be done without knowledge of tidyverse. 3.2 The births data We will work with births data-set from Epi package. First of all, load the Epi and tidyverse packages. Then load the births data-set. library(Epi) suppressPackageStartupMessages(library(tidyverse)) data(births) You can type ?birth in the R console to get a description of the birth data-set. (Alternatively, you can refer to chapter 1.3.2) 3.3 tibble vs data.frame Most dplyr functions outputs return tibble object instead of data.frame. Inspect the class and characteristics of the births object. class(births) head(births) Note: As any R object this can be summarized using str function. str(births) births object is a 500 x 8 data.frame. Let’s convert births to tibble format with as_tibble function. births_tbl &lt;- as_tibble(births) class(births_tbl) births_tbl # another way to visualize data set is to use glimpse function glimpse(births_tbl) You can see that tibble objects inherits from data.frame which implies that all functions working with data.frame objects will work with tibble. The opposite is not necessary true. tibble has a couple of extra features compared to classical data.frame. One of them is a slightly more user-friendly console print. The main difference is probably that tibble objects supports grouping/nesting features. Some examples we be done will see latter on. 3.4 Piping functions This is one of the most popular features of tidyverse grammar. It enables function chaining in R. Function output is transparently passed as input to the next function and so on. It can help to make the code more comprehensive and readable. Here is an example of classic vs piped functions. head(births, 4) births |&gt; head(4) Note: By default the chained object is given as the first argument to the following function. You can use . if this is not the case. This trick works only with dplyr pipe %&gt;% Here is a dummy example where we do not give the first argument to head function but the second one. 4 %&gt;% head(births, .) Which could be also be achieve with base pipe |&gt; with a slightly more complex syntax. 4 |&gt; (\\(.) head(births, .))() # Note the extra parentheses 3.5 mutate columns mutate will allow you to add and or modify columns in a tibble. Let’s create 2 new variables : agegrp (5 years mother’s age group) gest4 (gestation time split in 4 categories) And modify 2 others: hyp (factor version of hyp; normal vs hyper) sex (factor version of sex; M vs F) births_tbl &lt;- births_tbl |&gt; mutate( # modify hyp varible (conversion into factor) hyp = factor( hyp, levels = c(0, 1), labels = c(&quot;normal&quot;, &quot;hyper&quot;) ), # creating a new variable aggrep agegrp = cut( matage, breaks = c(20, 25, 30, 35, 40, 45), right = FALSE ), # modify sex variable (conversion into factor) sex = factor( sex, levels = c(1, 2), labels = c(&quot;M&quot;, &quot;F&quot;) ), # creating a new variable gest4 with case_when instead of cut gest4 = case_when( gestwks &lt; 25 ~ &quot;less than 25 weeks&quot;, gestwks &gt;= 25 &amp; gestwks &lt; 30 ~ &quot;25-30 weeks&quot;, gestwks &gt;= 30 &amp; gestwks &lt; 35 ~ &quot;30-35 weeks&quot;, gestwks &gt;= 35 ~ &quot;more than 35 weeks&quot; ) ) births_tbl You can see as header the type of data contained in each column. For instance &lt;dbl&gt; stands for double (i.e. numeric value) and fct stands for factor. In R data.frame (/ tibble) data type must be the same within a column (e.g. numeric only) but can be of different type across columns. (note: matrix object supports only one type of data) Note that case_when function do not return a factor but a character variable in this case. You will have to force the conversion from character to factor if needed. 3.6 select columns, filter and arrange rows select is used for column sub-setting while filter is for row sub-setting. They are equivalent to the [] in R base language. Let’s display a table where only babies’ id, sex, bweight and mothers’ agegrp are kept for babies with a bweight above 4000g. births_tbl |&gt; # select only id, women age group, sex # and birth weight of the baby select(id, agegrp, sex, bweight) |&gt; # keep only babies weighing more than 4000g filter(bweight &gt; 4000) select can also be useful to reorder and rename columns. arrange is a nice feature to reorder observations according to chosen attributes. Let’s rename agegrp, sex and bweight with better looking labels (e.g. Age group, Sex, Birth weight) and reorder the table according to babies’ decreasing birth weight. births_tbl |&gt; # select only id, women age group, sex # and birth weight of the baby select( id, &quot;Age group&quot; = agegrp, Sex = sex, &quot;Birth weight&quot; = bweight ) |&gt; # rearrange rows to put the heaviest newborn on top arrange(desc(`Birth weight`)) Note: tibble supports blank spaces in the column names which can be handy for final table rendering. When you want to work with columns with blank spaces, do not forget to use the \\(``\\) (back-quote). Try to produce the same table but arranging the rows by decreasing birth weights within each sex. births_tbl |&gt; # select only id, women age group, sex # and birth weight of the baby select( id, &quot;Age group&quot; = agegrp, Sex = sex, &quot;Birth weight&quot; = bweight ) |&gt; # rearrange rows to put the heaviest newborn on top arrange(Sex, desc(`Birth weight`)) You can arrange the tibble according to more than one column. 3.7 group_by and summarise data One greatest features of dplyr is the ability to aggregate data sharing a common attribute to process per group operations. Here we want to compute the number of boys and girls in the data-set. The idea here is to split the births table in two groups. One with the boys, the other with the girls and to count the number of rows in each group. births.01 &lt;- births_tbl |&gt; # group the data according to the sex attribute group_by(sex) |&gt; # count the number of rows/individuals in each group summarise( count = n() ) births.01 Note: n function is equivalent to nrow Now we have the number of boys and girls, we can compute the distribution (in percentage) of newborns per sex. births.02 &lt;- births.01 |&gt; mutate( percent = count / sum(count) * 100 ) Trick: most of dplyr functions can be combined with a column selection execution statement using across function. This can be very handy in some cases. As an example below a code to compute the sum of every birth.02 numerical columns (numerical columns only) births.03 &lt;- births_tbl |&gt; select(gest4, sex, gestwks, bweight, matage) |&gt; group_by(gest4, sex) |&gt; summarise( across( where(is.numeric), ~ mean(.x, na.rm = TRUE) ), .groups = &quot;drop&quot; ) births.03 across function supports the purrr-style lambda format, e.g. ~ mean(.x, na.rm = TRUE) where .x refers to the values from the data set to be passed to the function. This is a common notation you will find across several tidyverse functions. Some other functions ending by _with can be used conditionally within dplyr. As an example we can rename only columns which are not numeric at once (here we want to code all column names using upper characters) using the combination of rename_with and where. births.03 |&gt; rename_with(toupper, where(~ !is.numeric(.x))) Let’s now compute the number of births and the mean birth weight according to newborn gender. births.05 &lt;- births_tbl |&gt; group_by(sex) |&gt; summarise( count = n(), bweight.mean = mean(bweight) ) births.05 With births.05 table, compute the global mean birth weight. Note: with such a table the mean baby’s birth weight have to be weighted by number of boys and girls (see. ?weighted.mean). births.05 |&gt; summarise( count.tot = sum(count), bweight.mean.tot = weighted.mean(bweight.mean, count) ) # this is equivalent to births_tbl |&gt; summarise( count.tot = n(), bweight.mean.tot = mean(bweight) ) 3.8 Multiple grouping In some cases, we can be interested in looking at more than a single strata. This can be achieved using multiple grouping. Let’s count the number of births per gender and birth weight class (low vs not low) births.06 &lt;- births_tbl |&gt; group_by(sex, lowbw) |&gt; summarise( count = n() ) ## `summarise()` has grouped output by &#39;sex&#39;. You can override using ## the `.groups` argument. births.06 Try then to compute the percentage of babies in each group. Look at the difference between the 2 following command lines: births.06 |&gt; mutate( percent = count / sum(count) * 100 ) births.06 |&gt; ungroup() |&gt; mutate( percent = count / sum(count) * 100 ) Are the results the same? Note: summarizing a data-set will remove the last level of grouping but not the other ones if multiple grouping has been performed. In some cases you might have to explicitly ungroup your data.frame before doing further calculations. In the previous examples, if you do not ungroup the data-set, percentages are computed per gender. Ungrouping will let you compute the overall percentages. Trick: a good practice is to always ungroup the summarized dataset in order to prevent form confusion. You can do it using the .group = 'drop' option in summarize(). # this tibble will still be grouped by sex births_tbl |&gt; group_by(sex, lowbw) |&gt; summarise( count = n() ) # this tibble will be group free births_tbl |&gt; group_by(sex, lowbw) |&gt; summarise( count = n(), .groups = &quot;drop&quot; ) The same exercise can be done using gestation time group (gest4) as stratifying variable. Lets compute number of births and mean birth weights according to gestation time category. births_tbl |&gt; group_by(gest4) |&gt; summarise( count = n(), bweight.mean = mean(bweight) ) Any trend? It seems that birth weight increases with gestation time. We can also spot that in our data-set the gestation time is missing for 10 newborns. We will do not consider this observation for the rest of the exercise. Lets cross-tabulate the birth weight category and the gestation time groups. births_tbl |&gt; # keep only the newborn with defined gesational time category filter( !is.na(gest4) ) |&gt; group_by(lowbw, gest4) |&gt; # compute the number of babies in each cross category summarise( count = n() ) |&gt; # compute the percentage of babies in each gestational # time category per birth weight category mutate( percent = count / sum(count, na.rm = TRUE) ) ## `summarise()` has grouped output by &#39;lowbw&#39;. You can override ## using the `.groups` argument. Similarly we can be interested in the birth weight distribution per gestational time. births_tbl |&gt; filter( !is.na(gest4) ) |&gt; group_by(gest4, lowbw) |&gt; summarise( count = n() ) |&gt; # compute the percentage of babies in each birth weight category # per gestational time category mutate( percent = count / sum(count, na.rm = TRUE) ) ## `summarise()` has grouped output by &#39;gest4&#39;. You can override ## using the `.groups` argument. Note: grouping order matters! and can be confusing so think about ungrouping intermediate tables. 3.9 Bind and join tables Another nice feature of dplyr is tables binding and joining. To practice we will create two tibbles: age an individual database which contains pid (unique individuals id) and their age in year center an study center database which contains pid (unique individuals id) and center (the center where an individual is registered coded as a letter) age &lt;- tibble( pid = 1:6, age = sample(15:25, size = 6, replace = TRUE) ) center &lt;- tibble( pid = c(1, 2, 3, 4, 10), center = c(&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;) ) age center Now the tables are define we will try to make the linkage between individuals ages and the center they belong to. First of all let’s have a look to bind_rows function. bind_rows(age, center) Is it useful? Here not really because we do not want to bind the data-set (but join them instead) but that can be in other situations (e.g. several individuals data base to merge..). Note: in bind_rows, if columns names do not match, they are fill with NA. Here we want to join the 2 tibble according to their common attribute pid. Depending on the context you can be interested in joining tables differently. Have a look at the differences between left_join, full_join and inner_join. # all individuals from ages are kept left_join(age, center, by = c(&quot;pid&quot;)) # everithing is kept full_join(age, center, by = c(&quot;pid&quot;)) # only the individuals present in both dataset are kept inner_join(age, center, by = c(&quot;pid&quot;)) Can you spot the differences between the commands above? As an exercise, you can try to compute the individuals’ mean age per center. inner_join(age, center, by = c(&quot;pid&quot;)) |&gt; group_by(center) |&gt; summarise( mean_age = mean(age) ) Note: the by argument indicates which column should be use to make the join. In some cases, you might have to uses several columns to match (e.g. per sex and age group), this can be easily done specifying a vector of column names. From now on, we will consider other packages than dplyr from the tidyverse suits. 3.10 Data Visualization with ggplot2 One of the package that have contributed to tidyverse success is for sure ggplot2. We will go more into the details on how to produce advanced graphs with ggplot2 in another practical. Let’s just have a quick example of graphic creation using ggplot2. Let’s draw a bar plot to visualize the number of births by women age group. First you have to create a table with the number of birth per age group. birth_per_ageg &lt;- births_tbl |&gt; group_by(agegrp) |&gt; summarise(total_births = n()) (gg.01 &lt;- ggplot(birth_per_ageg, aes(x = agegrp, y = total_births)) + geom_bar(stat = &quot;identity&quot;)) This graph can be customize adding labels and title to the plot: (gg.02 &lt;- gg.01 + xlab(&quot;Women Age Group&quot;) + ylab(&quot;Total Births&quot;) + ggtitle(&quot;Number of Births per Women Age Group&quot;)) As you can see, plots from ggplot family are built incrementally using the + operator for each additional element. 3.11 pivoting data with tidyr dplyr often comes with its good friend tidyr when we are performing data manipulation. tidyr main features is to reshape tables from long to wide format and vis-versa. Let’s have an example. Let’s transform in wide format the previously created birth_per_ageg table. We want to have a table with one column per age group containing the total_births numbers. birth_per_ageg birth_per_ageg_wide &lt;- birth_per_ageg |&gt; pivot_wider( names_from = &quot;agegrp&quot;, values_from = &quot;total_births&quot; ) birth_per_ageg_wide This table can easily be formatted back in long format using pivot_longer function: birth_per_ageg_long &lt;- birth_per_ageg_wide |&gt; pivot_longer( cols = 1:5, names_to = &quot;agegrp&quot;, values_to = &quot;total_births&quot; ) birth_per_ageg_long Are the tables birth_per_ageg and birth_per_ageg_long identical? identical(birth_per_ageg, birth_per_ageg_long) Not really because the factor type of agegrp column has been lost during the transformation. Let’s convert agegrp column into a factor. Is the new table identical to birth_per_ageg ? birth_per_ageg_long_02 &lt;- birth_per_ageg_long |&gt; mutate(agegrp = as.factor(agegrp)) identical(birth_per_ageg, birth_per_ageg_long_02) Here we have seen the simplest example you can have of table reshaping with tidyr. If you are interested check the dedicated vignette (vignette(\"pivot\")) to learn how to perform more advanced tables reshaping. 3.12 reading files with readr Another package from tidyverse that can be introduced here is readr that contains a set of functions equivalent to the core R data.frame reading functions (e.g. read.table(), read.csv(), read.delim(), …). The main change is that data are loaded in R as tibble instead of data.frame, type of variables (columns) are guessed if possible, and some extra data checking tests are performed. Let’s explore this differences with fem dataset available in data directory. # read a csv using core R fem.csv.core &lt;- read.csv(&quot;data/fem.csv&quot;) # read a csv using tidyverse fem.csv.tidy &lt;- read_csv(&quot;data/fem.csv&quot;) ## Rows: 118 Columns: 9 ## ── Column specification ────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (9): ID, AGE, IQ, ANXIETY, DEPRESS, SLEEP, SEX, LIFE, WEIGHT ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # compare fem.csv.core fem.csv.tidy # table dimensions dim(fem.csv.core) dim(fem.csv.tidy) # compare column types map(fem.csv.core, class) map(fem.csv.tidy, class) note: in case you do not fully get the last lines and the map() call, it will be explained in the next section on purrr package. Here we see that the only difference is the type of object loaded data.frame vs tibble and the default type chosen to cast numeric values (integer vs numeric). What about loading occoh.txt you will be using in some other practical in the coming days. # read a csv using core R occoh.txt.core &lt;- read.table(&quot;data/occoh.txt&quot;) # read a csv using tidyverse occoh.txt.tidy &lt;- read_table(&quot;data/occoh.txt&quot;) ## ## ── Column specification ────────────────────────────────────────── ## cols( ## id = col_double(), ## birth = col_double(), ## entry = col_date(format = &quot;&quot;), ## exit = col_date(format = &quot;&quot;), ## death = col_date(format = &quot;&quot;), ## chdeath = col_double() ## ) ## Warning: 1501 parsing failures. ## row col expected actual file ## 1 -- 6 columns 7 columns &#39;data/occoh.txt&#39; ## 2 -- 6 columns 7 columns &#39;data/occoh.txt&#39; ## 3 -- 6 columns 7 columns &#39;data/occoh.txt&#39; ## 4 -- 6 columns 7 columns &#39;data/occoh.txt&#39; ## 5 -- 6 columns 7 columns &#39;data/occoh.txt&#39; ## ... ... ......... ......... ................ ## See problems(...) for more details. occoh.txt.tidy &lt;- read_table(&quot;data/occoh.txt&quot;) ## ## ── Column specification ────────────────────────────────────────── ## cols( ## id = col_double(), ## birth = col_double(), ## entry = col_date(format = &quot;&quot;), ## exit = col_date(format = &quot;&quot;), ## death = col_date(format = &quot;&quot;), ## chdeath = col_double() ## ) ## Warning: 1501 parsing failures. ## row col expected actual file ## 1 -- 6 columns 7 columns &#39;data/occoh.txt&#39; ## 2 -- 6 columns 7 columns &#39;data/occoh.txt&#39; ## 3 -- 6 columns 7 columns &#39;data/occoh.txt&#39; ## 4 -- 6 columns 7 columns &#39;data/occoh.txt&#39; ## 5 -- 6 columns 7 columns &#39;data/occoh.txt&#39; ## ... ... ......... ......... ................ ## See problems(...) for more details. # compare occoh.txt.core occoh.txt.tidy # table dimensions dim(occoh.txt.core) dim(occoh.txt.tidy) # compare column types map(occoh.txt.core, class) map(occoh.txt.tidy, class) As you can see, in addition to inferring the type of columns in the input data (here some dates), using readr to load you data-set can help you to detect inconsistencies in input data formatting (there are no true problem here). If you are interested, you can explore the other functions of readr and see how you can tune it. 3.13 String manipulation with stringr Another popular tidyverse popular package is stringr package. This package is specialized in the string manipulation. Here are couple of examples. Let’s create a character vector with the following elements representing country names: “Estonia”, “Finland”, “Denmark”, “United Kingdom”, “France”. countries &lt;- c(&quot;Estonia&quot;, &quot;Finland&quot;, &quot;Denmark&quot;, &quot;United Kingdom&quot;, &quot;France&quot;) With stringr functions perform the following actions. Extract the first three characters from each country name: country_initials &lt;- str_sub(countries, start = 1, end = 3) Convert all country names to uppercase: countries_upper &lt;- str_to_upper(countries) Replace “United” with “Utd” in each country name: countries_modified &lt;- str_replace(countries, &quot;United&quot;, &quot;Utd&quot;) Find the positions of the letter “n” in each country name: a_positions &lt;- str_locate_all(countries, &quot;n&quot;) As you can see, the output of str_locate_all is a list (one element per character string) containing a 2 column table with one line for each match. The first column (start) being the position of the beginning of the match and the second one (end) being the end of the match. In our case, since we are searching for a single character match, this 2 indexes are always the same. Count the number of characters in each country name: character_counts &lt;- str_length(countries) These examples demonstrate various string manipulation operations using the stringr package. You can modify the exercises, combine several operations or explore other string manipulation functions provided by stringr to further practice and enhance your skills in manipulating and analyzing text data. 3.14 purrr package to apply functions to list Among my favorite tidyverse packages, you will find purrr. This package contains several functions that are very similar to lapply function. Apply a function to each element of the vector using map(). Here producing the mean of some grades per class: # define the grade dataset grades &lt;- list( c1 = c(80, 85, 90), c2 = c(75, 70, 85, 88), c3 = c(90, 85, 95) ) # compute grades mean_grades &lt;- map(grades, mean) By default map() return a list. One of the nice feature of purrr functions is to be able to specify the type of output you want (e.g. _dbl for numeric, _chr for characters, …). Check and try to explain the differences between the following command lines: map(grades, mean) map_dbl(grades, mean) map_chr(grades, mean) ## Warning: Automatic coercion from double to character was deprecated in ## purrr 1.0.0. ## ℹ Please use an explicit call to `as.character()` within ## `map_chr()` instead. ## Call `lifecycle::last_lifecycle_warnings()` to see where this ## warning was generated. map_df(grades, mean) Other nice features of map like functions is he availability to support more than one argument. map2() for 2 arguments and pmap() for more than 2. This can be very handy in some conditions. If you are interested you can have a look to this function help file and play with the examples. purrr package has also a set of functions that can be used to apply iteratively a function using reduce and/or accumulate. The 2 functions behave the same way, it takes the 2 first element of a list, apply a function taking 2 arguments. The results is combined with the third element of the list and given as input to the same function and so on.. The only difference is that accumulate return intermediate results while reduce return only the final results. Here an example of the cumulative product of the 10 first numbers. 1:10 |&gt; purrr::reduce(`*`) 1:10 |&gt; purrr::accumulate(`*`) purrr have many of others useful features. Please check the dedicated documentation if you want to go further with this package. 3.15 Bonus: Rendering tables Once you have produced a nice data-set we can be interested in rendering it in a nice format that can meet presentation/publication expectations. The kableExtra table can be useful to achieve this goal. # if(!require(kableExtra)) install.packages(&#39;kableExtra&#39;) library(kableExtra) births.08 &lt;- births_tbl |&gt; filter( !is.na(gest4) ) |&gt; group_by(gest4) |&gt; summarise( N = n() ) |&gt; mutate( `(%)` = (N / sum(N)) |&gt; scales::percent() ) # default births.08 # create an html version of the table and save it on the hard drive births.08 |&gt; kable() |&gt; kable_styling( bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;), full_width = FALSE ) |&gt; save_kable(file = &quot;births.08.html&quot;, self_contained = TRUE) note: One other very cool package to produce advance formatted Excel spreadsheet I am using more and more is openxlsx. Check it out if you are interested. "],["tabulation.html", "Chapter 4 Tabulation 4.1 Introduction 4.2 The births data 4.3 One-way tables 4.4 Improving the Presentation of Tables 4.5 Two-way Tables 4.6 Printing", " Chapter 4 Tabulation 4.1 Introduction R and its add-on packages provide several different tabulation functions with different capabilities. The appropriate function to use depends on your goal. There are at least three different uses for tables. The first use is to create simple summary statistics that will be used for further calculations in R. For example, a two-by-two table created by the table function can be passed to fisher.test, which will calculate odds ratios and confidence intervals. The appearance of these tables may, however, be quite basic, as their principal goal is to create new objects for future calculations. A quite different use of tabulation is to make production quality tables for publication. You may want to generate reports from for publication in paper form, or on the World Wide Web. The package xtable provides this capability, but it is not covered by this course. An intermediate use of tabulation functions is to create human-readable tables for discussion within your work-group, but not for publication. The Epi package provides a function stat.table for this purpose, and this practical is designed to introduce this function. 4.2 The births data We shall use the births data which concern 500 mothers who had singleton births in a large London hospital. The outcome of interest is the birth weight of the baby, also dichotomised as normal or low birth weight. These data are available in the Epi package: library(Epi) data(births) names(births) head(births) In order to work with this data set we need to transform some of the variables into factors. This is done with the following commands: births$hyp &lt;- factor(births$hyp, labels = c(&quot;normal&quot;, &quot;hyper&quot;)) births$sex &lt;- factor(births$sex, labels = c(&quot;M&quot;, &quot;F&quot;)) births$agegrp &lt;- cut( births$matage, breaks = c(20, 25, 30, 35, 40, 45), right = FALSE ) births$gest4 &lt;- cut( births$gestwks, breaks = c(20, 35, 37, 39, 45), right = FALSE ) Now use str(births) to examine the modified data frame. We have transformed the binary variables hyp and sex into factors with informative labels. This will help when displaying the tables. We have also created grouped variables agegrp and gest4 from the continuous variables matage and gestwks so that they can be tabulated. 4.3 One-way tables The simplest table one-way table is created by stat.table(index = sex, data = births) This creates a count of individuals, classified by levels of the factor sex. Compare this table with the equivalent one produced by the table function. Note that stat.table has a data argument that allows you to use variables in a data frame without specifying the frame. You can display several summary statistics in the same table by giving a list of expressions to the contents argument: stat.table( index = sex, contents = list(count(), percent(sex)), data = births ) Only a limited set of expressions are allowed: see the help page for stat.table for details. You can also calculate marginal tables by specifying margin=TRUE in your call to stat.table. Do this for the above table. Check that the percentages add up to 100 and the total for count() is the same as the number of rows of the data frame births. To see how the mean birth weight changes with sex, try stat.table(index = sex, contents = mean(bweight), data = births) Add the count to this table. Add also the margin with margin=TRUE. As an alternative to bweight we can look at lowbw with stat.table(index = sex, contents = percent(lowbw), data = births) All the percentages are 100! To use the percent function the variable lowbw must also be in the index, as in stat.table( index = list(sex, lowbw), contents = percent(lowbw), data = births ) The final column is the percentage of babies with low birth weight by different categories of gestation. Obtain a table showing the frequency distribution of gest4. Show how the mean birth weight changes with gest4. Show how the percentage of low birth weight babies changes with gest4. Another way of obtaining the percentage of low birth weight babies by gestation is to use the ratio function: stat.table(gest4, ratio(lowbw, 1, 100), data = births) This only works because lowbw is coded 0/1, with 1 for low birth weight. Tables of odds can be produced in the same way by using ratio(lowbw, 1-lowbw). The ratio function is also very useful for making tables of rates with (say) ratio(D,Y,1000) where D is the number of failures, and Y is the follow-up time. We shall return to rates in a later practical. 4.4 Improving the Presentation of Tables The stat.table function provides default column headings based on the contents argument, but these are not always very informative. Supply your own column headings using tagged lists as the value of the contents argument, within a stat.table call: stat.table(gest4, contents = list( N = count(), &quot;(%)&quot; = percent(gest4) ), data = births) This improves the readability of the table. It remains to give an informative title to the index variable. You can do this in the same way: instead of giving gest4 as the index argument to stat.table, use a named list: stat.table(index = list(&quot;Gestation time&quot; = gest4), data = births) 4.5 Two-way Tables The following call gives a \\(2\\times 2\\) table showing the mean birth weight cross-classified by sex and hyp. stat.table( list(sex, hyp), contents = mean(bweight), data = births ) Add the count to this table and repeat the function call using margin = TRUE to calculate the marginal tables. Use stat.table with the ratio function to obtain a \\(2\\times 2\\) table of percent low birth weight by sex and hyp. You can have fine-grained control over which margins to calculate by giving a logical vector to the margin argument. Use margin=c(FALSE, TRUE) to calculate margins over sex but not hyp. This might not be what you expect, but the margin argument indicates which of the index variables are to be marginalized out, not which index variables are to remain. 4.6 Printing Just like every other R function, stat.table produces an object that can be saved and printed later, or used for further calculation. You can control the appearance of a table with an explicit call to print() There are two arguments to the print method for stat.table. The width argument which specifies the minimum column width, and the digits argument which controls the number of digits printed after the decimal point. This table odds.tab &lt;- stat.table( gest4, list(&quot;odds of low bw&quot; = ratio(lowbw, 1 - lowbw)), data = births ) print(odds.tab) shows a table of odds that the baby has low birth weight. Use width=15 and digits=3 and see the difference. "],["graphics-in-r.html", "Chapter 5 Graphics in R 5.1 Simple plot on the screen 5.2 Colours 5.3 Adding to a plot 5.4 Using indexing for plot elements 5.5 Generating colours 5.6 Saving your graphs for use in other documents 5.7 Interacting with a plot", " Chapter 5 Graphics in R There are three kinds of plotting functions in R: Functions that generate a new plot, e.g. hist() and plot(). Functions that add extra things to an existing plot, e.g. lines() and text(). Functions that allow you to interact with the plot, e.g. locator() and identify(). The normal procedure for making a graph in R is to make a fairly simple initial plot and then add on points, lines, text etc., preferably in a script. 5.1 Simple plot on the screen Load the births data and get an overview of the variables: library(Epi) data(births) str(births) Now look at the birth weight distribution with hist(births$bweight) The histogram can be refined – take a look at the possible options with help(hist) and try some of the options, for example: hist(births$bweight, col = &quot;gray&quot;, border = &quot;white&quot;) To look at the relationship between birthweight and gestational weeks, try with(births, plot(gestwks, bweight)) You can change the plot-symbol by the option pch=. If you want to see all the plot symbols try: plot(1:25, pch = 1:25) Make a plot of the birth weight versus maternal age with with(births, plot(matage, bweight)) - Label the axes with with( births, plot( matage, bweight, xlab = &quot;Maternal age&quot;, ylab = &quot;Birth weight (g)&quot; ) ) 5.2 Colours There are many colours recognized by R. You can list them all by colours() or, equivalently, colors() (R allows you to use British or American spelling). To colour the points of birthweight versus gestational weeks, try with(births, plot(gestwks, bweight, pch = 16, col = &quot;green&quot;)) This creates a solid mass of colour in the centre of the cluster of points and it is no longer possible to see individual points. You can recover this information by overwriting the points with black circles using the points() function. with(births, plot(gestwks, bweight, pch = 16, col = &quot;green&quot;)) with(births, points(gestwks, bweight, pch = 1)) Note: when the number of data points on a scatter plot is large, you may also want to decrease the point size: to get points that are 50% of the original size, add the parameter cex=0.5 (or another number &lt;1 for different sizes). 5.3 Adding to a plot The points() function just used is one of several functions that add elements to an existing plot. By using these functions, you can create quite complex graphs in small steps. Suppose we wish to recreate the plot of birthweight vs gestational weeks using different colours for male and female babies. To start with an empty plot, with type='n' argument. Then add the points with the points function. with(births, plot(gestwks, bweight, type = &quot;n&quot;)) with( births, points(gestwks[sex == 1], bweight[sex == 1], col = &quot;blue&quot;) ) with( births, points(gestwks[sex == 2], bweight[sex == 2], col = &quot;red&quot;) ) To add a legend explaining the colours, try with(births, plot(gestwks, bweight, type = &quot;n&quot;)) with( births, points(gestwks[sex == 1], bweight[sex == 1], col = &quot;blue&quot;) ) with( births, points(gestwks[sex == 2], bweight[sex == 2], col = &quot;red&quot;) ) legend( &quot;topleft&quot;, pch = 1, legend = c(&quot;Boys&quot;, &quot;Girls&quot;), col = c(&quot;blue&quot;, &quot;red&quot;) ) which puts the legend in the top left hand corner. Finally we can add a title to the plot with with(births, plot(gestwks, bweight, type = &quot;n&quot;)) with( births, points(gestwks[sex == 1], bweight[sex == 1], col = &quot;blue&quot;) ) with( births, points(gestwks[sex == 2], bweight[sex == 2], col = &quot;red&quot;) ) legend( &quot;topleft&quot;, pch = 1, legend = c(&quot;Boys&quot;, &quot;Girls&quot;), col = c(&quot;blue&quot;, &quot;red&quot;) ) title( &quot;Birth weight vs gestational weeks in 500 singleton births&quot; ) 5.4 Using indexing for plot elements One of the most powerful features of R is the possibility to index vectors, not only to get subsets of them, but also for repeating their elements in complex sequences. Putting separate colours on males and female as above would become very clumsy if we had a 5 level factor instead of sex. Instead of specifying one color for all points, we may specify a vector of colours of the same length as the gestwks and bweight vectors. This is rather tedious to do directly, but R allows you to specify an expression anywhere, so we can use the fact that sex takes the values 1 and 2, as follows: First create a colour vector with two colours, and take look at sex: c(&quot;blue&quot;, &quot;red&quot;) births$sex Now see what happens if you index the colour vector by sex: c(&quot;blue&quot;, &quot;red&quot;)[births$sex] For every occurrence of a 1 in sex you get \"blue\", and for every occurrence of 2 you get \"red\", so the result is a long vector of \"blue\"s and \"red\"s corresponding to the males and females. This can now be used in the plot: with( births, plot(gestwks, bweight, pch = 16, col = c(&quot;blue&quot;, &quot;red&quot;)[sex]) ) The same trick can be used if we want to have a separate symbol for mothers over 40 say. We first generate the indexing variable: births$oldmum &lt;- (births$matage &gt;= 40) + 1 Note we add 1 because ( matage &gt;= 40 ) generates a logic variable, so by adding 1 we get a numeric variable with values 1 and 2, suitable for indexing: with( births, plot( gestwks, bweight, pch = c(16, 3)[oldmum], col = c(&quot;blue&quot;, &quot;red&quot;)[sex] ) ) so where oldmum is 1 we get pch=16 (a dot) and where oldmum is 2 we get pch=3 (a cross). R will accept any kind of complexity in the indexing as long as the result is a valid index, so you don’t need to create the variable oldmum, you can create it on the fly: with( births, plot( gestwks, bweight, pch = c(16, 3)[(matage &gt;= 40) + 1], col = c(&quot;blue&quot;, &quot;red&quot;)[sex] ) ) 5.5 Generating colours R has functions that generate a vector of colours for you. For example, rainbow(4) produces a vector with 4 colours (not immediately human readable, though). There are a few other functions that generates other sequences of colours, type ?rainbow to see them. The color function (or colour function if you prefer) returns a vector of the colour names that R knows about. These names can also be used to specify colours. Gray-tones are produced by the function gray (or grey), which takes a numerical argument between 0 and 1; gray(0) is black and gray(1) is white. Try: plot(0:10, pch = 16, cex = 3, col = gray(0:10 / 10)) points(0:10, pch = 1, cex = 3) 5.6 Saving your graphs for use in other documents If you need to use the plot in a report or presentation, you can save it in a graphics file. Once you have generated the script (sequence of R commands) that produce the graph (and it looks ok on screen), you can start a non-interactive graphics device and then re-run the script. Instead of appearing on the screen, the plot will now be written directly to a file. After the plot has been completed you will need to close the device again in order to be able to access the file. Try: pdf(file = &quot;bweight_gwks.pdf&quot;, height = 4, width = 4) with(births, plot(gestwks, bweight, col = c(&quot;blue&quot;, &quot;red&quot;)[sex])) legend( &quot;topleft&quot;, pch = 1, legend = c(&quot;Boys&quot;, &quot;Girls&quot;), col = c(&quot;blue&quot;, &quot;red&quot;) ) dev.off() This will give you a portable document file bweight_gwks.pdf with a graph which is 4 inches tall and 4 inches wide. Instead of pdf, other formats can be used (jpg, png, tiff, …). See help(Devices) for the available options. In window-based environments (R GUI for Windows, R-Studio) you may also use the menu (File\\(\\rightarrow\\)Save as ... or Export) to save the active graph as a file and even copy-paste may work (from R graphics window to Word, for instance) – however, writing it manually into the file is recommended for reproducibility purposes (in case you need to redraw your graph with some modifications). It is possible to manipulate any element in a graph, by using the graphics options. These are collected on the help page of par(). For example, if you want axis labels always to be horizontal, use the command par(las=1). This will be in effect until a new graphics device is opened. Look at the typewriter-version of the help-page with help(par) or better, use the the html-version through Help \\(\\rightarrow\\) Html help \\(\\rightarrow\\) Packages \\(\\rightarrow\\) graphics \\(\\rightarrow\\) P\\(\\rightarrow\\) par. It is a good idea to take a print of this (having set the text size to smallest because it is long) and carry it with you at any time to read in buses, cinema queues, during boring lectures etc. Don’t despair, few R-users can understand what all the options are for. par() can also be used to ask about the current plot, for example par(\"usr\") will give you the exact extent of the axes in the current plot. If you want more plots on a single page you can use the command par(mfrow = c(2, 3)) This will give you a layout of 2 rows by 3 columns for the next 6 graphs you produce. The plots will appear by row, i.e. in the top row first. If you want the plots to appear columnwise, use par( mfcol=c(2,3) ) (you still get 2 rows by 3 columns). To restore the layout to a single plot per page use par(mfrow = c(1, 1)) If you want a more detailed control over the layout of multiple graphs on a single page look at ?layout. 5.7 Interacting with a plot The locator() function allows you to interact with the plot using the mouse. Typing locator(1) shifts you to the graphics window and waits for one click of the left mouse button. When you click, it will return the corresponding coordinates. You can use locator() inside other graphics functions to position graphical elements exactly where you want them. Recreate the birth-weight plot, with(births, plot(gestwks, bweight, col = c(&quot;blue&quot;, &quot;red&quot;)[sex])) and then add the legend where you wish it to appear by typing legend( locator(1), pch = 1, legend = c(&quot;Boys&quot;, &quot;Girls&quot;), col = c(&quot;blue&quot;, &quot;red&quot;) ) The identify() function allows you to find out which records in the data correspond to points on the graph. Try with(births, identify(gestwks, bweight)) When you click the left mouse button, a label will appear on the graph identifying the row number of the nearest point in the data frame births. If there is no point nearby, R will print a warning message on the console instead. To end the interaction with the graphics window, right click the mouse: the identify function returns a vector of identified points. Use identify() to find which records correspond to the smallest and largest number of gestational weeks and view the corresponding records: with(births, births[identify(gestwks, bweight), ]) "],["analysis-of-hazard-rates-their-ratios-and-differences-and-binary-regression.html", "Chapter 6 Analysis of hazard rates, their ratios and differences and binary regression 6.1 Hand calculations for a single rate 6.2 Poisson model for a single rate with logarithmic link 6.3 Poisson model for a single rate with identity link 6.4 Poisson model assuming the same rate for several periods 6.5 Analysis of rate ratio 6.6 Analysis of rate difference 6.7 Binary regression 6.8 Optional/Homework: Hand calculations and calculations using matrix tools", " Chapter 6 Analysis of hazard rates, their ratios and differences and binary regression This exercise is very prescriptive, so you should make an effort to really understand everything you type into R. Consult the relevant slides of the lecture on Poisson and Binary regression … 6.1 Hand calculations for a single rate Let \\(\\lambda\\) be the true hazard rate or theoretical incidence rate of a given outcome event. Its estimator is the empirical incidence rate \\(\\widehat\\lambda = D/Y\\) = no. cases/person-years. Recall that the standard error of the empirical rate is SE\\((\\widehat\\lambda) = \\widehat\\lambda/\\sqrt{D}\\). The simplest approximate 95% confidence interval (CI) for \\(\\lambda\\) is given by \\[ \\widehat\\lambda \\pm 1.96 \\times SE(\\widehat\\lambda) \\] Suppose \\(15\\) outcome events are observed during \\(5532\\) person-years in a given study cohort. Let’s use R as a simple desk calculator to estimate the underlying hazard rate \\(\\lambda\\) (in 1000 person-years; therefore 5.532) and to get the first version of an approximate confidence interval: 6.2 Poisson model for a single rate with logarithmic link You are able to estimate the hazard rate \\(\\lambda\\) and compute its CI with a Poisson regression model, as described in the relevant slides in the lecture handout. Poisson regression is a generalized linear model in which the family, i.e. the distribution of the response variable, is assumed to be the Poisson distribution. The most commonly applied link function in Poisson regression is the natural logarithm; log for short. A family object poisreg, a modified version of the original poisson family object, is available in package Epi. When using this, the response is defined as a matrix of two columns: numbers of cases \\(D\\) and person-years \\(Y\\), these being combined into a matrix by cbind(D,Y). No specification of offset is needed. If you want confidence interval for log rate In this course we endorse the use of family poisreg because of its advantages in more general settings. 6.3 Poisson model for a single rate with identity link The approach leaning on having the number of cases \\(D\\) as the response and log\\((Y)\\) as an offset, is limited only to models with log link. A major advantage of the poisreg family is that it allows a straighforward use of the identity link, too. With this link the response variable is the same, but the parameters to be directly estimated are now the rates itself and their differences, not the log-rates and their differences as with the log link. Fit a Poisson model with identity link to our simple data, and use ci.lin() to produce the estimate and the confidence interval for the hazard rate from this model: How is the coefficient of this model interpreted? Verify that you actually get the same rate estimate and CI as in section 1.6.1, item 1. 6.4 Poisson model assuming the same rate for several periods Now, suppose the events and person years are collected over three distinct periods. Read in the data and compute period-specific rates Using these data, fit the same model with log link as in section 1.6.2, assuming a common single hazard \\(\\lambda\\) for the separate periods. Compare the result from the previous ones Now test whether the rates are the same in the three periods: Try to fit a model with the period as a factor in the model: Compare the goodness-of-fit of the two models using anova() with the argument test=\"Chisq\": Compare the test statistic to the deviance of the model mp. – What is the deviance indicating? 6.5 Analysis of rate ratio We now switch to comparison of two rates \\(\\lambda_1\\) and \\(\\lambda_0\\), i.e. the hazard in an exposed group vs. that in an unexposed one. Consider first estimation of the hazard ratio or the underlying true rate ratio \\(\\rho = \\lambda_1/\\lambda_0\\) between the groups. Suppose we have pertinent empirical data (cases and person-times) from both groups, \\((D_1,Y_1)\\) and \\((D_0,Y_0)\\). The point estimate of \\(\\rho\\) is the empirical incidence rate ratio \\[ \\widehat{\\rho} = RR = \\frac{\\widehat\\lambda_1}{\\widehat\\lambda_0} = \\frac{D_1/Y_1}{D_0/Y_0} \\] Suppose you have \\(15\\) events during \\(5532\\) person-years in an unexposed group and \\(28\\) events during \\(4783\\) person-years in an exposed group: Calculate the incidence rates in the two groups, their ratio, and the CI of the true hazard ratio \\(\\rho\\) by direct application of the above formulae: Now achieve this using a Poisson model. For that we first combine the group-specific numbers into pertinent vectors and specify a factor to represent the contrast between the exposed and the unexposed group What do the parameters mean in this model? You can extract the estimation results for exponentiated parameters in two ways, as before: 6.6 Analysis of rate difference For the hazard difference \\(\\delta = \\lambda_1 - \\lambda_0\\), the natural estimator is the incidence rate difference \\[ \\widehat\\delta = \\widehat\\lambda_1 - \\widehat\\lambda_0 = D_1/Y_1 - D_0/Y_0 = \\mbox{RD} . \\] Its variance is just the sum of the variances of the two rates \\[ var(RD) = var(\\widehat\\lambda_1 ) + var( \\widehat\\lambda_0 ) = D_1/Y_1^2 + D_0/Y_0^2 \\] Use this formula to compute the point estimate of the rate difference \\(\\lambda\\) and a 95% confidence interval for it: Verify that this is the confidence interval you get when you fit an additive model (obtained by identity link) with exposure as a factor: 6.7 Binary regression Explore the factors associated with risk of low birth weight in 500 singleton births in a London Hospital. Indicator (lowbw) for birth weight less than 2500 g. Data available from the Epi package. Factors of interest are maternal hypertension (hyp), mother’s age at birth over 35 years and sex of the baby. Load the Epi package and the data set and look at its content Because all variables are numeric we need first to do a little housekeeping. Two of them are directly converted into factors, and categorical versions are created of two continuous variables by function cut(). Cross tabulate (dplyr) counts of children by hypertension and low birth weight. calculate (mutate) proportions of low birth weigth children by hypertension. Estimate relative risk of low birth weight for mothers with hypertension compared to those without using binary regression. Adjust relative risk of low birth and hypertension with the sex of children Adjust relative risk of low birth and hypertension with the sex of children and mother beeing over 35 years. 6.8 Optional/Homework: Hand calculations and calculations using matrix tools NB. This subsection requires some familiarity with matrix algebra. Do this only after you have done the other exercises of this session. First some basic hand calculations. Suppose \\(15\\) outcome events are observed during \\(5532\\) person-years in a given study cohort. Let’s use R as a simple desk calculator to estimate the underlying hazard rate \\(\\lambda\\) (in 1000 person-years; therefore 5.532) and to get the first version of an approximate confidence interval: Compute now the approximate confidence interval using the method based on log-transformation and compare the result with that in the previous item. Calculate the incidence rates in the two groups, their ratio, and the CI of the true hazard ratio \\(\\rho\\) by direct application of the above formulae: Explore the function ci.mat(), which lets you use matrix multiplication (operator '%*%' in R) to produce a confidence interval from an estimate and its standard error (or CIs from whole columns of estimates and SEs): As you see, this function returns a \\(2\\times 3\\) matrix (2 rows, 3 columns) containing familiar numbers. When you combine the single rate and its standard error into a row vector of length 2, i.e. a \\(1\\times 2\\) matrix, and multiply this by the \\(2\\times 3\\) matrix above, the computation returns a \\(1\\times 3\\) matrix containing the point estimate and the confidence limit. Apply this method to the single rate calculations in 1.6.1, first creating the \\(1 \\times 2\\) matrix and then performing the matrix multiplication. When the confidence interval is based on the log-rate and its standard error, the result is obtained by appropriate application of the exp-function on the pertinent matrix product For computing the rate ratio and its CI as in 1.6.5, matrix multiplication with ci.mat() should give the same result as there: The main argument in function ci.mat() is alpha, which sets the confidence level: \\(1 - \\alpha\\). The default value is alpha = 0.05, corresponding to the level \\(1 - 0.05\\) = 95%. If you wish to get the confidence interval for the rate ratio at the 90% level (= \\(1-0.1\\)), for instance, you may proceed as follows: Now achieve this using a Poisson model. For that we first combine the group-specific numbers into pertinent vectors and specify a factor to represent the contrast between the exposed and the unexposed group Look again to the model used to analyse the rate ratio in. Often one would like to get simultaneously both the rates and the ratio between them. This can be achieved in one go using the contrast matrix argument ctr.mat to ci.lin() or ci.exp(). Try: Use the same machinery to the additive model to get the rates and the rate-difference in one go. Note that the annotation of the resulting estimates are via the column-names of the contrast matrix. "],["estimation-of-effects-simple-and-more-complex.html", "Chapter 7 Estimation of effects: simple and more complex 7.1 Response and explanatory variables 7.2 Data set births 7.3 Simple estimation with effx(), lm() and glm() 7.4 confirms this (\\(3032.8-3229.9=-197.1\\)). 7.5 Factors on more than two levels 7.6 Stratified effects, and interaction or effect-measure modification 7.7 Controlling or adjusting for the effect of hyp for sex 7.8 Numeric exposures 7.9 Checking the assumptions of the linear model 7.10 Penalized spline model 7.11 Analysis of binary outcomes", " Chapter 7 Estimation of effects: simple and more complex This exercise deals with analysis of metric and binary response variables. We start with simple estimation of effects of a binary, categorical or a numeric explanatory variable, the explanatory or exposure variable of interest. Then evaluation of potential modification and/or confounding by other variables is considered by stratification by and adjustment/control for these variables. Use of function effx() for such tasks is introduced together with functions lm() and glm() that can be used for more general linear and generalized linear models. Finally, more complex spline modelling for the effect of a numeric exposure variable is illustrated. 7.1 Response and explanatory variables Identifying the response or outcome variable correctly is the key to analysis. The main types are: Metric or continuous (a measurement with units). Binary (yes vs. no, coded 1/0), or proportion. Failure in person-time, or incidence rate. All these response variable are numeric. Variables on which the response may depend are called explanatory variables or regressors. They can be categorical factors or numeric variables. A further important aspect of explanatory variables is the role they will play in the analysis. Primary role: exposure. Secondary role: confounder and/or effect-measure modifier. The word effect is used here as a general term referring to ways of contrasting or comparing the expected values of the response variable at different levels of an explanatory variable. The main comparative measures or effect measures are: Differences in means for a metric response. Ratios of odds for a binary response. Ratios of rates for a failure or count response. Other kinds of contrasts between exposure groups include (a) ratios of geometric means for positive-valued metric outcomes, (b) differences and ratios between proportions (risk difference and risk ratio), and (c) differences between incidence or mortality rates. Note that in spite of using the causally loaded word effect, we treat outcome regression modelling here primarily with descriptive or predictive aims in mind. Traditionally, these types of models have also been used to estimate causal effects of exposure variables from the pertinent regression coefficients. More serious causal analysis is introduced in the lecture and practical on Saturday afternoon, and modern approaches to estimate causal effects will be considered on Tuesday afternoon. 7.2 Data set births We shall use the births data to illustrate different aspects in estimating effects of various exposures on a metric response variable bweight = birth weight, recorded in grams. Load the packages needed in this exercise and the data set, and look at its content We perform similar housekeeping tasks as in the previous exercise. Have a look at univariate summaries of the different variables in the data; especially the location and dispersion of the distribution of bweight. 7.3 Simple estimation with effx(), lm() and glm() We are ready to analyze the effect of sex on bweight. A binary explanatory variable, like sex, leads to an elementary two-group comparison of group means for a metric response. Comparison of two groups is commonly done by the conventional \\(t\\)-test and the associated confidence interval. The \\(P\\)-value refers to the test of the null hypothesis that there is no effect of sex on birth weight (quite an uninteresting null hypothesis in itself!). However, t.test() does not provide the point estimate for the effect of sex; only the test result and a confidence interval. The function effx() in Epi is intended to introduce the estimation of effects in epidemiology, together with the related ideas of stratification and controlling, i.e. adjustment for confounding, without the need for familiarity with statistical modelling. It is in fact a wrapper of function glm() that fits generalized linear models. Now, let’s do the same analysis with effx() The estimated effect of sex on birth weight, measured as a difference in means between girls and boys, is \\(-197\\) g. Either the output from t.test() above or the command 7.4 confirms this (\\(3032.8-3229.9=-197.1\\)). The same task can easily be performed by lm() or by glm(). The main argument in both is the model formula, the left hand side being the response variable and the right hand side after \\(\\sim\\) defines the explanatory variables and their joint effects on the response. Here the only explanatory variable is the binary factor sex. With glm() one specifies the family, i.e. the assumed distribution of the response variable, but in case you use lm(), this argument is not needed, because lm() fits only models for metric responses assuming Gaussian distribution. Note the amount of output that summary() method produces. The point estimate plus confidence limits can, though, be concisely obtained by function ci.lin() found in Epi package. Now, use effx() to find the effect of hyp (maternal hypertension) on bweight. 7.5 Factors on more than two levels The variable gest4 became as the result of cutting gestwks into 4 groups with left-closed and right-open boundaries [20,35) [35,37) [37,39) [39,45). We shall find the effects of gest4 on the metric response bweight. There are now 3 effect estimates: [35,37) vs [20,35) 857 [37,39) vs [20,35) 1360 [39,45) vs [20,35) 1668 The command confirms that the effect of gest4 (level~2 vs level~1) is \\(2590-1733=857\\), etc. Compute these estimates by lm() and find out how the coefficients are related to the group means 7.6 Stratified effects, and interaction or effect-measure modification We shall now examine whether and to what extent the effect of hyp on bweight, i.e. the mean difference between hypertensive and normotensive mothers, varies by gest4 without assigning causal interpretation to the estimated contrasts. The following interaction plot shows how the mean bweight depends jointly on hyp and gest4 It appears that the mean difference in bweight between hypertensive and normotensive mothers is related to gestational age. Let us get numerical values for the mean differences in the different gest4 categories: The estimated effects of hyp in the different strata defined by gest4 thus range from about \\(-100\\) g among those with \\(\\geq 39\\) weeks of gestation to about \\(-700\\) g among those with \\(&lt; 35\\) weeks of gestation. The error margin especially around the latter estimate is quite wide, though. The \\(P\\)-value 0.055 from the test for effect(-measure) modification indicates weak evidence against the null hypothesis of no interaction between hyp and gest4. On the other hand, this test may well be not very sensitive given the small number of preterm babies in these data. - Stratified estimation of effects can also be done by lm(), and you should get the same results: An equivalent model with an explicit product term or interaction term between gest4 and hyp is fitted as follows From this output you would find a familiar estimate \\(-673\\) g for those \\(&lt; 35\\) gestational weeks. The remaining coefficients are estimates of the interaction effects such that e.g. \\(515 = -158 - (-673)\\) g describes the contrast in the effect of hyp on bweight between those 35 to \\(&lt; 37\\) weeks and those \\(&lt; 35\\) weeks of gestation. Perhaps a more appropriate reference level for the categorized gestational age would be the highest one. Changing the reference level, here to be the 4th category, can be done by Relevel() function in the Epi package, after which an equivalent interaction model is fitted, now using a shorter expression for it in the model formula: Notice now the coefficient \\(-91.6\\) for hyp. It estimates the contrast \"hyper\" vs. \"normal\" on bweight among those with \\(\\geq 39\\) weeks of gestation. The estimate \\(-88.5\\) g = \\(-180.1 -(-91.6)\\) g describes the additional effect of hyp in the category 37 to 38 weeks of gestation upon that in the reference class. At this stage it is interesting to compare the results from the interaction models to those from the corresponding main effects model, in which the effect of hyp is assumed not to be modified by gest4: The estimate \\(-201\\) g describing the overall contrast between hypertensive and normotensive mothers is obtained as a weighted average of the stratum-specific estimates that were got by effx() above. This assumption or the no interaction null hypothesis can formally be tested by a common deviance test. The \\(P\\)-value is practically the same as before, when the interaction was tested in effx(). However, in spite of obtaining a non-significant result from this test, the possibility of a real effect-measure modification should not be ignored in this case. Now, use effx() to stratify (i) the effect of hyp on bweight by sex and then (ii) perform the stratified analysis using the two ways of fitting an interaction model with lm. Look at the results. Is there evidence for the effect of hyp being modified by sex? 7.7 Controlling or adjusting for the effect of hyp for sex The effect of hyp is controlled for – or adjusted for – sex by first looking at the estimated effects of hyp in the two stata defined by sex, and then combining these effects if they seem sufficiently similar. In this case the estimated effects were \\(-496\\) and \\(-380\\) which look quite similar (and the \\(P\\)-value against no interaction was quite large, too), so we can perhaps combine them, and control for sex. The combining is done by declaring sex as a control variable: The same is done with lm() as follows: The estimated effect of hyp on bweight controlled for sex is thus \\(-448\\) g. There can be more than one control variable, e.g control=list(sex,maged). Many people go straight ahead and control for variables which are likely to confound the effect of exposure without bothering to stratify first, but usually it is useful to stratify first. 7.8 Numeric exposures If we wished to study the effect of gestation time on the baby’s birth weight then gestwks is a numeric exposure variable. Assuming that the relationship of the response with gestwks is roughly linear (for a continuous response), % or log-linear (for a binary or failure rate response) we can estimate the linear effect of gestwks, both with effx() and with lm() as follows: We have fitted a simple linear regression model and obtained estimates of the two regression coefficient: intercept and slope. The linear effect of gestwks is thus estimated by the slope coefficient, which is 197 g per each additional week of gestation. The linear effect of gestwks on the log-odds of lowbw can be estimated similarly: The linear effect of gestwks on the log-odds of lowbw is manifested as a reduction by a factor of 0.408 per extra week of gestation, i.e. the odds of a baby having a low birth weight is reduced by a factor of 0.408 per one week increase in gestation. You cannot stratify by a numeric variable, but you can study the effects of a numeric exposure stratified by (say) maged with You can control/adjust for a numeric variable by putting it in the control list. 7.9 Checking the assumptions of the linear model At this stage it will be best to make some visual check concerning our model assumptions using plot(). In particular, when the main argument for the generic function plot() is a fitted lm object, it will provide you some common diagnostic graphs. To check whether bweight goes up linearly with gestwks try Moreover, take a look at the basic diagnostic plots for the fitted model. What can you say about the agreement with data of the assumptions of the simple linear regression model, like linearity of the systematic dependence, homoskedasticity and normality of the error terms? 7.10 Penalized spline model We shall now continue the analysis such that the apparently curved effect of gestwks is modelled by a penalized spline, based on the recommendations of Martyn in his lecture of this morning. You cannot fit a penalized spline model with lm() or glm(), Instead, function gam() in package mgcv can be used for this purpose. Make sure that you have loaded this package. When calling gam(), the model formula contains expression ‘s(X)’ for any explanatory variable X, for which you wish to fit a smooth function From the output given by summary() you find that the estimated intercept is equal to the overall mean birth weight in the data. The estimated residual variance is given by Scale est. or from subobject sig2 of the fitted gam object. Taking square root you will obtain the estimated residual standard deviation: 445.2 g. The degrees of freedom in this model are not computed as simply as in previous models, and they typically are not integer-valued. However, the fitted spline seems to consume only a little more degrees of freedom as an 3rd degree polynomial model would take. A graphical presentation of the fitted curve together with the confidence and prediction intervals is more informative. Let us first write a short function script to facilitate the task. We utilize function matshade() in Epi, which creates shaded areas, and function matlines() which draws lines joining the pertinent end points over the \\(x\\)-values for which the predictions are computed. Finally, create a vector of \\(x\\)-values and compute the fitted/predicted values as well as the interval limits at these points from the fitted model object utilizing function predict(). This function creates a matrix of three columns: (1) fitted/predicted values, (2) lower limits, (3) upper limits and make the graph: Compare this with the graph on slide 20 of the lecture we had. Are you happy with the end result? 7.11 Analysis of binary outcomes Instead of investigating the distribution and determinants of birth weight as such, it is common in perinatal epidemiology to consider occurrence of low birth weight; whether birth weight is \\(&lt; 2.5\\) kg or not. Variable lowbw with values 1 and 0 in the births data represents that dichotomy. Some analyses on lowbw were already conducted in the previous exercise. Here we illustrate further aspects of effect estimation and modelling binary outcome. We start with simple tabulation of the prevalence of lowbw by maternal hypertension It seems that the prevalence for hypertensive mothers is about 18 percent points higher, or about three times as high as that for normotensive mothers The three comparative measures of prevalences can be estimated by glm() with different link functions, whereas effx() gives only odds ratio: Check that these results were quite compatible with the about estimates given in the previous item. How well is the odds ratio approximating the risk ratio here? The prevalence of low birth weight is expected to be inversely related to gestational age (weeks), as is evident from simple tabulation Let’s jump right away to spline modelling of this relationship Inspect the output. Would you agree, that the logit of the prevalence of outcome is almost linearly dependent on gestwks? Encouraged by the result of the previous item, we continue the analysis with glm() and assuming logit-linearity Inspect the results. How do you interpret the estimated coefficients and their exponentiated values? - Instead of fitted logits, it can be more informative to plot the fitted prevalences against gestwks, in which we utilize the previously created data frame nd The curve seems to cover practically the whole range of the outcome probability scale with a relatively steep slope between about 33 to 37 weeks. As with numeric birth weight, it may be of interest, whether the effect of gestwks is modified by maternal hypertension, so let’s fit an interaction model and view the results How would you interpret the coefficients and their antilogarithms here? Even though there seems to be no sufficient evidence for effect-measure modification, it can be of interest to compare both the fitted lines on the logit scale and the fitted curves on the probability scale between the two groups. Function qlogis() returns the value of the logit transformation of the given argument. The logit-line starts from a higher level and its slope is steeper for the hypertensive mothers, which sounds reasonable. However, the two lines appear to cross at about 38 weeks. On the other hand, the vertical difference of the two probability curves appears discernible only in the area from about 32 to 38 weeks of gestation When interpreting these findings, one needs to keep in mind that the precision of these curves is very low, because of the small number of outcome cases overall. "],["poisson-regression-analysis-of-curved-effects.html", "Chapter 8 Poisson regression &amp; analysis of curved effects 8.1 Testis cancer: Data input and housekeeping 8.2 Some descriptive analysis 8.3 Age and period as categorical factors 8.4 Generalized additive model with penalized splines", " Chapter 8 Poisson regression &amp; analysis of curved effects This exercise deals with modelling incidence rates using Poisson regression. Our special interest is in estimating and reporting curved effects of continuous explanatory variables on the theoretical rate We analyse the testisDK data found in the Epi package. It contains the numbers of cases of testis cancer and mid-year populations (person-years) in 1-year age groups in Denmark during 1943-96. In this analysis age and calendar time are first treated as categorical but finally, a penalized spline model is fitted. 8.1 Testis cancer: Data input and housekeeping Load the packages and the data set, and inspect its structure: There are nearly 5000 observations from 90 one-year age groups and 54 calendar years. To get a clearer picture of what’s going on, we do some housekeeping. The age range will be limited to 15-79 years, and age and period are both categorized into 5-year intervals according to the time-honoured practice in epidemiology. 8.2 Some descriptive analysis Computation and tabulation of incidence rates Tabulate numbers of cases and person-years, and compute the incidence rates (per 100,000 y) in each 5 y \\(\\times\\) 5 y cell using stat.table() Look at the incidence rates in the column margin and in the row margin. In which age group is the marginal age-specific rate highest? Do the period-specific marginal rates have any trend over time? From the saved table object tab you can plot an age-incidence curve for each period separately, after you have checked the structure of the table, so that you know the relevant dimensions in it. There is a function rateplot() in Epi that does default plotting of tables of rates (see the help page of rateplot) Is there any common pattern in the age-incidence curves across the periods? 8.3 Age and period as categorical factors We shall first fit a Poisson regression model with log link on age and period model in the traditional way, in which both factors are treated as categorical. The model is additive on the log-rate scale. It is useful to scale the person-years to be expressed in \\(10^5\\) y. In fitting the model we utilize the poisreg family object found in package Epi. What do the estimated rate ratios tell about the age and period effects? A graphical inspection of point estimates and confidence intervals can be obtained as follows. In the beginning it is useful to define shorthands for the pertinent mid-age and mid-period values of the different intervals In the fitted model the reference category for each factor was the first one. As age is the dominating factor, it may be more informative to remove the intercept from the model. As a consequence the age effects describe fitted rates at the reference level of the period factor. For the latter one could choose the middle period 1968-72. We shall plot just the point estimates from the latter model 8.4 Generalized additive model with penalized splines It is obvious that the age effect on the log-rate scale is highly non-linear. Yet, it is less clear whether the true period effect deviates from linearity. Nevertheless, there are good reasons to try fitting smooth continuous functions for both time scales. As the next task we fit a generalized additive model for the log-rate on continuous age and period applying penalized splines with default settings of function gam() in package mgcv. In this fitting an optimal value for the penalty parameter is chosen based on an AIC-like criterion known as UBRE. The summary is quite brief, and the only estimated coefficient is the intercept, which sets the baseline level for the log-rates, against which the relative age effects and period effects will be contrasted. On the rate scale the baseline level 5.53 per 100000 y is obtained by exp(1.7096). See also the default plot for the fitted curves (solid lines) describing the age and the period effects which are interpreted as contrasts to the baseline level on the log-rate scale. The dashed lines describe the 95% confidence band for the pertinent curve. One could get the impression that year 1968 would be some kind of reference value for the period effect, like period 1968-72 chosen as the reference in the categorical model previously fitted. This is not the case, however, because gam() by default parametrizes the spline effects such that the reference level, at which the spline effect is nominally zero, is the overall grand mean value of the log-rate in the data. This corresponds to the principle of sum contrasts (contr.sum) for categorical explanatory factors. From the summary you will also find that the degrees of freedom value required for the age effect is nearly the same as the default dimension \\(k-1 = 9\\) of the part of the model matrix (or basis) initially allocated for each smooth function. (Here \\(k\\) refers to the relevant argument that determines the basis dimension when specifying a smooth term by s() in the model formula). On the other hand the period effect takes just about 3 df. It is a good idea to do some diagnostic checking of the fitted model The four diagnostic plots are analogous to some of those used in the context of linear models for Gaussian responses, but not all of them may be as easy to interpret. - Pay attention to the note given in the printed output about the value of k. Let us refit the model but now with an increased k for age: With this choice of k the df value for age became about 11, which is well below \\(k-1 = 19\\). Let us plot the fitted curves from this fitting, too There does not seem to have happened any essential changes from the previously fitted curves, so maybe 8 df could, after all, be quite enough for the age effect. Graphical presentation of the effects using plot.gam() can be improved. For instance, we may present the age effect to describe the mean incidence rates by age, averaged over the whole time span of 54 years. This is obtained by adding the estimated intercept to the estimated smooth curve for the age effect and showing the antilogarithms of the ordinates of the curve. For that purpose we need to extract the intercept and modify the labels of the \\(y\\)-axis accordingly. The estimated period curve can also be expressed in terms of relative indidence rates in relation to the fitted baseline rate, as determined by the model intercept. Homework You could continue the analysis of these data by fitting an age-cohort model as an alternative to the age-period model, as well as an age-cohort-period model utilizing function apc.fit() in Epi. See (http://bendixcarstensen.com/APC/) for details. "],["causal-inference.html", "Chapter 9 Causal inference 9.1 Proper adjustment for confounding in regression models 9.2 DAG tools in the package dagitty 9.3 Instrumental variables estimation: Mendelian randomization 9.4 Why are simulation exercises useful for causal inference?", " Chapter 9 Causal inference 9.1 Proper adjustment for confounding in regression models The first exercise of this session will ask you to simulate some data according to pre-specified causal structure (don’t take the particular example too seriously) and see how you should adjust the analysis to obtain correct estimates of the causal effects. Suppose one is interested in the effect of beer-drinking on body weight. Let’s assume that in addition to the potential effect of beer on weight, the following is true in reality: Beer-drinking has an effect on the body weight. Men drink more beer than women Men have higher body weight than women People with higher body weight tend to have higher blood pressure Beer-drinking increases blood pressure The task is to simulate a dataset in accordance with this model, and subsequently analyse it to see, whether the results would allow us to conclude the true association structure. Sketch a causal graph (not necessarily with R) to see, how should one generate the data Suppose the actual effect sizes are following: People who drink beer weigh on average \\(2kg\\) more than those who don’t. The probability of beer-drinking is 0.2 for females and 0.7 for males Men weigh on average \\(10kg\\) more than women. One kg difference in body weight corresponds in average to \\(0.5mmHg\\) difference in (systolic) blood pressures. Beer-drinking increases blood pressure by \\(10mmHg\\) in average. Beer-drinking has no effect on body weight. The R commands to generate the data are: bdat &lt;- data.frame(sex = c(rep(0, 500), rep(1, 500))) # a data frame with 500 females, 500 males bdat$beer &lt;- rbinom(1000, 1, 0.2 + 0.5 * bdat$sex) bdat$weight &lt;- 60 + 10 * bdat$sex + rnorm(1000, 0, 7) bdat$bp &lt;- 110 + 0.5 * bdat$weight + 10 * bdat$beer + rnorm(1000, 0, 10) Now fit the following models for body weight as dependent variable and beer-drinking as independent variable. Look, what is the estimated effect size: Unadjusted (just simple linear regression) Adjusted for sex Adjusted for sex and blood pressure What would be the conclusions on the effect of beer on weight, based on the three models? Do they agree? Which (if any) of the models gives an unbiased estimate of the actual causal effect of interest? How can the answer be seen from the graph? Now change the data-generation algorithm so, that in fact beer-drinking does increase the body weight by 2kg. Look, what are the conclusions in the above models now. Thus the data is generated as before, but the weight variable is computed as: bdat$weight &lt;- 60 + 10 * bdat$sex + 2 * bdat$beer + rnorm(1000, 0, 7) Suppose one is interested in the effect of beer-drinking on blood pressure instead, and is fitting a) an unadjusted model for blood pressure, with beer as an only covariate; b) a model with beer and sex as covariates. Would either a) or b) give an unbiased estimate for the effect? (You may double-check whether the simulated data is consistent with your answer). 9.2 DAG tools in the package dagitty There is a software DAGitty (http://www.dagitty.net/) and also an R package dagitty that can be helpful in dealing with DAGs. Let’s try to get the answer to the previous exercise using this package. if (!(&quot;dagitty&quot; %in% installed.packages())){ install.packages(&quot;dagitty&quot;) } library(dagitty) Let’s recreate the graph on the lecture slide 23 (but omitting the direct causal effect of interest, \\(C \\rightarrow D\\)): g &lt;- dagitty(&quot;dag { C &lt;- S -&gt; Y -&gt; U -&gt; D C -&gt; Z &lt;- Y Z -&gt; D C &lt;- X -&gt; D C -&gt; Q W -&gt; D }&quot;) plot(g) To get a more similar look as on the slide, we must supply the coordinates (x increases from left to right, y from top to bottom): coordinates(g) &lt;- list( x = c( S = 1, C = 1, Q = 1, Y = 2, Z = 2, X = 2, U = 3, D = 3, W = 3 ), y = c( U = 1, Y = 1, S = 1, Z = 2, C = 3, D = 3, X = 4, W = 4, Q = 4 ) ) plot(g) Let’s look at all possible paths from \\(C\\) to \\(D\\): paths(g, &quot;C&quot;, &quot;D&quot;)$paths As you see, one path contains a collider and is therefore a closed path and the others are open. Let’s identify the minimal sets of variables needed to adjust the model for \\(D\\) for, to obtain an unbiased estimate of the effect of \\(C\\). You can specify, whether you want to estimate direct or total effect of \\(C\\): adjustmentSets( g, exposure = &quot;C&quot;, outcome = &quot;D&quot;, effect = &quot;direct&quot; ) adjustmentSets( g, exposure = &quot;C&quot;, outcome = &quot;D&quot;, effect = &quot;total&quot; ) Thus, for total effect estimation one should adjust for \\(X\\) and either \\(Y\\) or \\(S\\), whereas for direct effect estimation, one would also need to adjust for \\(Z\\). You can verify that, these are the variables that will block all open paths from \\(C\\) to \\(D\\). Now try to do the beer-weight exercise using dagitty: Create the DAG and plot it What are the paths from WEIGHT to BEER? Will you get the same recommendation for the adjustment variable selection as you found before? 9.3 Instrumental variables estimation: Mendelian randomization Suppose you want to estimate the effect of Body Mass Index (BMI) on blood glucose level (associated with the risk of diabetes). Let’s conduct a simulation study to verify that when the exposure-outcome association is confounded, but there is a valid instrument (genotype), one obtains an unbiased estimate of the causal effect. Start by generating the genotype variable as Binomial(2,p), with \\(p=0.2\\) (and look at the resulting genotype frequencies): n &lt;- 10000 mrdat &lt;- data.frame(G = rbinom(n, 2, 0.2)) table(mrdat$G) Also generate the confounder variable U mrdat$U &lt;- rnorm(n) Generate a continuous (normally distributed) exposure variable \\(BMI\\) so that it depends on \\(G\\) and \\(U\\). Check with linear regression, whether there is enough power to get significant parameter estimates. For instance: mrdat$BMI &lt;- with(mrdat, 25 + 0.7 * G + 2 * U + rnorm(n)) Finally generate \\(Y\\) (“Blood glucose level”) so that it depends on \\(BMI\\) and \\(U\\) (but not on \\(G\\)). mrdat$Y &lt;- with(mrdat, 3 + 0.1 * BMI - 1.5 * U + rnorm(n, 0, 0.5)) Verify, that simple regression model for \\(Y\\), with \\(BMI\\) as a covariate, results in a biased estimate of the causal effect (parameter estimate is different from what was generated) How different is the estimate from 0.1? Estimate a regression model for \\(Y\\) with two covariates, \\(G\\) and \\(BMI\\). Do you see a significant effect of \\(G\\)? Could you explain analytically, why one may see a significant parameter estimate for \\(G\\) there? Find an IV (instrumental variables) estimate, using G as an instrument, by following the algorithm in the lecture notes (use two linear models and find a ratio of the parameter estimates). Does the estimate get closer to the generated effect size? mgx &lt;- lm(BMI ~ G, data = mrdat) ci.lin(mgx) # check the instrument effect bgx &lt;- mgx$coef[2] # save the 2nd coefficient (coef of G) mgy &lt;- lm(Y ~ G, data = mrdat) ci.lin(mgy) bgy &lt;- mgy$coef[2] causeff &lt;- bgy / bgx causeff # closer to 0.1? A proper simulation study would require the analysis to be run several times, to see the extent of variability in the parameter estimates. A simple way to do it here would be using a for-loop. Modify the code as follows (exactly the same commands as executed so far, adding a few lines of code to the beginning and to the end): n &lt;- 10000 # initializing simulations: # 30 simulations (change it, if you want more): nsim &lt;- 30 mr &lt;- rep(NA, nsim) # empty vector for the outcome parameters for (i in 1:nsim) { # start the loop ## Exactly the same commands as before: mrdat &lt;- data.frame(G = rbinom(n, 2, 0.2)) mrdat$U &lt;- rnorm(n) mrdat$BMI &lt;- with(mrdat, 25 + 0.7 * G + 2 * U + rnorm(n)) mrdat$Y &lt;- with(mrdat, 3 + 0.1 * BMI - 1.5 * U + rnorm(n, 0, 0.5)) mgx &lt;- lm(BMI ~ G, data = mrdat) bgx &lt;- mgx$coef[2] mgy &lt;- lm(Y ~ G, data = mrdat) bgy &lt;- mgy$coef[2] # Save the i&#39;th parameter estimate: mr[i] &lt;- bgy / bgx } # end the loop Now look at the distribution of the parameter estimate: summary(mr) (optional) Change the code of simulations so that the assumptions are violated: add a weak direct effect of the genotype G to the equation that generates \\(Y\\): mrdat$Y &lt;- with( mrdat, 3 + 0.1 * BMI - 1.5 * U + 0.05 * G + rnorm(n, 0, 0.5) ) Repeat the simulation study to see, what is the bias in the average estimated causal effect of \\(BMI\\) on \\(Y\\). (optional) Using library sem and function tsls, one can obtain a two-stage least squares estimate for the causal effect and also the proper standard error. Do you get the same estimate as before? if (!(&quot;sem&quot; %in% installed.packages())) install.packages(&quot;sem&quot;) library(sem) summary(tsls(Y ~ BMI, ~G, data = mrdat)) (There are also several other R packages for IV estimation and Mendelian Randomization (MendelianRandomization for instance)) 9.4 Why are simulation exercises useful for causal inference? If we simulate the data, we know the data-generating mechanism and the true causal effects. So this is a way to check, whether an analysis approach will lead to estimates that correspond to what is generated. One could expect to see similar phenomena in real data analysis, if the data-generation mechanism is similar to what was used in simulations. "],["graphics-meccano.html", "Chapter 10 Graphics meccano 10.1 Base graphics 10.2 Using ggplot2 10.3 Grid graphics", " Chapter 10 Graphics meccano The plot below is from a randomized study of the effect of Tamoxifen treatment on bone mineral metabolism, in a group of patients who were treated for breast cancer. The data are available in the file alkfos.csv (using comma as separator, so read.csv will read it). # change filename as needed alkfos &lt;- read.csv(&quot;./data/alkfos.csv&quot;) The purpose of this exercise is to show you how to build a similar graph using base graphics in R. This will take you through a number of fundamental techniques. The exercise will also walk you through creating the graph using ggplot2. To get started, run the code in the housekeeping script alkfos-house.r . You probably should not study the code in too much detail at this point. The script create the following objects in your workspace. times, a vector of length 7 giving the observation times means, a \\(2 \\times 7\\) matrix giving the mean percentage change at each time point. Each group has its own row. sems, a \\(2 \\times 7\\) matrix giving standard errors of the means, used to create the error bars. available, a \\(2 \\times 7\\) matrixing giving the number of participants still available Use the objects() to see the objects created function to see them. 10.1 Base graphics Now we start building the plot. It is important that you use some form of script to hold the R code since you will frequently have to modify and rerun previously entered code. First, plot the means for group 1 (i.e. means[1,]) against times, using type=\"b\" (look up what this does) Then add a similar curve for group 2 to the plot using points or lines. Notice that the new points are below the \\(y\\) scale of the plot, so you need to revise the initial plot by setting a suitable ylim value. Add the error bars using segments. (You can calculate the endpoints using upper &lt;- means + sems etc.). You may have to adjust the ylim again. Add the horizontal line at \\(y=0\\) using abline Use xlab and ylab in the initial plot call to give better axis labels. We need a nonstandard x axis. Use xaxt=\"n\" to avoid plotting it at first, then add a custom axis with axis The counts below the x axis can be added using mtext on lines 5 and 6 below the bottom of the plot, but you need to make room for the extra lines. Use par(mar=.1 + c(8,4,4,2)) before plotting anything to achieve this. You now have quite a good reconstruction of the original plot. There are some additional steps you can take to reproduce the published plot exactly. These are advanced exercises so feel free to come back to them later. It is not too important here (it was for some other variables in the study), but the S-PLUS plot has the points for the second group offset horizontally by a small amount (.25) to prevent overlap. Redo the plot with this modification. Further things to fiddle with: Get rid of the bounding box. Add Control/Tamoxifen labels to the lines of counts. Perhaps use different plotting symbols. Rotate the y axis values. Modify the line widths or line styles. Finally, try plotting to the pdf() device and view the results using a PDF viewer (e.g. Adobe Acrobat Reader). You may need to change the pointsize option and/or the plot dimensions for optimal appearance. You might also try saving the plot as a metafile and include it in a Word document. 10.2 Using ggplot2 The housekeeping script alkfos-house.r also creates a data frame ggdata containing the variables in long format. The code for generating the data frame is shown below, but you do not need to repeat it if you have run the script. ggdata &lt;- data.frame( times = rep(times, 2), means = c(means[1, ], means[2, ]), sds = c(sds[1, ], sds[2, ]), available = c(available[1, ], available[2, ]), treat = rep(c(&quot;placebo&quot;, &quot;tamoxifen&quot;), each = 7) ) ggdata &lt;- transform(ggdata, sems = sds / sqrt(available)) To create a first approximation to the plot in ggplot2 we use the qplot function (short for quick plot). First you must install the ggplot2 package from CRAN and then load it: library(ggplot2) qplot( x = times, y = means, group = treat, geom = c(&quot;point&quot;, &quot;line&quot;), data = ggdata ) ## Warning: `qplot()` was deprecated in ggplot2 3.4.0. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this ## warning was generated. The first arguments to qplot are called aesthetics in the grammar of graphics. Here we want to plot y=means by x=times grouped by group=treat. The aesthetics are used by the geometries, which are specified with the geom argument. Here we want to plot both points and lines. The data argument tells qplot to get the aesthetics from the data frame ggdata. To add the error bars, we add a new geometry linerange which uses the aesthetics ymin and ymax p &lt;- qplot( x = times, y = means, group = treat, ymin = means - sems, ymax = means + sems, yintercept = 0, geom = c(&quot;point&quot;, &quot;line&quot;, &quot;linerange&quot;), data = ggdata ) print(p) In this case we are saving the output of qplot to an R object p. This means the plot will not appear automatically when we call qplot. Instead, we must explicitly print it. Note how the y axes are automatically adjusted to include the error bars. This is because they are included in the call to qplot and not added later (as was the case with base graphics). It remains to give informative axis labels and put the right tick marks on the x-axis. This is done by adding scales to the plot p &lt;- p + scale_x_continuous( name = &quot;Months after randomization&quot;, breaks = ggdata$times ) + scale_y_continuous(name = &quot;% change in alkaline phosphatase&quot;) print(p) We can also change the look and feel of the plot by adding a theme (in this case the black and white theme). p + theme_bw() As an alternative to qplot, we can use the ggplot function to define the data and the common aesthetics, then add the geometries with separate function calls. All the grobs (graphical objects) created by these function calls are combined with the + operator: p &lt;- ggplot( data = ggdata, aes( x = times, y = means, ymin = means - sems, ymax = means + sems, group = treat ) ) + geom_point() + geom_line() + geom_linerange() + geom_hline(yintercept = 0, colour = &quot;darkgrey&quot;) + scale_x_continuous(breaks = ggdata$times) + scale_y_continuous(breaks = seq(-35, 25, 5)) print(p) This call adds another geometry hline which uses the aesthetic yintercept to add a horizontal line at 0 on the y-axis. Note that this alternate syntax allows each geometry to have its own aesthetics: here we draw the horizontal line in darkgrey instead of the default black. 10.3 Grid graphics As a final, advanced topic, this subsection shows how viewports from the grid package may be used to display the plot and the table in the same graph. First we create a text table: tab &lt;- ggplot( data = ggdata, aes(x = times, y = treat, label = available) ) + geom_text(size = 3) + xlab(NULL) + ylab(NULL) + scale_x_continuous(breaks = NULL) tab Then we create a layout that will contain the graph above the table. Most of the space is taken by the graph. The grid.show.layout allows you to preview the layout. library(grid) Layout &lt;- grid.layout(nrow = 2, ncol = 1, heights = unit( c(2, 0.25), c(&quot;null&quot;, &quot;null&quot;) )) grid.show.layout(Layout) The units are relative (null) units. You can specify exact sizes in centimetres, inches, or lines if you prefer. We then print the graph and the table in the appropriate viewports grid.newpage() # Clear the page pushViewport(viewport(layout = Layout)) print(p, vp = viewport(layout.pos.row = 1, layout.pos.col = 1)) print(tab, vp = viewport(layout.pos.row = 2, layout.pos.col = 1)) Notice that the left margins do not match. One way to get the margins to match is to use the plot_grid function from thecowplot package. library(cowplot) ## ## Attaching package: &#39;cowplot&#39; ## The following object is masked from &#39;package:lubridate&#39;: ## ## stamp plot_grid( p, tab, align = &quot;v&quot;, ncol = 1, nrow = 2, rel_heights = c(5, 1) ) The cowplot package has a theme that is useful for publications. It is a black and white theme with no grid: theme_set(theme_cowplot()) plot_grid( p, tab, align = &quot;v&quot;, ncol = 1, nrow = 2, rel_heights = c(5, 1) ) The theme_set function changes the default ggplot2 theme so that all subsequent displays will use the given theme. "],["survival-analysis-with-competing-risks-oral-cancer-patients.html", "Chapter 11 Survival analysis with competing risks: Oral cancer patients 11.1 Description of the data 11.2 Loading the packages and the data 11.3 Total mortality: Kaplan–Meier analyses 11.4 Total mortality by stage 11.5 Event-specific cumulative mortality curves 11.6 Regression modelling of overall mortality. 11.7 Modelling event-specific hazards 11.8 Lexis object with multi-state set-up 11.9 Optional: Poisson regression as an alternative to Cox model", " Chapter 11 Survival analysis with competing risks: Oral cancer patients 11.1 Description of the data File oralca2.txt, that you may access from a url address to be given in the practical, contains data from 338 patients having an oral squamous cell carcinoma diagnosed and treated in one tertiary level oncological clinic in Finland since 1985, followed-up for mortality until 31 December 2008. The dataset contains the following variables: Variable Description sex sex, a factor with categories; 1 = \"Female\", 2 = \"Male\" age age (years) at the date of diagnosing the cancer stage TNM stage of the tumour (factor): 1 = \"I\", ..., 4 = \"IV\", 5 = \"unkn\" time follow-up time (in years) since diagnosis until death or censoring event event ending the follow-up (numeric): 0 = censoring alive, 1 = death from oral cancer, 2 = death from other causes. 11.2 Loading the packages and the data Load the R packages Epi, and survival needed in this exercise. library(Epi) library(survival) Read the datafile oralca2.txt from a website, whose precise address will be given in the practical, into an R data frame named orca. Look at the head, structure and the summary of the data frame. Using function table() count the numbers of censorings as well as deaths from oral cancer and other causes, respectively, from the event variable. orca &lt;- read.table(&quot;pracs/data/oralca2.txt&quot;, header = TRUE) head(orca) str(orca) summary(orca) 11.3 Total mortality: Kaplan–Meier analyses We start our analysis of total mortality pooling the two causes of death into a single outcome. First, construct a survival object orca$suob from the event variable and the follow-up time using function Surv(). Look at the structure and summary of !orca$suob! . orca$suob &lt;- Surv(orca$time, 1 * (orca$event &gt; 0)) str(orca$suob) summary(orca$suob) Create a survfit object s.all, which does the default calculations for a Kaplan–Meier analysis of the overall (marginal) survival curve. s.all &lt;- survfit(suob ~ 1, data = orca) See the structure of this object and apply print() method on it, too. Look at the results; what do you find? % Try also summary() and see the outcome. s.all str(s.all) The summary method for a survfit object would return a lengthy life table. However, the plot method with default arguments offers the Kaplan–Meier curve for a conventional illustration of the survival experience in the whole patient group. Alternatively, instead of graphing survival proportions, one can draw a curve describing their complements: the cumulative mortality proportions. This curve is drawn together with the survival curve as the result of the second command line below. plot(s.all) lines(s.all, fun = &quot;event&quot;, mark.time = F, conf.int = FALSE) The effect of option mark.time=F is to omit marking the times when censorings occurred. 11.4 Total mortality by stage Tumour stage is an important prognostic factor in cancer survival studies. Plot separate cumulative mortality curves for the different stage groups marking them with different colours, the order which you may define yourself. Also find the median survival time for each stage. s.stg &lt;- survfit(suob ~ stage, data = orca) col5 &lt;- c(&quot;green&quot;, &quot;blue&quot;, &quot;black&quot;, &quot;red&quot;, &quot;gray&quot;) plot(s.stg, col = col5, fun = &quot;event&quot;, mark.time = FALSE) s.stg Create now two parallel plots of which the first one describes the cumulative hazards and the second one graphs the log-cumulative hazards against log-time for the different stages. Compare the two presentations with each other and with the one in the previous item. par(mfrow = c(1, 2)) plot(s.stg, col = col5, fun = &quot;cumhaz&quot;, main = &quot;cum. hazards&quot;) plot( s.stg, col = col5, fun = &quot;cloglog&quot;, main = &quot;cloglog: log cum.haz&quot; ) If the survival times were exponentially distributed in a given (sub)population the corresponding cloglog-curve should follow an approximately linear pattern. Could this be the case here in the different stages? Also, if the survival distributions of the different subpopulations would obey the proportional hazards model, the vertical distance between the cloglog-curves should be approximately constant over the time axis. Do these curves indicate serious deviation from the proportional hazards assumption? In the lecture handouts it was observed that the crude contrast between males and females in total mortality appears unclear, but the age-adjustment in the Cox model provided a more expected hazard ratio estimate. We shall examine the confounding by age somewhat closer. First categorize the continuous age variable into, say, three categories by function cut() using suitable breakpoints, like 55 and 75 years, and cross-tabulate sex and age group: orca$agegr &lt;- cut(orca$age, br = c(0, 55, 75, 95)) stat.table(list(sex, agegr), list(count(), percent(agegr)), margins = TRUE, data = orca ) Male patients are clearly younger than females in these data. Now, plot Kaplan–Meier curves jointly classified by sex and age. s.agrx &lt;- survfit(suob ~ agegr + sex, data = orca) par(mfrow = c(1, 1)) plot(s.agrx, fun = &quot;event&quot;, mark.time = FALSE, xlim = c(0, 15), col = rep(c(&quot;red&quot;, &quot;blue&quot;), 3), lty = c(2, 2, 1, 1, 5, 5) ) In each age band the mortality curve for males is on a higher level than that for females. 11.5 Event-specific cumulative mortality curves We move on to analysing cumulative mortalities for the two causes of death separately, first overall and then by prognostic factors. Use the survfit-function in survival package with option type=\"mstate\". library(survival) cif1 &lt;- survfit(Surv(time, event, type = &quot;mstate&quot;) ~ 1, data = orca ) str(cif1) One could apply here the plot method of the survfit object to plot the cumulative incidences for each cause. However, we suggest that you use instead a simple function plotCIF() found in the Epi package. The main arguments are data data frame created by function }survfit() event indicator for the event: values 1 or 2. Other arguments are like in the ordinary plot() function. Draw two parallel plots describing the overall cumulative incidence curves for both causes of death par(mfrow = c(1, 2)) plotCIF(cif1, 1, main = &quot;Cancer death&quot;) plotCIF(cif1, 2, main = &quot;Other deaths&quot;) Compute the estimated cumulative incidences by stage for both causes of death. Now you have to add variable stage to survfit-function. See the structure of the resulting object, in which you should observe strata variable containing the stage grouping variable. Plot the pertinent curves in two parallel graphs. Cut the \\(y\\)-axis for a more efficient graphical presentation col5 &lt;- c(&quot;green&quot;, &quot;blue&quot;, &quot;black&quot;, &quot;red&quot;, &quot;gray&quot;) cif2 &lt;- survfit(Surv(time, event, type = &quot;mstate&quot;) ~ stage, data = orca ) str(cif2) par(mfrow = c(1, 2)) plotCIF(cif2, 1, main = &quot;Cancer death by stage&quot;, col = col5, ylim = c(0, 0.7) ) plotCIF(cif2, 2, main = &quot;Other deaths by stage&quot;, col = col5, ylim = c(0, 0.7) ) Compare the two plots. What would you conclude about the effect of stage on the two causes of death? Using another function stackedCIF() in Epi you can put the two cumulative incidence curves in one graph but stacked upon one another such that the lower curve is for the cancer deaths and the upper curve is for total mortality, and the vertical difference between the two curves describes the cumulative mortality from other causes. You can also add some colours for the different zones: par(mfrow = c(1, 1)) stackedCIF(cif1, colour = c(&quot;gray70&quot;, &quot;gray85&quot;)) 11.6 Regression modelling of overall mortality. Fit the semiparametric proportional hazards regression model, a.k.a. the Cox model, on all deaths including sex, age and stage as covariates. Use function coxph() in package survival. It is often useful to center and scale continuous covariates like age here. The estimated rate ratios and their confidence intervals can also here be displayed by applying ci.lin() on the fitted model object. options(show.signif.stars = FALSE) m1 &lt;- coxph(suob ~ sex + I((age - 65) / 10) + stage, data = orca) summary(m1) round(ci.exp(m1), 4) Look at the results. What are the main findings? Check whether the data are sufficiently consistent with the assumption of proportional hazards with respect to each of the variables separately as well as globally, using the cox.zph() function. cox.zph(m1) No evidence against proportionality assumption could apparently be found. Moreover, no difference can be observed between stages I and II in the estimates. On the other hand, the group with stage unknown is a complex mixture of patients from various true stages. Therefore, it may be prudent to exclude these subjects from the data and to pool the first two stage groups into one. After that fit a model in the reduced data with the new stage variable. orca2 &lt;- subset(orca, stage != &quot;unkn&quot;) orca2$st3 &lt;- Relevel(orca2$stage, list(1:2, 3, 4:5)) levels(orca2$st3) &lt;- c(&quot;I-II&quot;, &quot;III&quot;, &quot;IV&quot;) m2 &lt;- update(m1, . ~ . - stage + st3, data = orca2) round(ci.exp(m2), 4) Plot the predicted cumulative mortality curves by stage, jointly stratified by sex and age, focusing only on 40 and 80 year old patients, respectively, based on the fitted model m2. You need to create a new artificial data frame containing the desired values for the covariates. newd &lt;- data.frame( sex = c(rep(&quot;Male&quot;, 6), rep(&quot;Female&quot;, 6)), age = rep(c(rep(40, 3), rep(80, 3)), 2), st3 = rep(levels(orca2$st3), 4) ) newd col3 &lt;- c(&quot;green&quot;, &quot;black&quot;, &quot;red&quot;) par(mfrow = c(1, 2)) plot( survfit( m2, newdata = subset(newd, sex == &quot;Male&quot; &amp; age == 40) ), col = col3, fun = &quot;event&quot;, mark.time = FALSE ) lines( survfit( m2, newdata = subset(newd, sex == &quot;Female&quot; &amp; age == 40) ), col = col3, fun = &quot;event&quot;, lty = 2, mark.time = FALSE ) plot( survfit( m2, newdata = subset(newd, sex == &quot;Male&quot; &amp; age == 80) ), ylim = c(0, 1), col = col3, fun = &quot;event&quot;, mark.time = FALSE ) lines( survfit( m2, newdata = subset(newd, sex == &quot;Female&quot; &amp; age == 80) ), col = col3, fun = &quot;event&quot;, lty = 2, mark.time = FALSE ) 11.7 Modelling event-specific hazards Fit the Cox model for the cause-specific hazard of cancer deaths with the same covariates as above. In this case only cancer deaths are counted as events and deaths from other causes are included into censorings. m2haz1 &lt;- coxph( Surv(time, event == 1) ~ sex + I((age - 65) / 10) + st3, data = orca2 ) round(ci.exp(m2haz1), 4) cox.zph(m2haz1) Compare the results with those of model m2. What are the major differences? Fit a similar model for deaths from other causes and compare the results. m2haz2 &lt;- coxph( Surv(time, event == 2) ~ sex + I((age - 65) / 10) + st3, data = orca2 ) round(ci.exp(m2haz2), 4) cox.zph(m2haz2) 11.8 Lexis object with multi-state set-up Before entering to multi-state analyses, it might be instructive to apply some Lexis tools to illustrate the competing-risks set-up. More detailed explanation of these tools will be given by Bendix later. Form a Lexis object from the data frame and print a summary of it. We shall name the main (and only) time axis in this object as stime. orca.lex &lt;- Lexis( exit = list(stime = time), exit.status = factor(event, labels = c(&quot;Alive&quot;, &quot;Oral ca. death&quot;, &quot;Other death&quot;) ), data = orca ) summary(orca.lex) Draw a box diagram of the two-state set-up of competing transitions. Run first th e following command line boxes(orca.lex) Now, move the cursor to the point in the graphics window, at which you wish to put the box for Alive, and click. Next, move the cursor to the point at which you wish to have the box for Oral ca. death, and click. Finally, do the same with the box for Other death. If you are not happy with the outcome, run the command line again and repeat the necessary mouse moves and clicks. 11.9 Optional: Poisson regression as an alternative to Cox model It can be shown that the Cox model with an unspecified form for the baseline hazard \\(\\lambda_0(t)\\) is mathematically equivalent to the following kind of Poisson regression model. Time is treated as a categorical factor with a dense division of the time axis into disjoint intervals or timebands such that only one outcome event occurs in each timeband. The model formula contains this time factor plus the desired explanatory terms. A sufficient division of time axis is obtained by first setting the break points between adjacent timebands to be those time points at which an outcome event has been observed to occur. Then, the pertinent lexis object is created and after that it will be split according to those breakpoints. Finally, the Poisson regression model is fitted on the splitted lexis object using function glm() with appropriate specifications. We shall now demonstrate the numerical equivalence of the Cox model m2haz1 for oral cancer mortality that was fitted above, and the corresponding Poisson regression. First we form the necessary lexis object by just taking the relevant subset of the already available orca.lex object. Upon that the three-level stage factor st3 is created as above. orca2.lex &lt;- subset(orca.lex, stage != &quot;unkn&quot;) orca2.lex$st3 &lt;- Relevel(orca2$stage, list(1:2, 3, 4:5)) levels(orca2.lex$st3) &lt;- c(&quot;I-II&quot;, &quot;III&quot;, &quot;IV&quot;) Then, the break points of time axis are taken from the sorted event times, and the lexis object is split by those breakpoints. The timeband factor is defined according to the splitted survival times stored in variable stime. cuts &lt;- sort(orca2$time[orca2$event == 1]) orca2.spl &lt;- splitLexis(orca2.lex, br = cuts, time.scale = &quot;stime&quot;) orca2.spl$timeband &lt;- as.factor(orca2.spl$stime) As a result we now have an expanded lexis object in which each subject has several rows; as many rows as there are such timebands during which he/she is still at risk. The outcome status lex.Xst has value 0 in all those timebands, over which the subject stays alive, but assumes the value 1 or 2 at his/her last interval ending at the time of death. – See now the structure of the splitted object. str(orca2.spl) orca2.spl[1:20, ] We are ready to fit the desired Poisson model for oral cancer death as the outcome. The splitted person-years are contained in lex.dur, and the explanatory variables are the same as in model m2haz1. – This fitting may take some time …. m2pois1 &lt;- glm( 1 * (lex.Xst == &quot;Oral ca. death&quot;) ~ -1 + timeband + sex + I((age - 65) / 10) + st3, family = poisson, offset = log(lex.dur), data = orca2.spl ) We shall display the estimation results graphically for the baseline hazard (per 1000 person-years) and numerically for the rate ratios associated with the covariates. Before doing that it is useful to count the length ntb of the block occupied by baseline hazard in the whole vector of estimated parameters. However, owing to how the splitting to timebands was done, the last regression coefficient is necessarily zero and better be omitted when displaying the results. Also, as each timeband is quantitatively named accoding to its leftmost point, it is good to compute the midpoint values tbmid for the timebands tb &lt;- as.numeric(levels(orca2.spl$timeband)) ntb &lt;- length(tb) tbmid &lt;- (tb[-ntb] + tb[-1]) / 2 # midpoints of the intervals round(ci.exp(m2pois1), 3) par(mfrow = c(1, 1)) plot(tbmid, 1000 * exp(coef(m2pois1)[1:(ntb - 1)]), ylim = c(5, 3000), log = &quot;xy&quot;, type = &quot;l&quot; ) Compare the regression coefficients and their error margins to those model m2haz1. Do you find any differences? How does the estimated baseline hazard look like? The estimated baseline looks quite ragged when based on 71 separate parameters. A smoothed estimate may be obtained by spline modelling using the tools contained in package splines (see the practical of Saturday 25 May afternoon). With the following code you will be able to fit a reasonable spline model for the baseline hazard and draw the estimated curve (together with a band of the 95% confidence limits about the fitted values). From the same model you should also obtain quite familiar results for the rate ratios of interest. library(splines) m2pspli &lt;- update( m2pois1, . ~ ns(stime, df = 6, intercept = FALSE) + sex + I((age - 65) / 10) + st3) round(ci.exp(m2pspli), 3) news &lt;- data.frame( stime = seq(0, 25, length = 301), lex.dur = 1000, sex = &quot;Female&quot;, age = 65, st3 = &quot;I-II&quot; ) blhaz &lt;- predict(m2pspli, newdata = news, se.fit = TRUE, type = &quot;link&quot;) blh95 &lt;- cbind(blhaz$fit, blhaz$se.fit) %*% ci.mat() par(mfrow = c(1, 1)) matplot(news$stime, exp(blh95), type = &quot;l&quot;, lty = c(1, 1, 1), lwd = c(2, 1, 1), col = rep(&quot;black&quot;, 3), log = &quot;xy&quot;, ylim = c(5, 3000) ) "],["time-splitting-time-scales-and-smr.html", "Chapter 12 Time-splitting, time-scales and SMR 12.1 Age-specific mortality Further time scales: period and duration 12.2 SMR 12.3 SMR modeling", " Chapter 12 Time-splitting, time-scales and SMR This exercise is about mortaity among Danish Diabetes patients. It is based on the dataset DMlate, a random sample of 10,000 patients from the Danish Diabetes Register (scrambeled dates), all with date of diagnosis after 1994. Start by loading the relevant packages: library(Epi) library(popEpi) Attaching package: &#39;popEpi&#39; The following object is masked from &#39;package:lubridate&#39;: is.Date library(mgcv) library(tidyverse) Then load the data and take a look at the data: data(DMlate) str(DMlate) &#39;data.frame&#39;: 10000 obs. of 7 variables: $ sex : Factor w/ 2 levels &quot;M&quot;,&quot;F&quot;: 2 1 2 2 1 2 1 1 2 1 ... $ dobth: num 1940 1939 1918 1965 1933 ... $ dodm : num 1999 2003 2005 2009 2009 ... $ dodth: num NA NA NA NA NA ... $ dooad: num NA 2007 NA NA NA ... $ doins: num NA NA NA NA NA NA NA NA NA NA ... $ dox : num 2010 2010 2010 2010 2010 ... You can get a more detailed explanation of the data by referring to the help page: ?DMlate Set up the dataset as a Lexis object with age, calendar time and duration of diabetes as timescales, and date of death as event. Make sure that you know what each of the arguments to Lexis mean: LL &lt;- Lexis(entry = list(A = dodm - dobth, P = dodm, dur = 0), exit = list(P = dox), exit.status = factor(!is.na(dodth), labels = c(&quot;Alive&quot;, &quot;Dead&quot;)), data = DMlate) NOTE: entry.status has been set to &quot;Alive&quot; for all. NOTE: Dropping 4 rows with duration of follow up &lt; tol Take a look at the first few lines of the resulting dataset, for example using head(). Get an overview of the mortality by using stat.table to tabulate no. deaths, person-years (lex.dur) and the crude mortality rate by sex. Try: stat.table(sex, list(D = sum(lex.Xst == &quot;Dead&quot;), Y = sum(lex.dur), rate = ratio(lex.Xst == &quot;Dead&quot;, lex.dur, 1000)), margins = TRUE, data = LL) --------------------------------- sex D Y rate --------------------------------- M 1343.00 27614.21 48.63 F 1156.00 26659.05 43.36 Total 2499.00 54273.27 46.04 --------------------------------- # stat.table is more versatile than xtabs: xtabs(cbind(D = lex.Xst == &quot;Dead&quot;, Y = lex.dur) ~ sex, data = LL) sex D Y M 1343.00 27614.21 F 1156.00 26659.05 If we want to assess how mortality depends on age, calendar time and duration or how it relates to population mortality, we should in principle split the follow-up along all three time scales. In practice it is sufficient to split it along one of the time-scales and then use the value of each of the time-scales at the left endpoint of the intervals. Use splitLexis (or splitMulti from the popEpi package) to split the follow-up along the age-axis in sutiable intervals (here set to 1/2 year, but really immaterial as long as it is small): SL &lt;- splitLexis(LL, breaks = seq(0, 125, 1 / 2), time.scale = &quot;A&quot;) summary(SL) Transitions: To From Alive Dead Records: Events: Risk time: Persons: Alive 115974 2499 118473 2499 54273.27 9996 How many records are now in the dataset? How many person-years? Compare to the original Lexis-dataset. 12.1 Age-specific mortality Now estimate age-specific mortality curves for men and women separately, using splines as implemented in gam. We use k = 20 to be sure to catch any irregularities by age. r.m &lt;- mgcv::gam(cbind(lex.Xst == &quot;Dead&quot;, lex.dur) ~ s(A, k = 20), family = poisreg, data = subset(SL, sex == &quot;M&quot;)) Make sure you understand all the components on this modeling statement. Fit the same model for women. There is a convenient wrapper for this, exploiting the Lexis structure of data, but which does not have an update r.m &lt;- gam.Lexis(subset(SL, sex == &quot;M&quot;), ~ s(A, k = 20)) mgcv::gam Poisson analysis of Lexis object subset(SL, sex == &quot;M&quot;) with log link: Rates for the transition: Alive-&gt;Dead r.f &lt;- gam.Lexis(subset(SL, sex == &quot;F&quot;), ~ s(A, k = 20)) mgcv::gam Poisson analysis of Lexis object subset(SL, sex == &quot;F&quot;) with log link: Rates for the transition: Alive-&gt;Dead Now, extract the estimated rates by using the wrapper function ci.pred that computes predicted rates and confidence limits for these. glm.Lexis and gam.Lexis use the poisreg family that will return the rates in the (inverse) units in which the person-years were given; that is the units in which lex.dur is recorded. nd &lt;- data.frame(A = seq(20, 90, 0.5)) p.m &lt;- ci.pred(r.m, newdata = nd) p.f &lt;- ci.pred(r.f, newdata = nd) str(p.m) num [1:141, 1:3] 0.00132 0.00137 0.00142 0.00147 0.00152 ... - attr(*, &quot;dimnames&quot;)=List of 2 ..$ : chr [1:141] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ..$ : chr [1:3] &quot;Estimate&quot; &quot;2.5%&quot; &quot;97.5%&quot; Plot the predicted rates for men and women together - using for example matplot or matshade. par(mar = c(3.5,3.5,1,1), mgp = c(3,1,0) / 1.6, las = 1, bty = &quot;n&quot;, lend = &quot;butt&quot;) matplot(nd$A, cbind(p.m, p.f) * 1000, type = &quot;l&quot;, col = rep(c(&quot;blue&quot;, &quot;red&quot;), each = 3), lwd = c(3, 1, 1), lty = 1, log = &quot;y&quot;, yaxt = &quot;n&quot;, xlab = &quot;Age&quot;, ylab = &quot;Mortality per 1000 PY&quot;) axis(side = 2, at = ll &lt;- outer( c(5, 10, 20), -1:1, function(x,y) x * 10^y), labels = ll) Further time scales: period and duration We now want to model the mortality rates among diabetes patients also including current date and duration of diabetes, using penalized splines. Use the argument bs = \"cr\" to s() to get cubic splines instead of thin plate (\"tp\") splines which is the default. As before specify the model exploiting the Lexis class of the dataset, try: Mcr &lt;- gam.Lexis(subset(SL, sex == &quot;M&quot;), ~ s(A, bs = &quot;cr&quot;, k = 10) + s(P, bs = &quot;cr&quot;, k = 10) + s(dur, bs = &quot;cr&quot;, k = 10)) mgcv::gam Poisson analysis of Lexis object subset(SL, sex == &quot;M&quot;) with log link: Rates for the transition: Alive-&gt;Dead summary(Mcr) Family: poisson Link function: log Formula: cbind(trt(Lx$lex.Cst, Lx$lex.Xst) %in% trnam, Lx$lex.dur) ~ s(A, bs = &quot;cr&quot;, k = 10) + s(P, bs = &quot;cr&quot;, k = 10) + s(dur, bs = &quot;cr&quot;, k = 10) Parametric coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.54074 0.04938 -71.7 &lt;2e-16 Approximate significance of smooth terms: edf Ref.df Chi.sq p-value s(A) 3.645 4.517 1013.20 &lt; 2e-16 s(P) 1.024 1.048 17.58 3.48e-05 s(dur) 7.586 8.384 74.46 &lt; 2e-16 R-sq.(adj) = 0.00333 Deviance explained = 9.87% UBRE = -0.8054 Scale est. = 1 n = 60347 Fit the same model for women as well. Are the models reasonably fitting? Fcr &lt;- gam.Lexis(subset(SL, sex == &quot;F&quot;), ~ s(A, bs = &quot;cr&quot;, k = 10) + s(P, bs = &quot;cr&quot;, k = 10) + s(dur, bs = &quot;cr&quot;, k = 10)) mgcv::gam Poisson analysis of Lexis object subset(SL, sex == &quot;F&quot;) with log link: Rates for the transition: Alive-&gt;Dead summary(Fcr) Family: poisson Link function: log Formula: cbind(trt(Lx$lex.Cst, Lx$lex.Xst) %in% trnam, Lx$lex.dur) ~ s(A, bs = &quot;cr&quot;, k = 10) + s(P, bs = &quot;cr&quot;, k = 10) + s(dur, bs = &quot;cr&quot;, k = 10) Parametric coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.78483 0.05808 -65.17 &lt;2e-16 Approximate significance of smooth terms: edf Ref.df Chi.sq p-value s(A) 2.667 3.366 988.49 &lt; 2e-16 s(P) 1.904 2.391 20.08 0.000136 s(dur) 5.973 6.972 38.98 &lt; 2e-16 R-sq.(adj) = 0.00417 Deviance explained = 11.1% UBRE = -0.82405 Scale est. = 1 n = 58126 Plot the estimated effects, using the default plot method for gam objects. Remember that there are three effects estimated, so it is useful set up a multi-panel display, and for the sake of comparability to set ylim to the same for men and women: par(mfrow = c(2, 3)) plot(Mcr, ylim = c(-3, 3)) plot(Fcr, ylim = c(-3, 3)) par(mfcol = c(3, 2)) plot(Mcr, ylim = c(-3, 3)) plot(Fcr, ylim = c(-3, 3)) What is the absolute scale for these effects? Compare the fit of the naive model with just age and the three-factor models, using anova, e.g.: anova(Mcr, r.m, test = &quot;Chisq&quot;) Analysis of Deviance Table Model 1: cbind(trt(Lx$lex.Cst, Lx$lex.Xst) %in% trnam, Lx$lex.dur) ~ s(A, bs = &quot;cr&quot;, k = 10) + s(P, bs = &quot;cr&quot;, k = 10) + s(dur, bs = &quot;cr&quot;, k = 10) Model 2: cbind(trt(Lx$lex.Cst, Lx$lex.Xst) %in% trnam, Lx$lex.dur) ~ s(A, k = 20) Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 60332 11717 2 60340 11812 -7.9484 -95.094 &lt; 2.2e-16 What do you conclude? The model we fitted has three time-scales: current age, current date and current duration of diabetes, so the effects that we report are not immediately interpretable, as they are (as in any kind of multiple regressions) to be interpreted as all else equal which they are not, as the three time scales advance simultaneously at the same pace. The reporting would therefore more naturally be on the mortality scale as a function of age, but showing the mortality for persons diagnosed in different ages, using separate displays for separate years of diagnosis. This is most easily done using the ci.pred function with the newdata = argument. So a person diagnosed in age 50 in 1995 will have a mortality measured in cases per 1000 PY as: pts &lt;- seq(0, 12, 1/4) nd &lt;- data.frame(A = 50 + pts, P = 1995 + pts, dur = pts) m.pr &lt;- ci.pred(Mcr, newdata = nd) Note that because we used gam.Lexis which uses the poisreg family we need not specify lex.dur as a variable in the prediction data frame nd. Predictions will be rates in the same units as lex.dur (well, the inverse). Now take a look at the result from the ci.pred statement and construct prediction of mortality for men and women diagnosed in a range of ages, say 50, 60, 70, and plot these together in the same graph: cbind(nd, ci.pred(Mcr, newdata = nd))[1:10,] From figure it seems that the duration effect is over-modeled, so refit constraining the d.f. to 5: Mcr &lt;- gam.Lexis(subset(SL, sex == &quot;M&quot;), ~ s(A, bs = &quot;cr&quot;, k = 10) + s(P, bs = &quot;cr&quot;, k = 10) + s(dur, bs = &quot;cr&quot;, k = 5)) mgcv::gam Poisson analysis of Lexis object subset(SL, sex == &quot;M&quot;) with log link: Rates for the transition: Alive-&gt;Dead Fcr &lt;- gam.Lexis(subset(SL, sex == &quot;F&quot;), ~ s(A, bs = &quot;cr&quot;, k = 10) + s(P, bs = &quot;cr&quot;, k = 10) + s(dur, bs = &quot;cr&quot;, k = 5)) mgcv::gam Poisson analysis of Lexis object subset(SL, sex == &quot;F&quot;) with log link: Rates for the transition: Alive-&gt;Dead Plot the estimated rates from the revised models. What do you conclude from the plots? 12.2 SMR The SMR is the Standardized Mortality Ratio, which is the mortality rate-ratio between the diabetes patients and the general population. In real studies we would subtract the deaths and the person-years among the diabetes patients from those of the general population, but since we do not have access to these, we make the comparison to the general population at large, i.e. also including the diabetes patients. So we now want to include the population mortality rates as a fixed variable in the split dataset; for each record in the split dataset we attach the value of the population mortality for the relevant sex, and and calendar time. This can be achieved in two ways: Either we just use the current split of follow-up time and allocate the population mortality rates for some suitably chosen (mid-)point of the follow-up in each, or we make a second split by date, so that follow-up in the diabetes patients is in the same classification of age and data as the population mortality table. We will use the former approach, using the dataset split in 6 month intervals, and then include as an extra variable the population mortality as available from the data set M.dk. First create the variables in the diabetes dataset that we need for matching with the population mortality data, that is sex and age and date at the midpoint of each of the intervals (or rather at a point 3 months after the left endpoint of the interval; recall we split the follow-up in 6 month intervals). We need to have variables of the same type when we merge, so we must transform the sex variable in M.dk to a factor, and must for each follow-up interval in the SL data have an age and a period variable that can be used in merging with the population data. str(SL) Classes &#39;Lexis&#39; and &#39;data.frame&#39;: 118473 obs. of 14 variables: $ lex.id : int 1 1 1 1 1 1 1 1 1 1 ... $ A : num 58.7 59 59.5 60 60.5 ... $ P : num 1999 1999 2000 2000 2001 ... $ dur : num 0 0.339 0.839 1.339 1.839 ... $ lex.dur: num 0.339 0.5 0.5 0.5 0.5 ... $ lex.Cst: Factor w/ 2 levels &quot;Alive&quot;,&quot;Dead&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ lex.Xst: Factor w/ 2 levels &quot;Alive&quot;,&quot;Dead&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ sex : Factor w/ 2 levels &quot;M&quot;,&quot;F&quot;: 2 2 2 2 2 2 2 2 2 2 ... $ dobth : num 1940 1940 1940 1940 1940 ... $ dodm : num 1999 1999 1999 1999 1999 ... $ dodth : num NA NA NA NA NA NA NA NA NA NA ... $ dooad : num NA NA NA NA NA NA NA NA NA NA ... $ doins : num NA NA NA NA NA NA NA NA NA NA ... $ dox : num 2010 2010 2010 2010 2010 ... - attr(*, &quot;breaks&quot;)=List of 3 ..$ A : num [1:251] 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 ... ..$ P : NULL ..$ dur: NULL - attr(*, &quot;time.scales&quot;)= chr [1:3] &quot;A&quot; &quot;P&quot; &quot;dur&quot; - attr(*, &quot;time.since&quot;)= chr [1:3] &quot;&quot; &quot;&quot; &quot;&quot; SL$Am &lt;- floor(SL$A + 0.25) SL$Pm &lt;- floor(SL$P + 0.25) data(M.dk) str(M.dk) &#39;data.frame&#39;: 7800 obs. of 6 variables: $ A : num 0 0 0 0 0 0 0 0 0 0 ... $ sex : num 1 2 1 2 1 2 1 2 1 2 ... $ P : num 1974 1974 1975 1975 1976 ... $ D : num 459 303 435 311 405 258 332 205 312 233 ... $ Y : num 35963 34383 36099 34652 34965 ... $ rate: num 12.76 8.81 12.05 8.97 11.58 ... - attr(*, &quot;Contents&quot;)= chr &quot;Number of deaths and risk time in Denmark&quot; M.dk &lt;- transform(M.dk, Am = A, Pm = P, sex = factor(sex, labels = c(&quot;M&quot;, &quot;F&quot;))) str(M.dk) &#39;data.frame&#39;: 7800 obs. of 8 variables: $ A : num 0 0 0 0 0 0 0 0 0 0 ... $ sex : Factor w/ 2 levels &quot;M&quot;,&quot;F&quot;: 1 2 1 2 1 2 1 2 1 2 ... $ P : num 1974 1974 1975 1975 1976 ... $ D : num 459 303 435 311 405 258 332 205 312 233 ... $ Y : num 35963 34383 36099 34652 34965 ... $ rate: num 12.76 8.81 12.05 8.97 11.58 ... $ Am : num 0 0 0 0 0 0 0 0 0 0 ... $ Pm : num 1974 1974 1975 1975 1976 ... Then match the rates from M.dk into SL; sex, Am and Pm are the common variables, and therefore the match is on these variables: SLr &lt;- merge(SL, M.dk[, c(&quot;sex&quot;, &quot;Am&quot;, &quot;Pm&quot;, &quot;rate&quot;)]) dim(SL) [1] 118473 16 dim(SLr) [1] 118454 17 This merge (remember to ?merge!) only takes rows that have information from both datasets, hence the slightly fewer rows in SLr than in SL. Compute the expected number of deaths as the person-time multiplied (lex.dur) by the corresponding population rate, and put it in a new variable, E, say (Expected). Use stat.table to make a table of observed, expected and the ratio (SMR) by age (suitably grouped, look for cut) and sex. Fit a poisson model with sex as the explanatory variable and log-expected as offset to derive the SMR (and c.i.). Some of the population mortality rates are 0, so you need to exclude those records from the analysis. msmr &lt;- glm((lex.Xst == &quot;Dead&quot;) ~ sex - 1, offset = log(E), family = poisson, data = subset(SLr, E &gt; 0)) ci.exp(msmr) exp(Est.) 2.5% 97.5% sexM 1.685699 1.597881 1.778344 sexF 1.541922 1.455442 1.633540 Do you recognize the numbers? The same model can be fitted a bit simpler by the poisreg family, try: msmr &lt;- glm(cbind(lex.Xst == &quot;Dead&quot;, E) ~ sex - 1, family = poisreg, data = subset(SLr, E &gt; 0)) ci.exp(msmr) exp(Est.) 2.5% 97.5% sexM 1.685699 1.597881 1.778344 sexF 1.541922 1.455441 1.633541 We can assess the ratios of SMRs between men and women by using the ctr.mat argument which should be a matrix: (CM &lt;- rbind(M = c(1, 0), W = c(0, 1), &quot;M/F&quot; = c(1, -1))) [,1] [,2] M 1 0 W 0 1 M/F 1 -1 round(ci.exp(msmr, ctr.mat = CM), 2) exp(Est.) 2.5% 97.5% M 1.69 1.60 1.78 W 1.54 1.46 1.63 M/F 1.09 1.01 1.18 What do you conclude about the mortality rates among men and women? 12.3 SMR modeling Now model the SMR using age and date of diagnosis and diabetes duration as explanatory variables, including the expected-number instead of the person-years, using separate models for men and women. Note that you cannot use gam.Lexis from the code you used for fitting models for the rates, you need to use gam with the poisreg family. And remember to exclude those units where no deaths in the population occur (that is where the population mortality rate is 0). Msmr &lt;- gam(cbind(lex.Xst == &quot;Dead&quot;, E) ~ s( A, bs = &quot;cr&quot;, k = 5) + s( P, bs = &quot;cr&quot;, k = 5) + s(dur, bs = &quot;cr&quot;, k = 5), family = poisreg, data = subset(SLr, E &gt; 0 &amp; sex == &quot;M&quot;)) ci.exp(Msmr) exp(Est.) 2.5% 97.5% (Intercept) 2.1667053 2.0001046 2.3471833 s(A).1 0.5960527 0.5181962 0.6856068 s(A).2 0.8927331 0.8650901 0.9212594 s(A).3 0.3663506 0.2818154 0.4762435 s(A).4 0.4587803 0.3763049 0.5593320 s(P).1 0.9893048 0.9732835 1.0055898 s(P).2 0.9608720 0.9085181 1.0162428 s(P).3 0.9027294 0.7825209 1.0414040 s(P).4 0.9173557 0.8132938 1.0347323 s(dur).1 0.6581828 0.5769629 0.7508362 s(dur).2 0.8446645 0.7613599 0.9370838 s(dur).3 0.7830215 0.6916313 0.8864877 s(dur).4 1.8176744 1.1041338 2.9923368 Fsmr &lt;- update(Msmr, data = subset(SLr, E &gt; 0 &amp; sex == &quot;F&quot;)) Plot the estimated smooth effects for both men and women using e.g. plot.gam. What do you see? Plot the predicted SMRs from the models for men and women diagnosed in ages 50, 60 and 70 as you did for the rates. What do you see? par(mfrow = c(1,1)) n50 &lt;- nd n60 &lt;- mutate(n50, A = A + 10) n70 &lt;- mutate(n50, A = A + 20) head(n70) A P dur 1 70.00 1995.00 0.00 2 70.25 1995.25 0.25 3 70.50 1995.50 0.50 4 70.75 1995.75 0.75 5 71.00 1996.00 1.00 6 71.25 1996.25 1.25 matshade(n50$A, cbind(ci.pred(Msmr, n50), ci.pred(Fsmr, n50)), plot = TRUE, col = c(&quot;blue&quot;, &quot;red&quot;), lwd = 3, ylim = c(0.5, 5), log = &quot;y&quot;, xlim = c(50, 80)) matshade(n60$A, cbind(ci.pred(Msmr, n60), ci.pred(Fsmr, n60)), col = c(&quot;blue&quot;, &quot;red&quot;), lwd = 3) matshade(n70$A, cbind(ci.pred(Msmr, n70), ci.pred(Fsmr, n70)), col = c(&quot;blue&quot;, &quot;red&quot;), lwd = 3) abline(h = 1) abline(v = 50 + 0:5, lty = 3, col = &quot;gray&quot;) Describe the shapes of the curves. What aspects of the shapes are induced by the model ? Try to simplify the model to one with a simple sex effect, separate linear effects of age and date of follow-up for each sex, and a smooth effect of duration common for both sexes, giving an estimate of the change in SMR by age and calendar time. Bsmr &lt;- gam(cbind(lex.Xst == &quot;Dead&quot;, E) ~ sex / A + sex / P + s(dur, bs = &quot;cr&quot;, k = 5), family = poisreg, data = subset(SLr, E &gt; 0)) round(ci.exp(Bsmr)[-1,], 3) exp(Est.) 2.5% 97.5% sexF 52149.525 0.000 2.375788e+23 sexM:A 0.981 0.977 9.860000e-01 sexF:A 0.980 0.975 9.860000e-01 sexM:P 0.987 0.972 1.002000e+00 sexF:P 0.981 0.965 9.980000e-01 s(dur).1 0.663 0.601 7.320000e-01 s(dur).2 0.861 0.795 9.310000e-01 s(dur).3 0.885 0.805 9.740000e-01 s(dur).4 1.412 0.945 2.111000e+00 How much does SMR change by each year of age? And by each calendar year? What is the meaning of the sexF parameter? Use your previous code to plot the predicted mortality from this model too. Are the predicted SMR curves credible? m50 &lt;- mutate(n50, sex = &quot;M&quot;) f50 &lt;- mutate(n50, sex = &quot;F&quot;) m60 &lt;- mutate(m50, A = A + 10) f60 &lt;- mutate(f50, A = A + 10) m70 &lt;- mutate(m50, A = A + 20) f70 &lt;- mutate(f50, A = A + 20) matshade(n50$A, cbind(ci.pred(Bsmr, m50), ci.pred(Bsmr, f50)), plot = TRUE, col = c(&quot;blue&quot;, &quot;red&quot;), lwd = 3, ylim = c(0.5, 5), log = &quot;y&quot;, xlim = c(50, 80)) matshade(n60$A, cbind(ci.pred(Bsmr, m60), ci.pred(Bsmr, f60)), col = c(&quot;blue&quot;, &quot;red&quot;), lwd = 3) matshade(n70$A, cbind(ci.pred(Bsmr, m70), ci.pred(Bsmr, f70)), col = c(&quot;blue&quot;, &quot;red&quot;), lwd = 3) abline(h = 1) abline(h = 1:5, lty = 3, col = &quot;gray&quot;) What is your conclusion about SMR for diabetes patients relative to the general popuation? "],["nested-case-control-study-and-case-cohort-study-risk-factors-of-coronary-heart-disease.html", "Chapter 13 Nested case-control study and case-cohort study: Risk factors of coronary heart disease 13.1 Reading the cohort data, illustrating the study base and risk sets 13.2 Nested case-control study 13.3 Case-cohort study 13.4 Do these estimates resemble those obtained from nested case-control data? 13.5 Full cohort analysis and comparisons 13.6 Further exercises and homework", " Chapter 13 Nested case-control study and case-cohort study: Risk factors of coronary heart disease In this exercise we shall apply both the nested case-control (NCC) design and the case-cohort (CC) design in sampling control subjects from a defined cohort or closed study population. The case group comprises those cohort members who die from coronary heart disease (CHD) during a \\(&gt; 20\\) years follow-up of the cohort. The risk factors of interest are cigarette smoking, systolic blood pressure, and total cholesterol level. Our study population is an occupational cohort comprising 1501 men working in blue-collar jobs in one Nordic country. Eligible subjects had no history of coronary heart disease when recruited to the study in the early 1990s. Smoking habits and many other items were inquired at baseline by a questionnaire, and blood pressure was measured by a research nurse, the values being written down on the questionnaire. Serum samples were also taken from the cohort members at the same time and were stored in a freezer. For some reason, the data in the questionnaires were not entered to any computer file, but the questionnaires were kept in a safe storehouse for further purposes. Also, no biochemical analyses were initially performed for the sera collected from the participants. However, dates of birth and dates of entry to the study were recorded in an electronic file. In 2010 the study was suddenly reactivated by those investigators of the original team who were still alive then. As the first step mortality follow-up of the cohort members was executed by record linkage to the national population register, from which the dates of death and emigration were obtained. Another linkage was performed with the national register of causes of death in order to get the deaths from coronary heard disease identified. As a result a data file occoh.txt was completed containing the following variables: Variable Description id identification number birth date of birth entry date of recruitment and baseline measurements exit date of exit from mortality follow-up death indicator for vital status at the end of follow-up; 1, if dead from any cause, and = 0, if alive chdeath indicator for death from coronary heart disease; 1, if yes, and 0, if no. This exercise is divided into five main parts: Description of the study base or the follow-up experience of the whole cohort, identification of the cases and illustrating the risk sets. Nested case-control study within the cohort: selection of controls by risk set or time-matched sampling using function ccwc() in package Epi, collection of exposure data for cases and controls from the pertinent data base of the whole cohort to the case-control data set using function merge(), and analysis of the case-control data set with stratified Cox model using function clogit() in package survival(), Case-cohort study within the cohort: selection of a subcohort by simple random sampling from the cohort, collection of exposure data for subcohort members and cases, and analysis of the case-cohort data set with Cox model by weighted partial likelihood including appropriate weighting and correction of estimated covariance matrix for the model coefficients using function cch() in package survival(). Comparison of results from all previous analyses, also with those from a full cohort design. Further tasks and homework. 13.1 Reading the cohort data, illustrating the study base and risk sets Load the packages Epi and survival. Read in the cohort data file and name the resulting data frame as oc. See its structure and print the univariate summaries. library(Epi) library(survival) Attaching package: &#39;survival&#39; The following object is masked from &#39;package:popEpi&#39;: Surv url &lt;- &quot;https://raw.githubusercontent.com/SPE-R/SPE/master/pracs/data&quot; oc &lt;- read.table(paste(url, &quot;occoh.txt&quot;, sep = &quot;/&quot;), header = TRUE) str(oc) summary(oc) It is convenient to change all the dates into fractional calendar years oc$ybirth &lt;- cal.yr(oc$birth) oc$yentry &lt;- cal.yr(oc$entry) oc$yexit &lt;- cal.yr(oc$exit) We shall also compute the age at entry and at exit, respectively, as age will be the main time scale in our analyses. oc$agentry &lt;- oc$yentry - oc$ybirth oc$agexit &lt;- oc$yexit - oc$ybirth As the next step we shall create a lexis object from the data frame along the calendar period and age axes, and as the outcome event we specify the coronary death. oc.lex &lt;- Lexis( entry = list( per = yentry, age = yentry - ybirth ), exit = list(per = yexit), exit.status = chdeath, id = id, data = oc ) str(oc.lex) summary(oc.lex) At this stage it is informative to examine a graphical presentation of the follow-up lines and outcome cases in a conventional Lexis diagram. Make use of the plot method for Lexis objects. Gray lifelines are drawn and a bullet is put at the exit point of those lifelines that end with the outcome event. par(mfrow = c(1, 1)) plot(oc.lex, xlim = c(1990, 2010), grid = TRUE) points(oc.lex, pch = c(NA, 16)[oc.lex$lex.Xst + 1]) As age is here the main time axis, we shall graphically illustrate the study base, i.e. the follow-up lines and outcome events, only along the age scale, being ordered by age at exit. Vertical lines at those ages when new coronary deaths occur are drawn to identify the pertinent risk sets. For that purpose it is useful first to sort the data frame and the Lexis object jointly by age at exit &amp; age at entry, and to give a new ID number according to that order. oc.ord &lt;- cbind(ID = 1:1501, oc[order(oc$agexit, oc$agentry), ]) oc.lexord &lt;- Lexis( entry = list(age = agentry), exit = list(age = agexit), exit.status = chdeath, id = ID, data = oc.ord ) plot(oc.lexord, &quot;age&quot;) points(oc.lexord, pch = ifelse(oc.lexord$lex.Xst == 1, 16, NA)) with( subset(oc.lexord, lex.Xst == 1), abline(v = agexit, lty = 3) ) For a closer look, we now zoom the graphical illustration of the risk sets into event times occurring between 50 to 58 years. – Copy the last four lines from the previous item and add arguments xlim and ylim to the call of plot(). plot(oc.lexord, &quot;age&quot;, xlim = c(50, 58), ylim = c(5, 65)) points( oc.lexord, &quot;age&quot;, pch = ifelse(oc.lexord$lex.Xst == 1, 16, NA) ) with( subset(oc.lexord, lex.Xst == 1), abline(v = agexit, lty = 3) ) 13.2 Nested case-control study We shall now employ the strategy of risk-set sampling or time-matched sampling of controls, i.e. we are conducting a nested case-control study within the cohort. The risk sets are defined according to the age at diagnosis of the case. Further matching is applied for age at entry by 1-year agebands. For this purpose we first generate a categorical variable agen2 for age at entry oc.lex$agen2 &lt;- cut(oc.lex$agentry, br = seq(40, 62, 1)) Matched sampling from risk sets may be carried out using function ccwc() found in the Epi package. Its main arguments are the times of entry and exit which specify the time at risk along the main time scale (here age), and the outcome variable to be given in the fail argument. The number of controls per case is set to be two, and the additional matching factor is given. - After setting the RNG seed (with your own number), make a call of this function and see the structure of the resulting data frame cactrl containing the cases and the chosen individual controls. set.seed(98623) cactrl &lt;- ccwc( entry = agentry, exit = agexit, fail = chdeath, controls = 2, match = agen2, include = list(id, agentry), data = oc.lex, silent = FALSE ) str(cactrl) Check the meaning of the four first columns of the case-control data frame from the help page of function ccwc(). Now we shall start collecting data on the risk factors for the cases and their matched controls, including determination of the total cholesterol levels from the frozen sera! The storehouse of the risk factor measurements for the whole cohort is file occoh-Xdata.txt. It contains values of the following variables. Variable Description id identification number, the same as in occoh.txt smok. cigarette smoking with categories; 1: never, 2: former, 3: 1-14/d, 4: 15+/d sbp. systolic blood pressure (mmHg) tchol total cholesterol level (mmol/l) ocX &lt;- read.table( paste(url, &quot;occoh-Xdata.txt&quot;, sep = &quot;/&quot;), header = TRUE ) str(ocX) In the next step we collect the values of the risk factors for our cases and controls by merging the case-control data frame and the storehouse file. In this operation we utilize function merge() to select columns of two data frames: cactrl (all columns) and ocX (four columns) and to merge these into a single file (see exercise 1.1, subsection 1.1.8, where merge() was introduced). The id variable in both files is used as the key to link each individual case or control with his own data on risk factors. oc.ncc &lt;- merge(cactrl, ocX[, c(&quot;id&quot;, &quot;smok&quot;, &quot;tchol&quot;, &quot;sbp&quot;)], by = &quot;id&quot; ) str(oc.ncc) We shall treat smoking as categorical and total cholesterol and systolic blood pressure as quantitative risk factors, but the values of the latter will be divided by 10 to get more interpretable effect estimates. Variable Description cholgrp cholesterol class; 1: &lt;5, 2: 5-&lt;6.5, 3: &gt;=6.5 sbpgrp blood pressure class; 1: &lt;130, 2: 130-&lt;150, 3: 150-&lt;170, 4: &gt;=170 Convert the smoking variable into a factor. oc.ncc$smok &lt;- factor(oc.ncc$smok, labels = c(&quot;never&quot;, &quot;ex&quot;, &quot;1-14/d&quot;, &quot;&gt;14/d&quot;) ) It is useful to start the analysis of case-control data by simple tabulations by the categorized risk factors. Crude estimates of the rate ratios associated with them, in which matching is ignored, can be obtained as follows. We shall focus on smoking stat.table( index = list(smok, Fail), contents = list(count(), percent(smok)), margins = TRUE, data = oc.ncc ) smok.crncc &lt;- glm(Fail ~ smok, family = binomial, data = oc.ncc) round(ci.exp(smok.crncc), 3) A proper analysis takes into account matching that was employed in the selection of controls for each case from the pertinent risk set, further restricted to subjects who were about the same age at entry as the case was. Also, adjustment for the other risk factors is desirable. In this analysis function clogit() in survival package is utilized. It is in fact a wrapper of function coxph(). m.clogit &lt;- clogit(Fail ~ smok + I(sbp / 10) + tchol + strata(Set), data = oc.ncc) summary(m.clogit) round(ci.exp(m.clogit), 3) Compare these with the crude estimates obtained above. 13.3 Case-cohort study Now we start applying the second major outcome-selective sampling strategy for collecting exposure data from a big study population The subcohort is selected as a simple random sample (\\(n=260\\)) from the whole cohort. The id-numbers of the individuals that are selected will be stored in vector subcids, and subcind is an indicator for inclusion to the subcohort. N &lt;- 1501 n &lt;- 260 set.seed(15792) subcids &lt;- sample(N, n) oc.lexord$subcind &lt;- 1 * (oc.lexord$id %in% subcids) We form the data frame oc.cc to be used in the subsequent analysis selecting the union of the subcohort members and the case group from the data frame of the full cohort. After that we collect the data of the risk factors from the data storehouse for the subjects in the case-cohort data oc.cc &lt;- subset(oc.lexord, subcind == 1 | chdeath == 1) oc.cc &lt;- merge(oc.cc, ocX[, c(&quot;id&quot;, &quot;smok&quot;, &quot;tchol&quot;, &quot;sbp&quot;)], by = &quot;id&quot; ) str(oc.cc) We shall now create a graphical illustration of the lifelines contained in the case-cohort data. Lines for the subcohort non-cases are grey without bullet at exit, those for subcohort cases are blue with blue bullet at exit, and for cases outside the subcohort the lines are red and dotted with red bullets at exit. plot(subset(oc.cc, chdeath == 0), &quot;age&quot;) lines(subset(oc.cc, chdeath == 1 &amp; subcind == 1), col = &quot;blue&quot;) lines(subset(oc.cc, chdeath == 1 &amp; subcind == 0), col = &quot;red&quot;) points(subset(oc.cc, chdeath == 1), pch = 16, col = c(&quot;blue&quot;, &quot;red&quot;)[oc.cc$subcind + 1] ) Define the categorical smoking variable again. oc.cc$smok &lt;- factor(oc.cc$smok, labels = c(&quot;never&quot;, &quot;ex&quot;, &quot;1-14/d&quot;, &quot;&gt;14/d&quot;) ) A crude estimate of the hazard ratio for the various smoking categories \\(k\\) vs. non-smokers (\\(k=1\\)) can be obtained by tabulating cases \\((D_k)\\) and person-years (\\(y_k\\)) in the subcohort by smoking and then computing the relevant exposure odds ratio for each category: \\[ \\text{HR}_k ^{\\text{crude}} = \\frac{D_k/D_1}{y_k/y_1} \\] sm.cc &lt;- stat.table( index = smok, contents = list(Cases = sum(lex.Xst), Pyrs = sum(lex.dur)), margins = TRUE, data = oc.cc ) print(sm.cc, digits = c(sum = 0, ratio = 1)) HRcc &lt;- (sm.cc[1, -5] / sm.cc[1, 1]) / (sm.cc[2, -5] / sm.cc[2, 1]) round(HRcc, 3) 13.4 Do these estimates resemble those obtained from nested case-control data? To estimate the rate ratios associated with smoking and adjusted for the other risk factors we now fit the pertinent Cox model applying the method of weighted partial likelihood as presented by Ling &amp; Ying (1993) and Barlow (1994). This analysis can be done using function cch() in package survival with method = \"LinYing\" oc.cc$survobj &lt;- with(oc.cc, Surv(agentry, agexit, chdeath)) cch.LY &lt;- cch(survobj ~ smok + I(sbp / 10) + tchol, stratum = NULL, subcoh = ~subcind, id = ~id, cohort.size = N, data = oc.cc, method = &quot;LinYing&quot; ) summary(cch.LY) 13.5 Full cohort analysis and comparisons Finally, suppose the investigators after all could afford to collect the data on risk factors from the storehouse for the whole cohort. Let us form the data frame corresponding to the full cohort design and convert again smoking to be categorical. oc.full &lt;- merge(oc.lex, ocX[, c(&quot;id&quot;, &quot;smok&quot;, &quot;tchol&quot;, &quot;sbp&quot;)], by.x = &quot;id&quot;, by.y = &quot;id&quot; ) oc.full$smok &lt;- factor(oc.full$smok, labels = c(&quot;never&quot;, &quot;ex&quot;, &quot;1-14/d&quot;, &quot;&gt;14/d&quot;) ) Juts for comparison with the corresponding analysis in case-cohort data perform a similar crude estimation of hazard ratios associated with smoking. sm.coh &lt;- stat.table( index = smok, contents = list(Cases = sum(lex.Xst), Pyrs = sum(lex.dur)), margins = TRUE, data = oc.full ) print(sm.coh, digits = c(sum = 0, ratio = 1)) HRcoh &lt;- (sm.coh[1, -5] / sm.coh[1, 1]) / (sm.coh[2, -5] / sm.coh[2, 1]) round(HRcoh, 3) Fit now the ordinary Cox model to the full cohort. There is no need to employ extra tricks upon the ordinary coxph() fit. cox.coh &lt;- coxph(Surv(agentry, agexit, chdeath) ~ smok + I(sbp / 10) + tchol, data = oc.full) summary(cox.coh) Lastly, a comparison of the point estimates and standard errors between the different designs, including variants of analysis for the case-cohort design, can be performed. betas &lt;- cbind(coef(cox.coh), coef(m.clogit), coef(cch.LY)) colnames(betas) &lt;- c(&quot;coh&quot;, &quot;ncc&quot;, &quot;cch.LY&quot;) round(betas, 3) SEs &lt;- cbind( sqrt(diag(cox.coh$var)), sqrt(diag(m.clogit$var)), sqrt(diag(cch.LY$var)) ) colnames(SEs) &lt;- colnames(betas) round(SEs, 3) You will notice that the point estimates of the coefficients obtained from the full cohort, nested case-control, and case-cohort analyses, respectively, are somewhat variable. However, the standard errors from the NCC and CC analyses should be quite similar when the numbers of cases and non-cases are similar. 13.6 Further exercises and homework If you have time, you could run both the NCC study and CC study again but now with a larger control group or subcohort; for example 4 controls per case in NCC and \\(n=520\\) as the subcohort size in CC. Remember resetting the seed first. Pay attention in the results to how much closer will be the point estimates and the proper SEs to those obtained from the full cohort design. Instead of simple linear terms for sbp and tchol you could try to fit spline models to describe their effects. A popular alternative to weighted partial likelihood in the analysis of case-cohort data is the pseudo-likelihood method (Prentice 1986), which is based on late entry to follow-up of the case subjects not belonging to the subcohort. The way to do this is provided by function cch() which you can apply directly to the case-cohort data oc.cc as before but now with method = \"Prentice\". – Try this and compare the results with those obtained by weighted partial likelihood in model cch.LY. Yet another computational solution for maximizing weighted partial likelihood is provided by a combination of functions twophase() and svycoxph() of the survey package. The approach is illustrated with an example in a vignette Two-phase designs in epidemiology by Thomas Lumley (see http://cran.r-project.org/web/packages/survey/vignettes/epi.pdf). – You can try this at home and check that you would obtain similar results as with model cch.LY. "],["causal-inference-2-model-based-estimation-of-causal-estimands.html", "Chapter 14 Causal inference 2: Model-based estimation of causal estimands 14.1 Control of confounding 14.2 Generation of target population and true models 14.3 Factual and counterfactual risks - associational and causal contrasts 14.4 Outcome modelling and estimation of causal contrasts by g-formula 14.5 Inverse probability weighting (IPW) by 14.6 Improving IPW estimation and using R package PSweight 14.7 Targeted maximum likelihood estimation (TMLE) 14.8 TMLE with SuperLearner", " Chapter 14 Causal inference 2: Model-based estimation of causal estimands Sources of inspiration: Luque Fernandez, M.A. et al. (2018) Stat Med 2018;37(16):2530-2546 and Smith et al. (2022) Stat Med 2022;41(2):407-432. We shall illustrate with simulated data the estimation of causal effects of a binary exposure \\(X\\) when the outcome \\(Y\\) is also binary, and there is a set of four covariates \\(Z = (Z_1, Z_2, Z_3, Z_4)\\). As a background story, we imagine a population of cancer patients, in whom the variables and the assumed marginal distributions of the covariates are Variable Description \\(X\\) treatment; 1: radiotherapy only, 0: radiotherapy + chemotherapy \\(Y\\) death during one year after diagnosis of cancer \\(Z_1\\) sex; 0: man, 1: woman; \\(Z_1 \\sim \\text{Bern}(0.5)\\) \\(Z_2\\) age group 0; young, 1: old; \\(Z_2 \\sim \\text{Bern}(0.65)\\) \\(Z_3\\) stage of cancer; 4 classes; \\(Z_3 \\sim \\text{DiscUnif}(1, \\dots, 4)\\) \\(Z_4\\) comorbidity score; 5 classes; \\(Z_3 \\sim \\text{DiscUnif}(1, \\dots, 5)\\) For simplicity, covariates \\(Z_3\\) and \\(Z_4\\) are treated as continuous variables in the models. The assumed causal diagram is shown below. For more generic notation, the probabilities of \\(Y=1\\) will be expressed as expectations, e.g. \\(E(Y^{X=x}) = P(Y^{X=x}=1)\\) and \\(E(Y|X=x, Z=z) = P(Y=1|X=x, Z=z)\\), where \\(Z\\) is the vector of relevant covariates. The same principle is applied in expressing the conditional probability of \\(X=1\\) given \\(Z=z\\). The fitted or predicted probabilities of \\(Y=1\\) are denoted as fitted \\(\\widehat{Y}\\) or predicted values \\(\\widetilde{Y}\\) of \\(Y\\) with pertinent subscripts and/or superscripts. Both \\(X\\) and \\(Y\\) are modelled by logistic regression. The expit-function or inverse of the logit function is defined: \\(\\text{expit}(u) = 1/(1 + e^{-u})\\), \\(u\\in R\\). This is equal to the cumulative distribution function of the standard logistic distribution, the values of which are returned in R by plogis(u). The R function that returns values of the logit-function is qlogis(). The true model assumed for the dependence of exposure \\(X\\) on covariates: \\[ E(X|Z_1 = z_1, \\dots, Z_4 = z_4) = \\text{expit}(-5 + 0.05z_2 + 0.25z_3 + 0.5z_4 + 0.4z_2z_4) . \\] The assumed true model for the outcome is \\[ E(Y|X=x, Z_1 = z_1, \\dots, Z_4 = z_4) = \\text{expit}(-1 + x - 0.1z_1 + 0.35z_2 + 0.25z_3 + 0.20z_4 + 0.15z_2z_4) \\] Note that \\(X\\) does not depend on \\(Z_1\\), and that in both models there is a product term \\(Z_2 Z_4\\), which appears weaker for the outcome model. 14.1 Control of confounding Based on inspection of the causal diagram, can you provide justification for the claim that variables \\(Z_2, Z_3\\) and \\(Z_4\\) form a proper subset of the four covariates, which is sufficient to block all backdoor paths between \\(X\\) and \\(Y\\) and thus remove confounding? Even though we have such a minimal sufficient set as indicated in item (a), why could it still be worth while to include covariate \\(Z_1\\), too, when modelling the outcome? 14.2 Generation of target population and true models Load the necessary packages. library(Epi) library(stdReg) library(PSweight) library(SuperLearner) Loading required package: nnls Loading required package: gam Loading required package: splines Loading required package: foreach Attaching package: &#39;foreach&#39; The following objects are masked from &#39;package:purrr&#39;: accumulate, when Loaded gam 1.22-3 Attaching package: &#39;gam&#39; The following objects are masked from &#39;package:mgcv&#39;: gam, gam.control, gam.fit, s Super Learner Version: 2.0-29 Package created on 2024-02-06 library(tmle) Loading required package: glmnet Loading required package: Matrix Attaching package: &#39;Matrix&#39; The following objects are masked from &#39;package:tidyr&#39;: expand, pack, unpack Loaded glmnet 4.1-8 Welcome to the tmle package, version 2.0.1 Use tmleNews() to see details on changes and bug fixes Define the R-functions which computes expected values for the exposure and outcome based on the assumed true outcome model and the true exposure model. EX &lt;- function(z2, z3, z4) { plogis(-5 + 0.05 * z2 + 0.25 * z3 + 0.5 * z4 + 0.4 * z2 * z4) } EY &lt;- function(x, z1, z2, z3, z4) { plogis(-1 + x - 0.1 * z1 + 0.35 * z2 + 0.25 * z3 + 0.20 * z4 + 0.15 * z2 * z4) } Define the function for the generation of data by simulating random values from pertinent probability distributions based on the given assumptions. genData &lt;- function(N) { z1 &lt;- rbinom(N, size = 1, prob = 0.5) # Bern(0.5) z2 &lt;- rbinom(N, size = 1, prob = 0.65) # Bern(0.65) z3 &lt;- trunc(runif(N, min = 1, max = 5), digits = 0) # DiscUnif(1,4) z4 &lt;- trunc(runif(N, min = 1, max = 6), digits = 0) # DiscUnif(1,5) x &lt;- rbinom(N, size = 1, prob = EX(z2, z3, z4)) y &lt;- rbinom(N, size = 1, prob = EY(x, z1, z2, z3, z4)) data.frame(z1, z2, z3, z4, x, y) } Generate a data frame dd for a big target population of 500000 subjects N &lt;- 500000 set.seed(7777) dd &lt;- genData(N) 14.3 Factual and counterfactual risks - associational and causal contrasts Compute the factual risks of death for the two exposure groups \\[ E(Y|X=x) = P(Y=1|X=x) = \\frac{P(Y=1\\ \\&amp;\\ X=x)}{P(X=x)}, \\quad x=0,1, \\] in the whole target population, as well as their associational contrasts: risk difference, risk ratio, and odds ratio. Before that define a useful function Contr &lt;- function(mu1, mu0) { RD &lt;- mu1 - mu0 RR &lt;- mu1 / mu0 OR &lt;- (mu1 / (1 - mu1)) / (mu0 / (1 - mu0)) return(c(mu1, mu0, RD = RD, RR = RR, OR = OR)) } Ey1 &lt;- with(dd, sum(y == 1 &amp; x == 1) / sum(x == 1)) Ey0 &lt;- with(dd, sum(y == 1 &amp; x == 0) / sum(x == 0)) round(Contr(Ey1, Ey0), 4) How much bigger is the risk of death of those factually exposed to radiotherapy only as compared with those receiving chemotherapy, too? Compute now first the counterfactual risks of death \\(E(Y_i^{X_i=x}) = P(Y_i^{X_i=x}=1) = \\pi_i^{X_i=x}\\) for each individual under the alternative treatments or exposure values \\(x=0,1\\) with given covariate values, then the average or overall counterfactual risks \\(E(Y^{X=1}) = \\pi^1\\) and \\(E(Y^{X=0}) = \\pi^0\\) in the population, and finally the true marginal causal contrasts for the effect of \\(X\\): \\[ \\begin{aligned} \\text{RD} &amp; = E(Y^{X=1})-E(Y^{X=0}), \\qquad \\text{RR} = E(Y^{X=1})/E(Y^{X=0}), \\\\ \\text{OR} &amp; = \\frac{E(Y^{X=1})/[1 - E(Y^{X=1})]}{E(Y^{X=0})/[1 - E(Y^{X=0})] } \\end{aligned} \\] dd &lt;- transform(dd, EY1.ind = EY(x = 1, z1, z2, z3, z4), EY0.ind = EY(x = 0, z1, z2, z3, z4) ) EY1 &lt;- mean(dd$EY1.ind) EY0 &lt;- mean(dd$EY0.ind) round(Contr(EY1, EY0), 4) Compare the associational contrasts computed in in item (a) with the causal contrasts in item (b). What do you conclude about confoundedness of the associational contrasts? 14.4 Outcome modelling and estimation of causal contrasts by g-formula As the first approach for estimating causal contrasts of interest we apply the method of standardization or g-formula. It is based on a hopefully realistic enough model for \\(E(Y|X=x, Z=z)\\), i.e. how the risk of outcome is expected to depend on the exposure variable \\(X\\) and on a sufficient set \\(Z\\) of confounders. The counterfactual risks \\(E(Y^{X=x}), x=0,1\\), are marginal expectations of the above quantities, standardized over the joint distribution of the confounders \\(Z\\) in the target population. \\[ E(Y^{X=x}) = E_Z[E(Y|X=x,Z)] = \\int E(Y|X=x, Z=z)dF_Z(z), \\quad x=0,1. \\] Assume now a slightly misspecified model mY for the outcome, which contains only main effect terms of the explanatory variables: \\[ \\pi_i = E(Y_i|X_i=x_i, Z_{i1}=z_{i1}, \\dots, Z_{i4}=z_{i4}) = \\text{expit}\\left(\\beta_0 + \\delta x_i + \\sum_{j=1}^4 \\beta_j z_{ij} \\right) \\] Fit this model on the target population using function glm() mY &lt;- glm(y ~ x + z1 + z2 + z3 + z4, family = binomial, data = dd) round(ci.lin(mY, Exp = TRUE)[, c(1, 5)], 3) There is not much idea in looking at the standard errors or confidence intervals in such a big population. For each subject \\(i\\), compute the fitted individual risk \\(\\widehat{Y_i}\\) as well as the predicted counterfactual risks \\(\\widetilde{Y_i}^{X_i=x}\\) for both exposure levels \\(x=0,1\\) separately, keeping the individual values of the \\(Z\\)-variables as they are. dd$yh &lt;- predict(mY, type = &quot;response&quot;) # fitted values dd$yp1 &lt;- predict(mY, newdata = data.frame( x = rep(1, N), # x=1 dd[, c(&quot;z1&quot;, &quot;z2&quot;, &quot;z3&quot;, &quot;z4&quot;)] ), type = &quot;response&quot;) dd$yp0 &lt;- predict(mY, newdata = data.frame( x = rep(0, N), # x=0 dd[, c(&quot;z1&quot;, &quot;z2&quot;, &quot;z3&quot;, &quot;z4&quot;)] ), type = &quot;response&quot;) Applying the method of standardization or g-formula compute now the point estimates \\[ \\widehat{E}_g(Y^{X=x}) = \\frac{1}{n} \\sum_{i=1}^n \\widetilde{Y}_i^{X_i=x}, \\quad x=0,1. \\] of the two counterfactual risks \\(E(Y^{X=1}) = \\pi^1\\) and \\(E(Y^{X=0})=\\pi^0\\) as well as the marginal causal contrasts EY1.g &lt;- mean(dd$yp1) EY0.g &lt;- mean(dd$yp0) round(Contr(EY1.g, EY0.g), 4) The expectations \\(E_Z[E(X=x, Z)]\\) taken over the joint distribution of the confounders \\(Z\\) are empirically estimated from the data by simply computing the arithmetic means of the individually predicted values \\(\\widetilde{Y_i}^{X_i=x}\\) of the outcome for the two exposure levels. Compare the estimated contrasts with the true ones in item 3(b) above. How big is the bias due to slight misspecification of the outcome model? Compare in particular the estimate of the marginal OR here with the conditional OR obtained in item (a) from the pertinent coefficient in the logistic model. Which one is closer to 1? Perform the same calculations using the tools in package stdReg (see Sjölander 2016) mY.std &lt;- stdGlm(fit = mY, data = dd, X = &quot;x&quot;) summary(mY.std) round(summary(mY.std, contrast = &quot;difference&quot;, reference = 0)$est.table, 4) round(summary(mY.std, contrast = &quot;ratio&quot;, reference = 0)$est.table, 4) round(summary(mY.std, transform = &quot;odds&quot;, contrast = &quot;ratio&quot;, reference = 0 )$est.table, 4) Check that you got the same point estimates as in the previous item. Again, the confidence intervals are not very meaningful when analysing the data covering the whole big target population. Of course, when applied to real sample data they are relevant. In stdReg package, the standard errors are obtained by the multivariate delta method built upon M-estimation and robust sandwich estimator of the pertinent covariance matrix, and approximate confidence intervals are derived from these in the usual way. If we are interested in the causal contrasts describing the effect of exposure among those exposed (like ATT), the relevant factual and counterfactual risks in that subset are \\[ \\begin{aligned} \\pi^1_1 &amp; = E(Y^{X=1}|X=1) = E(Y|X=1) = \\pi_1, \\\\ \\pi^0_1 &amp; = E(Y^{X=0}|X=1) = \\sum_{X_i=1} E(Y|X=0, Z=z)P(Z=z|X=1) \\end{aligned} \\] We are thus making and observed vs. expected comparison, in which the \\(z\\)-specific risks in the unexposed are weighted by the distribution of \\(Z\\) in the exposed subset of the target population. The risks and their contrasts are estimated from the fit of the outcome model: EY1att.g &lt;- mean(subset(dd, x == 1)$yp1) EY0att.g &lt;- mean(subset(dd, x == 1)$yp0) round(Contr(EY1att.g, EY0att.g), 4) Compare the results here with those for the whole target population. What do you observe? Any guess about the causal effect of exposure among the unexposed; is it bigger or smaller than among the exposed or among the whole population? Incidentally, the true causal contrasts among the exposed based on the true model are similarly obtained from the quantities in item 3(b) above: EY1att &lt;- mean(subset(dd, x == 1)$EY1.ind) EY0att &lt;- mean(subset(dd, x == 1)$EY0.ind) round(Contr(EY1att, EY0att), 4) Compare the estimates in the previous item with the true values obtained here. 14.5 Inverse probability weighting (IPW) by propensity scores, and augmented IPW The next method is based on weighting each individual observation by the inverse of the probability of belonging to that particular exposure group, which was realized, this probability being predicted by determinants of exposure. Fit first a model for the exposure including main effects of the \\(Z\\)-variables only. \\[ p_i = E(X_i| Z_{1i} = z_{1i}, \\dots, Z_{4i} = z_{4i}) = \\text{expit}(\\gamma_0 + \\gamma_1 z_{1i} + \\gamma_2 z_{2i} + \\gamma_3 z_{i3} + \\gamma_4 z_{4i} ), \\quad i=1, \\dots N \\] mX &lt;- glm(x ~ z1 + z2 + z3 + z4, family = binomial(link = logit), data = dd ) round(ci.lin(mX, Exp = TRUE)[, c(1, 5)], 4) Extract the propensity scores, i.e. fitted probabilities of belonging to exposure group 1: \\[ PS_i = \\widehat{p_i} \\], and compare their distribution between the two groups. dd$PS &lt;- predict(mX, type = &quot;response&quot;) summary(dd$PS) with(subset(dd, x == 0), plot(density(PS), lty = 2)) with(subset(dd, x == 1), lines(density(PS), lty = 1)) How different are the distributions? Are they sufficiently overlapping? Compute the weights \\(W_i = 1/\\text{PS}_i\\), when \\(X_i=1\\), and \\(W_i = 1/(1-\\text{PS}_i)\\), when \\(X_i=0\\). Look at the sum as well as the distribution summary of the weights in the exposure groups. The sum of weights should be close to \\(n\\) in both groups. dd$w &lt;- ifelse(dd$x == 1, 1 / dd$PS, 1 / (1 - dd$PS)) with(dd, tapply(w, x, sum)) Compute now the weighted estimates of the counterfactual risks for both exposure categories \\[ \\widehat{E}_w(Y^{X = x}) = \\frac{ \\sum_{i=1}^n {\\mathbf 1}_{ \\{X_i=x\\} } W_i Y_i } {\\sum_{i=1}^n {\\mathbf 1}_{ \\{X_i=x\\} }W_i} = \\frac{ \\sum_{X_i = x} W_i Y_i }{\\sum_{X_i=x} W_i}, \\quad x = 0,1, \\] and their causal contrasts, for instance \\[ \\widehat{\\text{RD}}_{w} = \\widehat{E}_w(Y^{X = 1}) - \\widehat{E}_w(Y^{X = 0}) = \\frac{ \\sum_{i=1}^n X_i W_i Y_i }{\\sum_{i=1}^n X_i W_i} - \\frac{ \\sum_{i=1}^n (1-X_i) W_i Y_i }{\\sum_{i=1}^n (1-X_i) W_i} \\] EY1.w &lt;- sum(dd$x * dd$w * dd$y) / sum(dd$x * dd$w) EY0.w &lt;- sum((1 - dd$x) * dd$w * dd$y) / sum((1 - dd$x) * dd$w) round(Contr(EY1.w, EY0.w), 4) These estimates seem to be somewhat downward biased when comparing to true values. Could this be because of omitting the relatively strong product term effect of \\(Z_2\\) and \\(Z_4\\)? 5.Let us attempt to correct the estimates by a double robust approach called augmented IPW estimation (AIPW), which combines the g-formula and the IPW approach. The AIPW-estimator can be expressed in two ways: either an IPW-corrected g-formula estimator, or a g-corrected IPW-estimator. \\[ \\begin{aligned} \\widehat{E}_a(Y^{X=x}) &amp; = \\widehat{E}_g(Y^{X=x}) + \\frac{1}{n} \\sum_{i=1}^n \\frac{ {\\mathbf 1}_{\\{X_i=x\\}} W_i ( Y_i - \\widetilde{Y}_i^{X_i=x} ) } {\\sum_{i=1}^n {\\mathbf 1}_{\\{X_i=x\\}} W_i} \\\\ &amp; = \\widehat{E}_w(Y^{X=x}) - \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{ {\\mathbf 1}_{\\{X_i=x\\}} W_i } {\\sum_{i=1}^n {\\mathbf 1}_{\\{X_i=x\\}} W_i } - 1 \\right] \\widetilde{Y}_i^{X_i=x}. \\end{aligned} \\] EY1.a &lt;- EY1.g + mean(dd$x * (dd$y - dd$yp1) * dd$w / sum(dd$x * dd$w)) ## or EY1.w - mean( ( ( dd$x*dd$w /sum(dd$x*dd$w) ) - 1 )*dd$yp1 ) EY0.a &lt;- EY0.g + mean((1 - dd$x) * (dd$y - dd$yp0) * dd$w / sum((1 - dd$x) * dd$w)) ## or EY0.w - mean( ( ( (1-dd$x)*dd$w/sum((1-dd$x)*dd$w) ) - 1 )*dd$yp0 ) round(Contr(EY1.a, EY0.a), 4) Compare these results with those obtained by g-formula and by non-augmented IPW method. Was augmentation successful? 14.6 Improving IPW estimation and using R package PSweight We now try to improve IPW-estimation by a richer exposure model. In computations we shall utilize the R package PSweight (see PSweight vignette). First, we compute the weights from a more flexible exposure model which contains all pairwise product terms of the parents of \\(X\\). According to the causal diagram, \\(Z_1\\) is not in that subset, so it is left out. The exposure model is specified and the weights are obtained as follows. mX2 &lt;- glm(x ~ (z2 + z3 + z4)^2, family = binomial, data = dd) round(ci.lin(mX2, Exp = TRUE)[, c(1, 5)], 3) psw &lt;- SumStat( ps.formula = mX2$formula, data = dd, weight = c(&quot;IPW&quot;, &quot;treated&quot;, &quot;overlap&quot;) ) dd$PS2 &lt;- psw$propensity[, 2] # propensity scores extracted plot(density(dd$PS2[dd$x == 0]), lty = 2) lines(density(dd$PS2[dd$x == 1]), lty = 1) Note that apart from ordinary IPW, other types of weights can also also obtained. These are relevant when estimating other kinds of causal contrasts, like average treatment effect among the treated (ATT) and ``average treatment effect in the overlap (or equipoise) population’’ (ATO). PSweight includes some useful tools to examine the properties of the distribution and to check the balance of the propensity scores, for instance plot(psw, type = &quot;balance&quot;, metric = &quot;PSD&quot;) It is desirable that the horisontal values of these measures for given weights are less than 0.1. Estimation and reporting of the causal contrasts. For relative contrasts, the summary method provides the results on the log-scale. ipwest &lt;- PSweight(ps.formula = mX2, yname = &quot;y&quot;, data = dd, weight = &quot;IPW&quot;) ipwest summary(ipwest) (logRR.ipw &lt;- summary(ipwest, type = &quot;RR&quot;)) round(exp(logRR.ipw$estimates[c(1, 4, 5)]), 3) round(exp(summary(ipwest, type = &quot;OR&quot;)$estimates[c(1, 4, 5)]), 3) Compare these with the previous IPW estimate and the AIPW estimate as well as the true values. Have we obtained nearly unbiased results? The standard errors provided by PSweight are by default based on the empirical sandwich covariance matrix and application of delta method as appropriate. Bootstrapping is also possible but is computationally very intensive and is recommended to be used only in relatively small samples. If we are interested in the effect of exposure among the exposed (like ATT) then the weights are \\(W_i = 1\\) for the exposed and \\(W_i = \\text{PS}_i/(1-\\text{PS}_i)\\) for the unexposed. Call again PSweight but with another choice of weight: psatt &lt;- PSweight(ps.formula = mX2, yname = &quot;y&quot;, data = dd, weight = &quot;treated&quot;) psatt round(summary(psatt)$estimates[1], 4) round(exp(summary(psatt, type = &quot;RR&quot;)$estimates[1]), 3) round(exp(summary(psatt, type = &quot;OR&quot;)$estimates[1]), 3) Compare the results here with those obtained by g-formula in item 4(e) and with the true contrasts in item 4(f) above. 14.7 Targeted maximum likelihood estimation (TMLE) We now consider now another double robust approach, known as targeted maximum likelihood estimation (TMLE). It also corrects the estimator obtained from the outcome model by elements that are derived from the exposure model. See Schuler and Rose (2017) for more details The first step is to utilize the propensity scores obtained above and define the so called clever covariates dd$H1 &lt;- dd$x / dd$PS2 dd$H0 &lt;- (1 - dd$x) / (1 - dd$PS2) Then, a working model is fitted for the outcome, in which the clever covariates are explanatory variables, but the model also includes the previously fitted linear predictor \\(\\widehat{\\eta}_i = \\text{logit}(\\widehat Y_i)\\) from the original outcome model mY as an offset term. Moreover, the intercept is removed. epsmod &lt;- glm(y ~ -1 + H0 + H1 + offset(qlogis(yh)), family = binomial(link = logit), data = dd ) eps &lt;- coef(epsmod) eps The logit-transformed predicted values \\(\\widetilde{Y}_i^{X_i=1}\\) and \\(\\widetilde{Y}_i^{X_i=0}\\) of counterfactual individual risks from the original outcome model are now corrected by the estimated coefficients of the clever covariates, and the corrected predictions are returned to the original scale. yp0.H &lt;- plogis(qlogis(dd$yp0) + eps[1] / (1 - dd$PS2)) yp1.H &lt;- plogis(qlogis(dd$yp1) + eps[2] / dd$PS2) Estimates of the causal contrasts: EY0.t &lt;- mean(yp0.H) EY1.t &lt;- mean(yp1.H) round(Contr(EY1.t, EY0.t), 4) Compare these with previous results and with the true values. 14.8 TMLE with SuperLearner Let us finally apply some fashionable tools of statistical learning, aka machine learning, using the package SuperLearner to fit flexible models for both exposure and outcome. As this method is computationally much more demanding, we illustrate its use by a sample of 2000 subjects only. A simple random sample of \\(n=2000\\) is drawn from the population. set.seed(7622) n &lt;- 2000 sampind &lt;- sample(N, n) samp &lt;- dd[sampind, ] The algorithms to be used in this exercise are chosen SL.library &lt;- c( &quot;SL.glm&quot;, &quot;SL.step&quot;, &quot;SL.step.interaction&quot;, &quot;SL.glm.interaction&quot;, &quot;SL.gam&quot;, &quot;SL.randomForest&quot;, &quot;SL.rpart&quot; ) Function tmle() computes estimates of the causal contrasts of interest. Argument A is for the exposure variable, and argument W contains the confounders. The run can take a while … tmlest &lt;- tmle( Y = samp$y, A = samp$x, W = samp[, c(&quot;z1&quot;, &quot;z2&quot;, &quot;z3&quot;, &quot;z4&quot;)], family = &quot;binomial&quot;, Q.SL.library = SL.library, g.SL.library = SL.library ) summary(tmlest) Let us take a closer look at the results. In the beginning are reported the fractions by which the separate algorithms contribute to the combined algorithm. After that are given estimates of the causal contrasts together with their estimated variances and 95% confidence intervals. The variance of each contrast (on log-scale for RR and OR) is estimated as the variance of the empirical influence curve divided by \\(n\\), the number of i.i.d. units of observation. Furthermore, causal risk differences are estimated also for those factually exposed and unexposed, respectively. Note that because this analysis was based on sample data, the estimates are most probably deviating from the true values because of pure random error. Therefore it is not possible to assess the magnitude of a possible bias from a single sample. Homework. When you have more time, try to run tmle on as large sample as is possible and compare its results with previous ones computed for the whole target population. "],["time-dependent-variables-and-multiple-states.html", "Chapter 15 Time-dependent variables and multiple states 15.1 The renal failure dataset Splitting the follow-up time", " Chapter 15 Time-dependent variables and multiple states The following practical exercise is based on the data from paper: P Hovind, L Tarnow, P Rossing, B Carstensen, and HH Parving: Improved survival in patients obtaining remission of nephrotic range albuminuria in diabetic nephropathy. Kidney Int, 66(3):1180–1186, Sept 2004. You can find a .pdf-version of the paper here: http://BendixCarstensen.com/AdvCoh/papers/Hovind.2004.pdf 15.1 The renal failure dataset The dataset renal.dta contains data on follow up of 125 patients from Steno Diabetes Center. They enter the study when they are diagnosed with nephrotic range albuminuria (NRA). This is a condition where the levels of albumin in the urine is exceeds a certain level as a sign of kidney disease. The levels may however drop as a consequence of treatment, this is called remission. Patients exit the study at death or kidney failure (dialysis or transplant). Variable Description id Patient id sex 1=male, 2=female dob Date of birth doe Date of entry into the study (2.5 years after NRA) dor Date of remission. Missing if no remission has occurred dox Date of exit from study event Exit status: 1,2,3=event (death, ESRD), 0=censored The dataset is in Stata-format, so you must read the dataset using read.dta from the foreign package (which is part of the standard R-distribution). At the same time, convert sex to a proper factor. Choose where to read the dataset. library(Epi) library(survival) library(mgcv) library(foreign) # renal &lt;- read.dta( # &quot;https://raw.githubusercontent.com/SPE-R/SPE/master/pracs/data/renal.dta&quot;) renal &lt;- read.dta(&quot;http://BendixCarstensen.com/SPE/data/renal.dta&quot;) renal$sex &lt;- factor(renal$sex, labels = c(&quot;M&quot;, &quot;F&quot;)) head(renal) Use the Lexis function to declare the data as survival data with age, calendar time and time since entry into the study as timescales. Label any event \\(&gt;0\\) as ESRD, i.e. renal death (death of kidney (transplant or dialysis), or person). Note that you must make sure that the alive state (here NRA) is the first, as Lexis assumes that everyone starts in this state (unless of course entry.status is specified): Lr &lt;- Lexis(entry = list(per = doe, age = doe - dob, tfi = 0), exit = list(per = dox), exit.status = factor(event &gt; 0, labels = c(&quot;NRA&quot;, &quot;ESRD&quot;)), data = renal) NOTE: entry.status has been set to &quot;NRA&quot; for all. str(Lr) Classes &#39;Lexis&#39; and &#39;data.frame&#39;: 125 obs. of 14 variables: $ per : num 1996 1990 1988 1995 1988 ... $ age : num 28.1 30.2 25.8 44.5 26.6 ... $ tfi : num 0 0 0 0 0 0 0 0 0 0 ... $ lex.dur: num 1.08 6.6 5.39 8.75 16.07 ... $ lex.Cst: Factor w/ 2 levels &quot;NRA&quot;,&quot;ESRD&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ lex.Xst: Factor w/ 2 levels &quot;NRA&quot;,&quot;ESRD&quot;: 2 2 2 1 1 2 2 1 2 1 ... $ lex.id : int 1 2 3 4 5 6 7 8 9 10 ... $ id : num 17 26 27 33 42 46 47 55 62 64 ... $ sex : Factor w/ 2 levels &quot;M&quot;,&quot;F&quot;: 1 2 2 1 2 2 1 1 2 1 ... $ dob : num 1968 1959 1962 1951 1961 ... $ doe : num 1996 1990 1988 1995 1988 ... $ dor : num NA 1990 NA 1996 1997 ... $ dox : num 1997 1996 1993 2004 2004 ... $ event : num 2 1 3 0 0 2 1 0 2 0 ... - attr(*, &quot;time.scales&quot;)= chr [1:3] &quot;per&quot; &quot;age&quot; &quot;tfi&quot; - attr(*, &quot;time.since&quot;)= chr [1:3] &quot;&quot; &quot;&quot; &quot;&quot; - attr(*, &quot;breaks&quot;)=List of 3 ..$ per: NULL ..$ age: NULL ..$ tfi: NULL summary(Lr) Transitions: To From NRA ESRD Records: Events: Risk time: Persons: NRA 48 77 125 77 1084.67 125 Make sure you know what the variables in Lr stand for. Visualize the follow-up in a Lexis-diagram, by using the plot method for Lexis objects. plot(Lr, col = &quot;black&quot;, lwd = 3) subset(Lr, age &lt; 0) lex.id per age tfi lex.dur lex.Cst lex.Xst id sex dob doe dor dox 88 1989.34 -38.81 0 3.5 NRA ESRD 586 M 2028.155 1989.343 NA 1992.839 event 1 What is wrong here? List the data for the person with negative entry age. Correct the data and make a new plot, for example by: Lr &lt;- transform(Lr, age = ifelse(dob &gt; 2000, age + 100, age), dob = ifelse(dob &gt; 2000, dob - 100, dob)) subset(Lr, id == 586) lex.id per age tfi lex.dur lex.Cst lex.Xst id sex dob doe dor dox 88 1989.34 61.19 0 3.5 NRA ESRD 586 M 1928.155 1989.343 NA 1992.839 event 1 plot(Lr, col = &quot;black&quot;, lwd = 3) Now make a Cox-regression analysis of ESRD occurrence with the variables sex and age at entry into the study, using time since entry to the study as time scale. mc &lt;- coxph(Surv(lex.dur, lex.Xst == &quot;ESRD&quot;) ~ I(age / 10) + sex, data = Lr) summary(mc) Call: coxph(formula = Surv(lex.dur, lex.Xst == &quot;ESRD&quot;) ~ I(age/10) + sex, data = Lr) n= 125, number of events= 77 coef exp(coef) se(coef) z Pr(&gt;|z|) I(age/10) 0.5514 1.7357 0.1402 3.932 8.43e-05 sexF -0.1817 0.8338 0.2727 -0.666 0.505 exp(coef) exp(-coef) lower .95 upper .95 I(age/10) 1.7357 0.5761 1.3186 2.285 sexF 0.8338 1.1993 0.4886 1.423 Concordance= 0.612 (se = 0.036 ) Likelihood ratio test= 16.07 on 2 df, p=3e-04 Wald test = 16.38 on 2 df, p=3e-04 Score (logrank) test = 16.77 on 2 df, p=2e-04 What is the hazard ratio between males and females? Between two persons who differ 10 years in age at entry? The main focus of the paper was to assess whether the occurrence of remission (return to a lower level of albumin excretion, an indication of kidney recovery) influences mortality. Remission is a time-dependent variable which is initially 0, but takes the value 1 when remission occurs. In order to handle this, each person who sees a remission must have two records: One record for the time before remission, where entry is doe, exit is dor, remission is 0, and event is 0. One record for the time after remission, where entry is dor, exit is dox, remission is 1, and event is 0 or 1 according to whether the person had an event at dox. This is accomplished using the cutLexis function on the Lexis object, where we introduce a remission state Rem. Also use split.state=TRUE to have different ESRD states according to whether a person had had remission or not prioer to ESRD. The statement to do this is: Lc &lt;- cutLexis(Lr, cut = Lr$dor, # where to cut follow up timescale = &quot;per&quot;, # what timescale are we referring to new.state = &quot;Rem&quot;, # name of the new state split.state = TRUE) # different states depending on previous summary(Lc) Transitions: To From NRA Rem ESRD ESRD(Rem) Records: Events: Risk time: Persons: NRA 24 29 69 0 122 98 824.77 122 Rem 0 24 0 8 32 8 259.90 32 Sum 24 53 69 8 154 106 1084.67 125 List the records from a few select persons (choose values for lex.id, using for example subset(Lc, lex.id %in% c(5,7,9))). Now show how the states are connected and the number of transitions between them by using boxes. This is an interactive command that requires you to click in the graph window: boxes(Lc) It has a couple of fancy arguments, try: boxes(Lc, boxpos = TRUE, scale.R = 100, show.BE = TRUE, hm = 1.5, wm = 1.5) You may even be tempted to read the help page for boxes.Lexis … Plot a Lexis diagram where different coloring is used for different segments of the follow-up. The plot.Lexis function draws a line for each record in the dataset, so you can index the coloring by lex.Cst and lex.Xst as appropriate — indexing by a factor corresponds to indexing by the index number of the factor levels, so you must be know which order the factor levels are in: levels(Lc) # names and order of states in lex.Cst and lex.Xst [1] &quot;NRA&quot; &quot;Rem&quot; &quot;ESRD&quot; &quot;ESRD(Rem)&quot; par(mai = c(3, 3, 1, 1) / 4, mgp = c(3, 1, 0) / 1.6) plot(Lc, col = c(&quot;red&quot;, &quot;limegreen&quot;)[Lc$lex.Cst], xlab = &quot;Calendar time&quot;, ylab = &quot;Age&quot;, lwd = 3, grid = 0:20 * 5, las = 1, xlim = c(1970, 2010), ylim = c(20, 70), xaxs = &quot;i&quot;, yaxs = &quot;i&quot;) points(Lc, pch = c(NA, NA, 16, 16)[Lc$lex.Xst], col = c(&quot;red&quot;, &quot;limegreen&quot;, &quot;transparent&quot;, &quot;transparent&quot;)[Lc$lex.Cst]) points(Lc, pch = c(NA, NA, 1, 1)[Lc$lex.Xst], col = &quot;black&quot;, lwd = 2) Make a Cox-regression of mortality rates (i.e. endpoint ESRD or ESRD(Rem)) with sex, age at entry and remission as explanatory variables, using time since entry as timescale, and include lex.Cst as time-dependent variable, and indicate that each record represents follow-up from tfi to tfi+lex.dur. Make sure that you know why what goes where here in the call to coxph. (EP &lt;- levels(Lc)[3:4]) # define EndPoint states [1] &quot;ESRD&quot; &quot;ESRD(Rem)&quot; m1 &lt;- coxph(Surv(tfi, # entry time tfi + lex.dur, # exit time lex.Xst %in% EP) # event ~ sex + I((doe - dob - 50) / 10) + # fixed covariates (lex.Cst == &quot;Rem&quot;), # time-dependent variable data = Lc) summary(m1) Call: coxph(formula = Surv(tfi, tfi + lex.dur, lex.Xst %in% EP) ~ sex + I((doe - dob - 50)/10) + (lex.Cst == &quot;Rem&quot;), data = Lc) n= 154, number of events= 77 coef exp(coef) se(coef) z Pr(&gt;|z|) sexF -0.05534 0.94616 0.27500 -0.201 0.840517 I((doe - dob - 50)/10) 0.52190 1.68522 0.13655 3.822 0.000132 lex.Cst == &quot;Rem&quot;TRUE -1.26241 0.28297 0.38483 -3.280 0.001036 exp(coef) exp(-coef) lower .95 upper .95 sexF 0.9462 1.0569 0.5519 1.6220 I((doe - dob - 50)/10) 1.6852 0.5934 1.2895 2.2024 lex.Cst == &quot;Rem&quot;TRUE 0.2830 3.5339 0.1331 0.6016 Concordance= 0.664 (se = 0.033 ) Likelihood ratio test= 30.31 on 3 df, p=1e-06 Wald test = 27.07 on 3 df, p=6e-06 Score (logrank) test = 29.41 on 3 df, p=2e-06 What is the effect of of remission on the rate of ESRD? Splitting the follow-up time In order to explore the effect of remission on the rate of ESRD, we split the data further into small pieces of follow-up. To this end we use the function splitLexis. The rates can then be modeled using a Poisson-model, and the shape of the effect of the underlying rates be explored. Furthermore, we can allow effects of both time since NRA and current age. To this end we will use splines, so we need the splines and also the mgcv packages. Now split the follow-up time every month after entry, and verify that the number of events and risk time is the same as before and after the split: sLc &lt;- splitLexis(Lc, &quot;tfi&quot;, breaks = seq(0, 30, 1/12)) summary( Lc) Transitions: To From NRA Rem ESRD ESRD(Rem) Records: Events: Risk time: Persons: NRA 24 29 69 0 122 98 824.77 122 Rem 0 24 0 8 32 8 259.90 32 Sum 24 53 69 8 154 106 1084.67 125 summary(sLc) Transitions: To From NRA Rem ESRD ESRD(Rem) Records: Events: Risk time: Persons: NRA 9854 29 69 0 9952 98 824.77 122 Rem 0 3139 0 8 3147 8 259.90 32 Sum 9854 3168 69 8 13099 106 1084.67 125 Now fit the Poisson-model corresponding to the Cox-model we fitted previously. The function Ns() produces a model matrix corresponding to a piece-wise cubic function, modeling the baseline hazard explicitly (think of the Ns terms as the baseline hazard that is not visible in the Cox-model). You can use the wrapper function glm.Lexis mp &lt;- glm.Lexis(sLc, ~ Ns(tfi, knots = c(0, 2, 5, 10)) + sex + I((doe - dob - 40) / 10) + I(lex.Cst == &quot;Rem&quot;)) stats::glm Poisson analysis of Lexis object sLc with log link: Rates for transitions: NRA-&gt;ESRD Rem-&gt;ESRD(Rem) ci.exp(mp) exp(Est.) 2.5% 97.5% (Intercept) 0.01664432 0.003956666 0.07001685 Ns(tfi, knots = c(0, 2, 5, 10))1 5.18917655 1.949197027 13.81469029 Ns(tfi, knots = c(0, 2, 5, 10))2 34.20004199 1.764818735 662.75524463 Ns(tfi, knots = c(0, 2, 5, 10))3 4.43318269 2.179977108 9.01528219 sexF 0.91751162 0.536258443 1.56981691 I((doe - dob - 40)/10) 1.70082390 1.300813859 2.22384004 I(lex.Cst == &quot;Rem&quot;)TRUE 0.27927558 0.131396852 0.59358233 How does the effects of sex change from the Cox-model? Try instead using the gam function from the mgcv package. There is convenience wrapper for this for Lexis objects as well: mx &lt;- gam.Lexis(sLc, ~ s(tfi, k = 10) + sex + I((doe - dob - 40) / 10) + I(lex.Cst == &quot;Rem&quot;)) mgcv::gam Poisson analysis of Lexis object sLc with log link: Rates for transitions: NRA-&gt;ESRD Rem-&gt;ESRD(Rem) ci.exp(mp, subset = c(&quot;Cst&quot;, &quot;doe&quot;, &quot;sex&quot;)) exp(Est.) 2.5% 97.5% I(lex.Cst == &quot;Rem&quot;)TRUE 0.2792756 0.1313969 0.5935823 I((doe - dob - 40)/10) 1.7008239 1.3008139 2.2238400 sexF 0.9175116 0.5362584 1.5698169 ci.exp(mx, subset = c(&quot;Cst&quot;, &quot;doe&quot;, &quot;sex&quot;)) exp(Est.) 2.5% 97.5% I(lex.Cst == &quot;Rem&quot;)TRUE 0.2784659 0.1309446 0.5921838 I((doe - dob - 40)/10) 1.6992069 1.2995225 2.2218192 sexF 0.9309945 0.5435486 1.5946150 We see that there is virtually no difference between the two approaches in terms of the regression parameters. Extract the regression parameters from the models using ci.exp and compare with the estimates from the Cox-model: ci.exp(mx, subset = c(&quot;sex&quot;, &quot;dob&quot;, &quot;Cst&quot;), pval = TRUE) exp(Est.) 2.5% 97.5% P sexF 0.9309945 0.5435486 1.5946150 0.7945394004 I((doe - dob - 40)/10) 1.6992069 1.2995225 2.2218192 0.0001066910 I(lex.Cst == &quot;Rem&quot;)TRUE 0.2784659 0.1309446 0.5921838 0.0008970863 ci.exp(m1) exp(Est.) 2.5% 97.5% sexF 0.9461646 0.5519334 1.621985 I((doe - dob - 50)/10) 1.6852196 1.2895097 2.202360 lex.Cst == &quot;Rem&quot;TRUE 0.2829710 0.1330996 0.601599 round(ci.exp(mp, subset = c(&quot;sex&quot;, &quot;dob&quot;, &quot;Cst&quot;)) / ci.exp(m1), 2) exp(Est.) 2.5% 97.5% sexF 0.97 0.97 0.97 I((doe - dob - 40)/10) 1.01 1.01 1.01 I(lex.Cst == &quot;Rem&quot;)TRUE 0.99 0.99 0.99 How large are the differences in estimated regression parameters? The model has the same assumptions as the Cox-model about proportionality of rates, but there is an additional assumption that the hazard is a smooth function of time since entry. It seems to be a sensible assumption (well, restriction) to put on the rates that they vary smoothly by time. No such restriction is made in the Cox model. The gam model optimizes the shape of the smoother by general cross-validation. Try to look at the shape of the estimated effect of tfi: plot(mx) Is this a useful plot? However, plot (well, plot.gam) does not give you the absolute level of the underlying rates because it bypasses the intercept. So in order to predict the rates as a function of tfi and the covariates, we set up a prediction data frame. Note that age in the model specification is entered as doe-dob, hence the prediction data frame must have these two variables and not the age, but it is only the difference that matters for the prediction: nd &lt;- data.frame(tfi = seq(0, 20, 0.1), sex = &quot;M&quot;, doe = 1990, dob = 1940, lex.Cst = &quot;NRA&quot;) str(nd) &#39;data.frame&#39;: 201 obs. of 5 variables: $ tfi : num 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 ... $ sex : chr &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; ... $ doe : num 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ... $ dob : num 1940 1940 1940 1940 1940 1940 1940 1940 1940 1940 ... $ lex.Cst: chr &quot;NRA&quot; &quot;NRA&quot; &quot;NRA&quot; &quot;NRA&quot; ... matshade(nd$tfi, cbind(ci.pred(mp, newdata = nd), ci.pred(mx, newdata = nd)) * 100, plot = TRUE, type = &quot;l&quot;, lwd = 3:4, col = c(&quot;black&quot;, &quot;forestgreen&quot;), log = &quot;y&quot;, xlab = &quot;Time since entry (years)&quot;, ylab = &quot;ESRD rate (per 100 PY) for 50 year old men&quot;) Try to overlay with the corresponding prediction from the glm model using Ns. ## Prediction from the multistate model If we want to make proper statements about the survival and disease probabilities we must know not only how the occurrence of remission influences the rate of death/ESRD, but we must also model the occurrence rate of remission itself. The rates of ESRD were modelled by a Poisson model with effects of age and time since NRA — in the models mp and mx. But if we want to model whole process we must also model the remission rates transition from NRA to Rem, but the number of events is rather small so we restrict covariates in this model to only time since NRA and sex. Note that only the records that represent follow-up in the NRA state should be used; this is most easily done using the gam.Lexis function mr &lt;- gam.Lexis(sLc, ~ s(tfi, k = 10) + sex, from = &quot;NRA&quot;, to = &quot;Rem&quot;) mgcv::gam Poisson analysis of Lexis object sLc with log link: Rates for the transition: NRA-&gt;Rem summary(mr) Family: poisson Link function: log Formula: cbind(trt(Lx$lex.Cst, Lx$lex.Xst) %in% trnam, Lx$lex.dur) ~ s(tfi, k = 10) + sex Parametric coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.7025 0.2582 -14.339 &lt;2e-16 sexF 0.9579 0.3728 2.569 0.0102 Approximate significance of smooth terms: edf Ref.df Chi.sq p-value s(tfi) 1.013 1.025 0.066 0.813 R-sq.(adj) = -5.65e-06 Deviance explained = 1.65% UBRE = -0.96024 Scale est. = 1 n = 9952 ci.exp(mr, pval = TRUE) exp(Est.) 2.5% 97.5% P (Intercept) 0.02466174 0.01486718 0.04090901 1.254019e-46 sexF 2.60620470 1.25503844 5.41202779 1.019130e-02 s(tfi).1 1.00499489 0.89131271 1.13317662 9.351638e-01 s(tfi).2 0.99623769 0.80778743 1.22865188 9.718940e-01 s(tfi).3 0.99822247 0.91911268 1.08414140 9.663137e-01 s(tfi).4 1.00188999 0.89006998 1.12775801 9.750528e-01 s(tfi).5 0.99842904 0.92280785 1.08024715 9.687920e-01 s(tfi).6 0.99817367 0.90142303 1.10530865 9.719666e-01 s(tfi).7 1.00168704 0.92615949 1.08337380 9.663850e-01 s(tfi).8 0.99448155 0.68445450 1.44493689 9.768400e-01 s(tfi).9 0.94790052 0.63349476 1.41834700 7.946918e-01 What is the remission rate-ratio between men and women? If we want to predict the probability of being in each of the three states using these estimated rates, we may resort to analytical calculations of the probabilities from the estimated rates, which is actually doable in this case, but which will be largely intractable for more complicated models. Alternatively we can simulate the life course for a large group of (identical) individuals through a model using the estimated rates. That will give a simulated cohort (in the form of a Lexis object), and we can then just count the number of persons in each state at each of a set of time points. This is accomplished using the function simLexis. The input to this is the initial status of the persons whose life-course we shall simulate, and the transition rates in suitable form: Suppose we want predictions for men aged 50 at NRA. The input is in the form of a Lexis object (where lex.dur and lex.Xst will be ignored). Note that in order to carry over the time.scales and the time.since attributes, we construct the input object using subset to select columns, and NULL to select rows (see the example in the help file for simLexis): inL &lt;- subset(sLc, select = 1:11)[NULL, ] str(inL) Classes &#39;Lexis&#39; and &#39;data.frame&#39;: 0 obs. of 11 variables: $ lex.id : int $ per : num $ age : num $ tfi : num $ lex.dur: num $ lex.Cst: Factor w/ 4 levels &quot;NRA&quot;,&quot;Rem&quot;,&quot;ESRD&quot;,..: $ lex.Xst: Factor w/ 4 levels &quot;NRA&quot;,&quot;Rem&quot;,&quot;ESRD&quot;,..: $ id : num $ sex : Factor w/ 2 levels &quot;M&quot;,&quot;F&quot;: $ dob : num $ doe : num - attr(*, &quot;time.scales&quot;)= chr [1:3] &quot;per&quot; &quot;age&quot; &quot;tfi&quot; - attr(*, &quot;time.since&quot;)= chr [1:3] &quot;&quot; &quot;&quot; &quot;&quot; - attr(*, &quot;breaks&quot;)=List of 3 ..$ per: NULL ..$ age: NULL ..$ tfi: num [1:361] 0 0.0833 0.1667 0.25 0.3333 ... timeScales(inL) [1] &quot;per&quot; &quot;age&quot; &quot;tfi&quot; inL[1, &quot;lex.id&quot;] &lt;- 1 inL[1, &quot;per&quot;] &lt;- 2000 inL[1, &quot;age&quot;] &lt;- 50 inL[1, &quot;tfi&quot;] &lt;- 0 inL[1, &quot;lex.Cst&quot;] &lt;- &quot;NRA&quot; inL[1, &quot;lex.Xst&quot;] &lt;- NA inL[1, &quot;lex.dur&quot;] &lt;- NA inL[1, &quot;sex&quot;] &lt;- &quot;M&quot; inL[1, &quot;doe&quot;] &lt;- 2000 inL[1, &quot;dob&quot;] &lt;- 1950 inL &lt;- rbind(inL, inL) inL[2, &quot;sex&quot;] &lt;- &quot;F&quot; inL lex.id per age tfi lex.dur lex.Cst lex.Xst id sex dob doe 1 2000 50 0 NA NRA &lt;NA&gt; NA M 1950 2000 1 2000 50 0 NA NRA &lt;NA&gt; NA F 1950 2000 str(inL) Classes &#39;Lexis&#39; and &#39;data.frame&#39;: 2 obs. of 11 variables: $ lex.id : num 1 1 $ per : num 2000 2000 $ age : num 50 50 $ tfi : num 0 0 $ lex.dur: num NA NA $ lex.Cst: Factor w/ 4 levels &quot;NRA&quot;,&quot;Rem&quot;,&quot;ESRD&quot;,..: 1 1 $ lex.Xst: Factor w/ 4 levels &quot;NRA&quot;,&quot;Rem&quot;,&quot;ESRD&quot;,..: NA NA $ id : num NA NA $ sex : Factor w/ 2 levels &quot;M&quot;,&quot;F&quot;: 1 2 $ dob : num 1950 1950 $ doe : num 2000 2000 - attr(*, &quot;breaks&quot;)=List of 3 ..$ per: NULL ..$ age: NULL ..$ tfi: num [1:361] 0 0.0833 0.1667 0.25 0.3333 ... - attr(*, &quot;time.scales&quot;)= chr [1:3] &quot;per&quot; &quot;age&quot; &quot;tfi&quot; - attr(*, &quot;time.since&quot;)= chr [1:3] &quot;&quot; &quot;&quot; &quot;&quot; The other input for the simulation is the models for the transitions. This is given as a list with an element for each transient state (that is NRA and Rem), each of which is again a list with names equal to the states that can be reached from the transient state. The content of the list will be glm objects, in this case the models we just fitted, describing the transition rates: Tr &lt;- list(&quot;NRA&quot; = list(&quot;Rem&quot; = mr, &quot;ESRD&quot; = mx), &quot;Rem&quot; = list(&quot;ESRD(Rem)&quot; = mx)) With this as input we can now generate a cohort, using N=5 to simulate life course of 10 persons (5 for each set of starting values in inL): (iL &lt;- simLexis(Tr, inL, N = 10)) lex.id per age tfi lex.dur lex.Cst lex.Xst id sex dob doe cens 1 2000.00 50.00 0.00 6.37 NRA ESRD NA M 1950 2000 2020 2 2000.00 50.00 0.00 2.77 NRA ESRD NA M 1950 2000 2020 3 2000.00 50.00 0.00 4.56 NRA ESRD NA M 1950 2000 2020 4 2000.00 50.00 0.00 8.35 NRA ESRD NA M 1950 2000 2020 5 2000.00 50.00 0.00 4.21 NRA ESRD NA M 1950 2000 2020 6 2000.00 50.00 0.00 5.46 NRA ESRD NA M 1950 2000 2020 7 2000.00 50.00 0.00 1.99 NRA Rem NA M 1950 2000 2020 7 2001.99 51.99 1.99 18.01 Rem Rem NA M 1950 2000 2020 8 2000.00 50.00 0.00 3.97 NRA ESRD NA M 1950 2000 2020 9 2000.00 50.00 0.00 4.75 NRA ESRD NA M 1950 2000 2020 10 2000.00 50.00 0.00 5.68 NRA ESRD NA M 1950 2000 2020 11 2000.00 50.00 0.00 3.71 NRA ESRD NA F 1950 2000 2020 12 2000.00 50.00 0.00 4.76 NRA Rem NA F 1950 2000 2020 12 2004.76 54.76 4.76 9.92 Rem ESRD(Rem) NA F 1950 2000 2020 13 2000.00 50.00 0.00 5.67 NRA ESRD NA F 1950 2000 2020 14 2000.00 50.00 0.00 5.86 NRA ESRD NA F 1950 2000 2020 15 2000.00 50.00 0.00 10.21 NRA ESRD NA F 1950 2000 2020 16 2000.00 50.00 0.00 0.49 NRA Rem NA F 1950 2000 2020 16 2000.49 50.49 0.49 1.83 Rem ESRD(Rem) NA F 1950 2000 2020 17 2000.00 50.00 0.00 6.45 NRA ESRD NA F 1950 2000 2020 18 2000.00 50.00 0.00 0.82 NRA ESRD NA F 1950 2000 2020 19 2000.00 50.00 0.00 1.27 NRA ESRD NA F 1950 2000 2020 20 2000.00 50.00 0.00 12.64 NRA ESRD NA F 1950 2000 2020 summary(iL, by = &quot;sex&quot;) $M Transitions: To From NRA Rem ESRD ESRD(Rem) Records: Events: Risk time: Persons: NRA 0 1 9 0 10 10 48.12 10 Rem 0 1 0 0 1 0 18.01 1 Sum 0 2 9 0 11 10 66.12 10 $F Transitions: To From NRA Rem ESRD ESRD(Rem) Records: Events: Risk time: Persons: NRA 0 2 8 0 10 10 51.87 10 Rem 0 0 0 2 2 2 11.75 2 Sum 0 2 8 2 12 12 63.62 10 What type of object have you got as iL? Now generate the life course of, say, 5,000 persons, and look at the summary. The system.time command is just to tell you how long it took, you may want to start with 500 just to see how long that takes. system.time(sM &lt;- simLexis(Tr, inL, N = 500, t.range = 12)) user system elapsed 2.645 3.048 2.016 summary(sM, by = &quot;sex&quot;) $M Transitions: To From NRA Rem ESRD ESRD(Rem) Records: Events: Risk time: Persons: NRA 46 78 376 0 500 454 2707.59 500 Rem 0 51 0 27 78 27 569.15 78 Sum 46 129 376 27 578 481 3276.74 500 $F Transitions: To From NRA Rem ESRD ESRD(Rem) Records: Events: Risk time: Persons: NRA 27 168 305 0 500 473 2481.57 500 Rem 0 96 0 72 168 72 1092.90 168 Sum 27 264 305 72 668 545 3574.47 500 Why are there so many ESRD-events in the resulting data set? Now count how many persons are present in each state at each time for the first 10 years after entry (which is at age 50). This can be done by using nState. Try: nStm &lt;- nState(subset(sM, sex == &quot;M&quot;), time.scale = &quot;age&quot;, at = seq(0, 10, 0.1), from = 50) nStf &lt;- nState(subset(sM, sex == &quot;F&quot;), time.scale = &quot;age&quot;, at = seq(0, 10, 0.1), from = 50) head(nStf, 15) State when NRA Rem ESRD ESRD(Rem) 50 500 0 0 0 50.1 497 1 2 0 50.2 492 5 3 0 50.3 489 8 3 0 50.4 478 19 3 0 50.5 471 24 5 0 50.6 467 26 7 0 50.7 465 27 8 0 50.8 461 29 10 0 50.9 453 32 15 0 51 447 35 18 0 51.1 441 36 23 0 51.2 433 41 26 0 51.3 427 44 29 0 51.4 425 44 31 0 What is in the object nStf? With the counts of persons in each state at the designated time points (in nStm), compute the cumulative fraction over the states, arranged in order given by perm: ppm &lt;- pState(nStm, perm = c(2, 1, 3, 4)) ppf &lt;- pState(nStf, perm = c(2, 1, 3, 4)) head(ppf) State when Rem NRA ESRD ESRD(Rem) 50 0.000 1.000 1 1 50.1 0.002 0.996 1 1 50.2 0.010 0.994 1 1 50.3 0.016 0.994 1 1 50.4 0.038 0.994 1 1 50.5 0.048 0.990 1 1 tail(ppf) State when Rem NRA ESRD ESRD(Rem) 59.5 0.226 0.352 0.898 1 59.6 0.222 0.348 0.894 1 59.7 0.224 0.346 0.894 1 59.8 0.222 0.338 0.892 1 59.9 0.220 0.334 0.890 1 60 0.222 0.330 0.890 1 What do the entries in ppf represent? Try to plot the cumulative probabilities using the plot method for pState objects: plot(ppf) Is this useful? Now try to improve the plot so that it is easier to read, and easier to compare between men and women, for example: par(mfrow = c(1, 2)) # Men plot(ppm, col = c(&quot;limegreen&quot;, &quot;red&quot;, &quot;#991111&quot;, &quot;forestgreen&quot;)) lines(as.numeric(rownames(ppm)), ppm[, &quot;NRA&quot;], lwd = 2) text(59.5, 0.95, &quot;Men&quot;, adj = 1, col = &quot;white&quot;, font = 2, cex = 1.2) axis(side = 4, at = 0:10 / 10) axis(side = 4, at = 1:99 / 100, labels = NA, tck = -0.01) # Women plot(ppf, col = c(&quot;limegreen&quot;, &quot;red&quot;, &quot;#991111&quot;, &quot;forestgreen&quot;), xlim = c(60, 50)) # inverted x-axis lines(as.numeric(rownames(ppf)), ppf[, &quot;NRA&quot;], lwd = 2) text(59.5, 0.95, &quot;Women&quot;, adj = 0, col = &quot;white&quot;, font = 2, cex = 1.2) axis(side = 2, at = 0:10 / 10) axis(side = 2, at = 1:99 / 100, labels = NA, tck = -0.01) What is the 10-year risk of remission for men and women respectively? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
